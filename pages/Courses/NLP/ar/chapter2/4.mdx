# Tokenizers

Tokenizers هي أحد المكونات الأساسية في خط أنابيب معالجة اللغات الطبيعية (NLP). وتتمثل مهمتها في تحويل النص إلى بيانات يمكن للنموذج معالجتها. حيث لا يمكن للنموذج سوى معالجة الأرقام، لذا يجب على برامج Tokenizers تحويل إدخالات النص إلى بيانات رقمية. في هذا القسم، سنستكشف بالضبط ما يحدث في خط أنابيب Tokenization.

في مهام NLP، تكون البيانات التي تتم معالجتها عادةً نصًا خامًا. فيما يلي مثال على مثل هذا النص:

```
كان جيم هانسون صانع دمى
```

ومع ذلك، لا يمكن للنماذج سوى معالجة الأرقام، لذلك نحتاج إلى إيجاد طريقة لتحويل النص الخام إلى أرقام. هذا ما تفعله برامج Tokenizers، وهناك الكثير من الطرق للقيام بذلك. الهدف هو إيجاد أكثر التمثيلات دلالية - أي التمثيل الذي يكون أكثر منطقية بالنسبة للنموذج - وأصغر تمثيل ممكن.

دعونا نلقي نظرة على بعض أمثلة خوارزميات Tokenization، ونحاول الإجابة على بعض الأسئلة التي قد تكون لديك حول Tokenization.

## القائم على الكلمات

أول نوع من برامج Tokenizers التي تخطر على البال هو برنامج Tokenizer القائم على الكلمات. إنه بشكل عام سهل الإعداد والاستخدام مع عدد قليل فقط من القواعد، وغالبا ما يعطي نتائج جيدة. على سبيل المثال، في الصورة أدناه، الهدف هو تقسيم النص الخام إلى كلمات وإيجاد تمثيل رقمي لكل منها:

هناك طرق مختلفة لتقسيم النص. على سبيل المثال، يمكننا استخدام المسافة البيضاء لتوكينيز النص إلى كلمات عن طريق تطبيق دالة `split()` في بايثون:

```py
النص المميز = "كان جيم هانسون صانع دمى". الانقسام ()
طباعة (النص المميز)
```

```py
['جيم'، 'هانسون'، 'كان'، 'دمية']
```

هناك أيضًا متغيرات من برامج Tokenizers القائمة على الكلمات والتي تحتوي على قواعد إضافية للترقيم. مع هذا النوع من برامج Tokenizers، يمكننا أن نصل إلى "معاجم" كبيرة جدًا، حيث يتم تعريف المعجم بواسطة العدد الإجمالي للرموز المستقلة التي لدينا في فيلمنا.

يتم تعيين معرف لكل كلمة، بدءًا من 0 وحتى حجم المعجم. يستخدم النموذج هذه المعرفات لتحديد كل كلمة.

إذا أردنا تغطية لغة ما تمامًا باستخدام برنامج Tokenizer القائم على الكلمات، فسوف نحتاج إلى معرف لكل كلمة في اللغة، والتي ستولد عددًا كبيرًا من الرموز. على سبيل المثال، هناك أكثر من 500000 كلمة في اللغة الإنجليزية، لذلك لبناء خريطة من كل كلمة إلى معرف الإدخال، سنحتاج إلى تتبع هذا العدد الكبير من المعرفات. علاوة على ذلك، يتم تمثيل الكلمات مثل "كلب" بشكل مختلف عن الكلمات مثل "كلاب"، ولن يكون لدى النموذج في البداية أي طريقة لمعرفة أن "كلب" و "كلاب" متشابهة: فسيحدد الكلمتين على أنهما غير مرتبطتين. وينطبق الشيء نفسه على الكلمات المتشابهة الأخرى، مثل "الجري" و "الجري"، والتي لن يراها النموذج في البداية على أنها متشابهة.

أخيرًا، نحتاج إلى رمز مخصص لتمثيل الكلمات التي ليست في معجمنا. يُعرف هذا باسم الرمز "مجهول"، والذي يُشار إليه غالبًا باسم "[UNK]" أو "<unk>". من السيئ أن ترى أن برنامج Tokenizer ينتج الكثير من هذه الرموز، لأنه لم يتمكن من استرداد تمثيل منطقي للكلمة وأنك تفقد المعلومات على طول الطريق. الهدف عند صياغة المعجم هو القيام بذلك بطريقة تؤدي إلى توكينيز برنامج Tokenizer لأقل عدد ممكن من الكلمات إلى الرمز المجهول.

تتمثل إحدى طرق تقليل عدد الرموز المجهولة في النزول بمستوى أعمق، باستخدام برنامج Tokenizer القائم على الأحرف.

## القائم على الأحرف

تقوم برامج Tokenizers القائمة على الأحرف بتقسيم النص إلى أحرف، بدلاً من الكلمات. لهذا فائدتان رئيسيتان:

- المعجم أصغر بكثير.
- هناك عدد أقل بكثير من الرموز خارج المعجم (مجهول)، لأن كل كلمة يمكن بناؤها من الأحرف.

ولكن هنا أيضًا تثار بعض التساؤلات حول المسافات والفواصل:

هذا النهج ليس مثاليًا أيضًا. نظرًا لأن التمثيل يعتمد الآن على الأحرف بدلاً من الكلمات، يمكن للمرء أن يجادل بأنه، بديهيًا، أقل دلالة: لا يعني كل حرف الكثير بمفرده، في حين أن هذه هي الحالة مع الكلمات. ومع ذلك، يختلف هذا مرة أخرى حسب اللغة؛ في اللغة الصينية، على سبيل المثال، يحمل كل حرف معلومات أكثر من حرف في لغة لاتينية.

شيء آخر يجب مراعاته هو أننا سننتهي بكمية كبيرة جدًا من الرموز التي يجب على نموذجنا معالجتها: في حين أن الكلمة ستكون رمزًا واحدًا فقط مع برنامج Tokenizer القائم على الكلمات، فيمكن أن تتحول بسهولة إلى 10 رموز أو أكثر عند تحويلها إلى أحرف.

للحصول على أفضل ما في العالمين، يمكننا استخدام تقنية ثالثة تجمع بين النهجين: توكينيزات الأحرف الفرعية.

## توكينيزات الأحرف الفرعية

تعتمد خوارزميات توكينيزات الأحرف الفرعية على المبدأ القائل بأن الكلمات المستخدمة بشكل متكرر لا ينبغي تقسيمها إلى أحرف فرعية أصغر، ولكن يجب تحليل الكلمات النادرة إلى أحرف فرعية ذات معنى.

على سبيل المثال، قد يُعتبر "مثير للإزعاج" كلمة نادرة ويمكن تحليلها إلى "مزعج" و "لي". من المحتمل أن يظهر كلاهما بشكل متكرر كأحرف فرعية مستقلة، وفي الوقت نفسه، يتم الحفاظ على معنى "مثير للإزعاج" بواسطة المعنى المركب لـ "مزعج" و "لي".

فيما يلي مثال يوضح كيفية توكينيز تسلسل "دعونا نفعل Tokenization!" باستخدام خوارزمية توكينيزات الأحرف الفرعية:

توفر هذه الأحرف الفرعية الكثير من المعنى الدلالي: على سبيل المثال، في المثال أعلاه، تم تقسيم "Tokenization" إلى "Token" و "ization"، وهما رمزان لهما معنى دلالي أثناء كفاءة المساحة (يتم استخدام رمزين فقط لتمثيل كلمة طويلة). يسمح لنا هذا بالحصول على تغطية جيدة نسبيًا بمعاجم صغيرة، ولا يوجد تقريبًا أي رموز مجهولة.

هذا النهج مفيد بشكل خاص في اللغات التراصية مثل التركية، حيث يمكنك تشكيل كلمات معقدة طويلة (تقريبًا) عن طريق ربط الأحرف الفرعية معًا.

## المزيد

ليس من المستغرب أن هناك العديد من التقنيات الأخرى هناك. لذكر بعض:

- Byte-level BPE، كما هو مستخدم في GPT-2
- WordPiece، كما هو مستخدم في BERT
- SentencePiece أو Unigram، كما هو مستخدم في العديد من النماذج متعددة اللغات

الآن يجب أن يكون لديك معرفة كافية بكيفية عمل برامج Tokenizers للبدء في واجهة برمجة التطبيقات.

## التحميل والتخزين

التحميل والتخزين بسيطان مثل النماذج. في الواقع، فهو يعتمد على نفس الطريقتين: `from_pretrained()` و `save_pretrained()`. ستقوم هذه الطرق بتحميل أو حفظ الخوارزمية التي يستخدمها برنامج Tokenizer (تشبه إلى حد ما "هندسة" النموذج) بالإضافة إلى معجمه (تشبه إلى حد ما "أوزان" النموذج).

يتم تحميل برنامج Tokenizer BERT المدرب بنفس نقطة التفتيش مثل BERT بنفس طريقة تحميل النموذج، باستثناء أننا نستخدم فئة `BertTokenizer`:

```py
من المحولات استيراد BertTokenizer

برنامج Tokenizer = BertTokenizer.from_pretrained ("bert-base-cased")
```

مشابه لـ `AutoModel`، ستستخرج فئة `AutoTokenizer` فئة برنامج Tokenizer الصحيحة في المكتبة بناءً على اسم نقطة التفتيش، ويمكن استخدامها مباشرة مع أي نقطة تفتيش:

```py
من المحولات استيراد AutoTokenizer

برنامج Tokenizer = AutoTokenizer.from_pretrained ("bert-base-cased")
```

يمكننا الآن استخدام برنامج Tokenizer كما هو موضح في القسم السابق:

```py
برنامج Tokenizer ("استخدام شبكة المحول بسيط")
```

```py
{'input_ids': [101، 7993، 170، 11303، 1200، 2443، 1110، 3014، 102]،
'token_type_ids': [0، 0، 0، 0، 0، 0، 0، 0، 0]،
'attention_mask': [1، 1، 1، 1، 1، 1، 1، 1، 1]}
```

يتم حفظ برنامج Tokenizer بنفس طريقة حفظ النموذج:

```py
برنامج Tokenizer.save_pretrained ("directory_on_my_computer")
```

سنتحدث أكثر عن `token_type_ids` في [الفصل 3] (/ course / chapter3)، وسنقوم بتوضيح مفتاح `attention_mask` لاحقًا. أولاً، دعنا نرى كيف يتم إنشاء `input_ids`. للقيام بذلك، سنحتاج إلى النظر في الطرق الوسيطة لبرنامج Tokenizer.

## الترميز

يُعرف تحويل النص إلى أرقام باسم الترميز. يتم الترميز في عملية من خطوتين: Tokenization، تليها التحويل إلى معرفات الإدخال.

كما رأينا، تتمثل الخطوة الأولى في تقسيم النص إلى كلمات (أو أجزاء من الكلمات، أو رموز الترقيم، وما إلى ذلك)، والتي يُطلق عليها عادةً *الرموز*. هناك العديد من القواعد التي يمكن أن تحكم تلك العملية، وهذا هو السبب في أننا بحاجة إلى إنشاء مثيل لبرنامج Tokenizer باستخدام اسم النموذج، للتأكد من أننا نستخدم نفس القواعد التي تم استخدامها عند تدريب النموذج مسبقًا.

الخطوة الثانية هي تحويل هذه الرموز إلى أرقام، حتى نتمكن من بناء tensor منها وإطعامها للنموذج. للقيام بذلك، لدى برنامج Tokenizer *معجم*، وهو الجزء الذي نقوم بتنزيله عند إنشاء مثيل له باستخدام طريقة `from_pretrained()`. مرة أخرى، نحتاج إلى استخدام نفس المعجم المستخدم عند تدريب النموذج مسبقًا.

لفهم أفضل للخطوتين، سنستكشفها بشكل منفصل. لاحظ أننا سنستخدم بعض الطرق التي تؤدي أجزاء من خط أنابيب Tokenization بشكل منفصل لإظهار النتائج الوسيطة لتلك الخطوات، ولكن في الممارسة العملية، يجب عليك استدعاء برنامج Tokenizer مباشرةً على إدخالاتك (كما هو موضح في القسم 2).

### Tokenization

تتم عملية Tokenization بواسطة طريقة `tokenize()` لبرنامج Tokenizer:

```py
من المحولات استيراد AutoTokenizer

برنامج Tokenizer = AutoTokenizer.from_pretrained ("bert-base-cased")

تسلسل = "استخدام شبكة المحول بسيط"
الرموز = برنامج Tokenizer. tokenize (تسلسل)

طباعة (الرموز)
```

الناتج من هذه الطريقة هو قائمة من السلاسل، أو الرموز:

```بايثون
['Using'، 'a'، 'transform'، '##er'، 'network'، 'is'، 'simple']
```

هذا برنامج Tokenizer هو برنامج Tokenizer فرعي: فهو يقسم الكلمات حتى يحصل على رموز يمكن تمثيلها بمعجمه. هذا هو الحال هنا مع `المحول`، والذي يتم تقسيمه إلى رمزين: `التحويل` و `##er`.

### من الرموز إلى معرفات الإدخال

يتعامل برنامج Tokenizer مع التحويل إلى معرفات الإدخال باستخدام طريقة `convert_tokens_to_ids()` لبرنامج Tokenizer:

```بايثون
معرفات = برنامج Tokenizer. convert_tokens_to_ids (الرموز)

طباعة (معرفات)
```

```بايثون
[7993، 170، 11303، 1200، 2443، 1110، 3014]
```

يمكن استخدام هذه المخرجات، بمجرد تحويلها إلى tensor الإطار المناسب، كإدخالات لنموذج كما هو موضح سابقًا في هذا الفصل.

✏️ **جربه!** كرر الخطوتين الأخيرتين (Tokenization والتحويل إلى معرفات الإدخال) على جمل الإدخال التي استخدمناها في القسم 2 ("كنت أنتظر دورة HuggingFace طوال حياتي." و "أكره هذا كثيرًا!") تحقق من حصولك على نفس معرفات الإدخال التي حصلنا عليها سابقًا!
## فك التشفير

*فك التشفير* هو الذهاب في الاتجاه المعاكس: من فهرس المفردات، نريد الحصول على سلسلة. يمكن القيام بذلك باستخدام طريقة `decode()` كما يلي:

```py
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)
```

```python out
'Using a Transformer network is simple'
```

لاحظ أن طريقة `decode` لا تحول المؤشرات مرة أخرى إلى رموز فحسب، بل تجمع أيضًا الرموز التي كانت جزءًا من الكلمات نفسها لإنتاج جملة مقروءة. سيكون هذا السلوك مفيدًا للغاية عندما نستخدم نماذج تتنبأ بنص جديد (سواء كان النص مولدًا من موجه أو لمشكلات التسلسل إلى تسلسل مثل الترجمة أو الملخص).

الآن، يجب أن تفهم العمليات الذرية التي يمكن أن يتعامل معها المرمز: التمييز، والتحويل إلى معرفات، وتحويل المعرفات مرة أخرى إلى سلسلة. ومع ذلك، فقد قمنا للتو بتجريف الجزء السفلي من الجبل الجليدي. في القسم التالي، سنأخذ نهجنا إلى حدوده ونلقي نظرة على كيفية التغلب عليها.