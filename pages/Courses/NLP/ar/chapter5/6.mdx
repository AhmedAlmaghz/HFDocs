# البحث الدلالي باستخدام FAISS

في [القسم 5](/course/chapter5/5)، قمنا بإنشاء مجموعة بيانات من قضايا GitHub والتعليقات من مستودع 🤗 Datasets. في هذا القسم، سنستخدم هذه المعلومات لبناء محرك بحث يمكنه مساعدتنا في العثور على إجابات لأسئلتنا الأكثر إلحاحًا حول المكتبة!

## استخدام التضمين للبحث الدلالي

كما رأينا في [الفصل 1](/course/chapter1)، تقوم نماذج اللغة القائمة على المحول بتمثيل كل رمز في جزء من النص على أنه _متجه تضمين_. اتضح أنه يمكن "تجمع" التضمينات الفردية لإنشاء تمثيل متجه للجمل أو الفقرات أو (في بعض الحالات) المستندات. بعد ذلك، يمكن استخدام هذه التضمينات للعثور على مستندات مماثلة في الفهرس عن طريق حساب تشابه النقاط (أو بعض مقاييس التشابه الأخرى) بين كل تضمين وإرجاع المستندات ذات التداخل الأكبر.

في هذا القسم، سنستخدم التضمين لتطوير محرك بحث دلالي. توفر محركات البحث هذه العديد من المزايا على الأساليب التقليدية التي تعتمد على مطابقة الكلمات الرئيسية في الاستعلام بالمستندات.

## تحميل مجموعة البيانات والتحضير لها

أول شيء نحتاج إلى فعله هو تنزيل مجموعة بيانات قضايا GitHub الخاصة بنا، لذا دعنا نستخدم الدالة `load_dataset()` كما هو معتاد:

```py
from datasets import load_dataset

issues_dataset = load_dataset("lewtun/github-issues", split="train")
issues_dataset
```

```python out
Dataset({
features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_
```

لقد حددنا هنا الانقسام الافتراضي `train` في `load_dataset()`، لذا فإنه يعيد `Dataset` بدلاً من `DatasetDict`. أولوية العمل هي تصفية طلبات السحب، حيث نادرًا ما يتم استخدامها للإجابة على استفسارات المستخدمين وستؤدي إلى إدخال ضوضاء في محرك البحث الخاص بنا. كما هو مألوف الآن، يمكننا استخدام دالة `Dataset.filter()` لاستبعاد هذه الصفوف في مجموعة البيانات الخاصة بنا. أثناء قيامنا بذلك، دعنا نقوم أيضًا بتصفية الصفوف التي لا تحتوي على تعليقات، حيث لا توفر إجابات على استفسارات المستخدمين:

```py
issues_dataset = issues_dataset.filter(
lambda x: (x["is_pull_request"] == False and len(x["comments"]) > 0)
)
issues_dataset
```

```python out
Dataset({
features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
num_rows: 771
})
```

يمكننا أن نرى أن هناك العديد من الأعمدة في مجموعة البيانات الخاصة بنا، معظمها لا نحتاجها لبناء محرك البحث الخاص بنا. من منظور البحث، فإن أكثر الأعمدة إفادة هي `title`، و`body`، و`comments`، في حين أن `html_url` يوفر لنا رابطًا يعيدنا إلى مصدر القضية. دعنا نستخدم دالة `Dataset.remove_columns()` لإسقاط الباقي:

```py
columns = issues_dataset.column_names
columns_to_keep = ["title", "body", "html_url", "comments"]
columns_to_remove = set(columns_to_keep).symmetric_difference(columns)
issues_dataset = issues_dataset.remove_columns(columns_to_remove)
issues_dataset
```

```python out
Dataset({
features: ['html_url', 'title', 'comments', 'body'],
num_rows: 771
})
```

لإنشاء التضمينات الخاصة بنا، سنقوم بزيادة كل تعليق مع عنوان القضية ووصفها، حيث غالبًا ما تتضمن هذه الحقول معلومات سياقية مفيدة. نظرًا لأن عمود `comments` الخاص بنا هو حاليًا قائمة من التعليقات لكل قضية، فنحن بحاجة إلى "تفجير" العمود بحيث يتكون كل صف من رابطة `(html_url، title، body، comment)`. في Pandas، يمكننا القيام بذلك باستخدام دالة [`DataFrame.explode()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html)، والتي تقوم بإنشاء صف جديد لكل عنصر في عمود يشبه القائمة، مع تكرار جميع قيم الأعمدة الأخرى. لمشاهدة ذلك في العمل، دعنا نتحول أولاً إلى تنسيق `DataFrame` لـ Pandas:

```py
issues_dataset.set_format("pandas")
df = issues_dataset[:]
```

إذا قمنا بفحص الصف الأول في هذا `DataFrame`، فيمكننا أن نرى أنه توجد أربعة تعليقات مرتبطة بهذه القضية:

```py
df["comments"][0].tolist()
```

```python out
['the bug code locate in ：
if data_args.task_name is not None:
# Downloading and loading a dataset from the hub.
datasets = load_dataset("glue", data_args.task_name, cache_dir=model_args.cache_dir)',
'Hi @jinec,

From time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com

Normally, it should work if you wait a little and then retry.

Could you please confirm if the problem persists?',
'cannot connect،even by Web browser،please check that there is some problems.',
'I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...']
```

عندما نقوم بتفجير `df`، نتوقع الحصول على صف واحد لكل من هذه التعليقات. دعنا نتحقق مما إذا كان هذا هو الحال:

```py
comments_df = df.explode("comments", ignore_index=True)
comments_df.head(4)
```

<table border="1" class="dataframe" style="table-layout: fixed; word-wrap:break-word; width: 100%;">
<thead>
<tr style="text-align: right;">
<th></th>
<th>html_url</th>
<th>title</th>
<th>comments</th>
<th>body</th>
</tr>
</thead>
<tbody>
<tr>
<th>0</th>
<td>https://github.com/huggingface/datasets/issues/2787</td>
<td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
<td>the bug code locate in  :
if data_args.task_name is not None ...</td>
<td>Hello,
I am trying to run run_glue.py and it gives me this error ...</td>
</tr>
<tr>
<th>1</th>
<td>https://github.com/huggingface/datasets/issues/2787</td>
<td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
<td>Hi @jinec,

From time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com ...</td>
<td>Hello,
I am trying to run run_glue.py and it gives me this error ...</td>
</tr>
<tr>
<th>2</th>
<td>https://github.com/huggingface/datasets/issues/2787</td>
<td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
<td>cannot connect،even by Web browser،please check that there is some problems.</td>
<td>Hello,
I am trying to run run_glue.py and it gives me this error ...</td>
</tr>
<tr>
<th>3</th>
<td>https://github.com/huggingface/datasets/issues/2787</td>
<td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
<td>I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem ...</td>
<td>Hello,
I am trying to run run_glue.py and it gives me this error ...</td>
</tr>
</tbody>
</table>

رائع، يمكننا أن نرى أن الصفوف قد تم تكرارها، مع احتواء عمود "التعليقات" على التعليقات الفردية! الآن بعد أن انتهينا من Pandas، يمكننا العودة بسرعة إلى "مجموعة بيانات" عن طريق تحميل "إطار البيانات" في الذاكرة:

```py
from datasets import Dataset

comments_dataset = Dataset.from_pandas(comments_df)
comments_dataset
```

```python out
Dataset({
features: ['html_url', 'title', 'comments', 'body'],
num_rows: 2842
})
```

حسنًا، لقد أعطانا هذا بضعة آلاف من التعليقات للعمل بها!

✏️ **جربه!** انظر إذا كنت تستطيع استخدام `Dataset.map()` لتفجير عمود "التعليقات" من `issues_dataset` _بدون_ اللجوء إلى استخدام Pandas. هذا أمر صعب بعض الشيء؛ قد تجد قسم ["التخطيط الدفعي"](https://huggingface.co/docs/datasets/about_map_batch#batch-mapping) من وثائق 🤗 Datasets مفيدًا لهذه المهمة.

الآن بعد أن أصبح لدينا تعليق واحد لكل صف، دعنا نقوم بإنشاء عمود جديد يسمى `comment_length` يحتوي على عدد الكلمات لكل تعليق:

```py
comments_dataset = comments_dataset.map(
lambda x: {"comment_length": len(x["comments"].split())}
)
```

يمكننا استخدام هذا العمود الجديد لتصفية التعليقات القصيرة، والتي غالبًا ما تتضمن أشياء مثل "cc @lewtun" أو "شكرًا!" والتي لا تتعلق بمحرك البحث الخاص بنا. لا يوجد رقم دقيق للاختيار من أجل التصفية، ولكن حوالي 15 كلمة تبدو بداية جيدة:

```py
comments_dataset = comments_dataset.filter(lambda x: x["comment_length"] > 15)
comments_dataset
```

```python out
Dataset({
features: ['html_url', 'title', 'comments', 'body', 'comment_length'],
num_rows: 2098
})
```

بعد تنظيف مجموعة البيانات الخاصة بنا قليلاً، دعنا نقوم بدمج عنوان القضية والوصف والتعليقات معًا في عمود جديد يسمى `text`. كما هو معتاد، سنقوم بكتابة دالة بسيطة يمكننا تمريرها إلى `Dataset.map()`:

```py
def concatenate_text(examples):
return {
"text": examples["title"]
+ " \n "
+ examples["body"]
+ " \n "
+ examples["comments"]
}

comments_dataset = comments_dataset.map(concatenate_text)
```

أخيرًا، نحن مستعدون لإنشاء بعض التضمينات! دعنا نلقي نظرة.
## إنشاء تضمين نصي [[creating-text-embeddings]]

رأينا في [الفصل 2](/course/chapter2) أنه يمكننا الحصول على تضمينات الرموز باستخدام فئة `AutoModel`. كل ما نحتاج إلى فعله هو اختيار نقطة تفتيش مناسبة لتحميل النموذج منها. لحسن الحظ، هناك مكتبة تسمى `sentence-transformers` مخصصة لإنشاء التضمينات. كما هو موضح في [وثائق] (https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search) المكتبة، فإن حالتنا الاستخدامية هي مثال على _البحث الدلالي غير المتماثل_ لأن لدينا استعلامًا قصيرًا نرغب في العثور على إجابته في وثيقة أطول، مثل تعليق على إصدار. يشير جدول [نظرة عامة على النموذج] (https://www.sbert.net/docs/pretrained_models.html#model-overview) المفيد في الوثائق إلى أن نقطة تفتيش `multi-qa-mpnet-base-dot-v1` لديها أفضل أداء للبحث الدلالي، لذا فسنستخدم ذلك لتطبيقنا. سنقوم أيضًا بتحميل الرموز باستخدام نفس نقطة التفتيش:

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, AutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)
```

لتسريع عملية التضمين، من المفيد وضع النموذج والمدخلات على جهاز GPU، لذا دعنا نفعل ذلك الآن:

```py
import torch

device = torch.device("cuda")
model.to(device)
```

{:else}

```py
from transformers import AutoTokenizer, TFAutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = TFAutoModel.from_pretrained(model_ckpt, from_pt=True)
```

لاحظ أننا قمنا بتعيين `from_pt=True` كحجة لطريقة `from_pretrained()`. ويرجع ذلك إلى أن نقطة تفتيش `multi-qa-mpnet-base-dot-v1` تحتوي فقط على أوزان PyTorch، لذا فإن تعيين `from_pt=True` سيحولها تلقائيًا إلى تنسيق TensorFlow بالنسبة لنا. كما ترون، من السهل جدًا التبديل بين الأطر في 🤗 Transformers!

{/if}

كما ذكرنا سابقًا، نرغب في تمثيل كل إدخال في مجموعة بيانات GitHub الخاصة بنا على أنها متجه واحد، لذلك نحتاج إلى "تجمع" متوسطات تضمينات الرموز بطريقة ما. أحد الأساليب الشائعة هو تنفيذ *تجمع CLS* على نواتج النموذج، حيث نقوم ببساطة بجمع حالة الإخفاء الأخيرة للرمز الخاص `[CLS]`. تقوم الدالة التالية بالخدعة لنا:

```py
def cls_pooling(model_output):
    return model_output.last_hidden_state[:, 0]
```

بعد ذلك، سنقوم بإنشاء دالة مساعدة ستقوم برمزية قائمة من المستندات، ووضع المنسوجات على GPU، وإطعامها للنموذج، وأخيرًا تطبيق تجميع CLS على النواتج:

{#if fw === 'pt'}

```py
def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="pt"
    )
    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)
```

يمكننا اختبار وظيفة العمل عن طريق تغذيتها بإدخال النص الأول في مجموعة البيانات الخاصة بنا وفحص شكل الإخراج:

```py
embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape
```

```python out
torch.Size([1, 768])
```

رائع، لقد قمنا بتحويل الإدخال الأول في مجموعة البيانات الخاصة بنا إلى متجه 768-الأبعاد! يمكننا استخدام `Dataset.map()` لتطبيق وظيفة `get_embeddings()` على كل صف في مجموعة البيانات الخاصة بنا، لذا دعنا نقوم بإنشاء عمود "تضمينات" جديد كما يلي:

```py
embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).detach().cpu().numpy()[0]}
)
```

{:else}

```py
def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="tf"
    )
    encoded_input = {k: v for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)
```

يمكننا اختبار وظيفة العمل عن طريق تغذيتها بإدخال النص الأول في مجموعة البيانات الخاصة بنا وفحص شكل الإخراج:

```py
embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape
```

```python out
TensorShape([1, 768])
```

رائع، لقد قمنا بتحويل الإدخال الأول في مجموعة البيانات الخاصة بنا إلى متجه 768-الأبعاد! يمكننا استخدام `Dataset.map()` لتطبيق وظيفة `get_embeddings()` على كل صف في مجموعة البيانات الخاصة بنا، لذا دعنا نقوم بإنشاء عمود "تضمينات" جديد كما يلي:

```py
embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).numpy()[0]}
)
```

{/if}

لاحظ أننا قمنا بتحويل التضمينات إلى مصفوفات NumPy - ويرجع ذلك إلى أن 🤗 Datasets تتطلب هذا التنسيق عند محاولة فهرستها باستخدام FAISS، والتي سنقوم بها بعد ذلك.

## استخدام FAISS للبحث عن التشابه بكفاءة [[using-faiss-for-efficient-similarity-search]]

الآن بعد أن أصبح لدينا مجموعة بيانات من التضمينات، نحتاج إلى طريقة للبحث فيها. للقيام بذلك، سنستخدم بنية بيانات خاصة في 🤗 Datasets تسمى _FAISS index_. [FAISS] (https://faiss.ai/) (اختصار لـ Facebook AI Similarity Search) هي مكتبة توفر خوارزميات فعالة للبحث عن مجموعات التضمين وتجميعها بسرعة.

الفكرة الأساسية وراء FAISS هي إنشاء بنية بيانات خاصة تسمى _index_ والتي تتيح العثور على التضمينات المتشابهة مع تضمين الإدخال. إنشاء فهرس FAISS في 🤗 Datasets أمر بسيط - نستخدم وظيفة `Dataset.add_faiss_index()` ونحدد عمود مجموعة البيانات الذي نرغب في فهرسته:

```py
embeddings_dataset.add_faiss_index(column="embeddings")
```

يمكننا الآن إجراء استعلامات على هذا الفهرس عن طريق إجراء بحث عن أقرب جار باستخدام وظيفة `Dataset.get_nearest_examples()`. دعنا نجرب ذلك عن طريق تضمين سؤال كما يلي:

{#if fw === 'pt'}

```py
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).cpu().detach().numpy()
question_embedding.shape
```

```python out
torch.Size([1, 768])
```

{:else}

```py
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).numpy()
question_embedding.shape
```

```python out
(1, 768)
```

{/if}

مثل المستندات، لدينا الآن متجه 768-الأبعاد يمثل الاستعلام، والذي يمكننا مقارنته بمجموعة البيانات بأكملها للعثور على أكثر التضمينات تشابهًا:

```py
scores, samples = embeddings_dataset.get_nearest_examples(
    "embeddings", question_embedding, k=5
)
```

تُرجع وظيفة `Dataset.get_nearest_examples()` زوجًا من الدرجات التي تصنف التداخل بين الاستعلام والمستند، ومجموعة مقابلة من العينات (أفضل 5 مباريات هنا). دعنا نجمعها في `pandas.DataFrame` حتى نتمكن من فرزها بسهولة:

```py
import pandas as pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df["scores"] = scores
samples_df.sort_values("scores", ascending=False, inplace=True)
```

الآن يمكننا التكرار فوق الصفوف القليلة الأولى لمعرفة مدى تطابق الاستعلام مع التعليقات المتاحة:

```py
for _, row in samples_df.iterrows():
    print(f"COMMENT: {row.comments}")
    print(f"SCORE: {row.scores}")
    print(f"TITLE: {row.title}")
    print(f"URL: {row.html_url}")
    print("=" * 50)
    print()
```

```python out
"""
COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.

@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?
SCORE: 25.505046844482422
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)
You can now use them offline
\`\`\`python
datasets = load_dataset("text", data_files=data_files)
\`\`\`

We'll do a new release soon
SCORE: 24.555509567260742
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.

Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :)

I already note the "freeze" modules option, to prevent local modules updates. It would be a cool feature.

----------

> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?

Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.
For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do
\`\`\`python
load_dataset("./my_dataset")
\`\`\`
and the dataset script will generate your dataset once and for all.

----------

About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.
cf #1724
SCORE: 24.14896583557129
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine
>
> 1. (online machine)
>
> ```
>
> import datasets
>
> data = datasets.load_dataset(...)
>
> data.save_to_disk(/YOUR/DATASET/DIR)
>
> ```
>
> 2. copy the dir from online to the offline machine
>
> 3. (offline machine)
>
> ```
>
> import datasets
>
> data = datasets.load_from_disk(/SAVED/DATA/DIR)
>
> ```
>
>
>
> HTH.


SCORE: 22.893993377685547
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: here is my way to load a dataset offline, but it **requires** an online machine
1. (online machine)
\`\`\`
import datasets
data = datasets.load_dataset(...)
data.save_to_disk(/YOUR/DATASET/DIR)
\`\`\`
2. copy the dir from online to the offline machine
3. (offline machine)
\`\`\`
import datasets
data = datasets.load_from_disk(/SAVED/DATA/DIR)
\`\`\`

HTH.
SCORE: 22.406635284423828
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================
"""
```

ليس سيئا! يبدو أن إصابتنا الثانية تتطابق مع الاستعلام.

<Tip>

✏️ **جربه!** قم بإنشاء استعلامك الخاص وشاهد ما إذا كان بإمكانك العثور على إجابة في المستندات المستردة. قد تضطر إلى زيادة معلمة `k` في `Dataset.get_nearest_examples()` لتوسيع نطاق البحث.

</Tip>