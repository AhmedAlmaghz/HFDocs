# القوى الخاصة لمقسّمات الرموز السريعة

في هذا القسم، سنلقي نظرة فاحصة على قدرات مقسّمات الرموز في مكتبة 🤗 Transformers. حتى الآن، لم نستخدمها إلا لتقسيم المدخلات إلى رموز أو فك ترميز المعرّفات (IDs) مرة أخرى إلى نص، ولكن يمكن لمقسّمات الرموز - خاصة تلك المدعومة بمكتبة 🤗 Tokenizers - أن تفعل أكثر من ذلك بكثير. ولتوضيح هذه الميزات الإضافية، سنستكشف كيفية إعادة إنتاج نتائج خطوط أنابيب `token-classification` (التي أطلقنا عليها اسم `ner`) و`question-answering` التي واجهناها لأول مرة في [الفصل 1](/course/chapter1).

في المناقشة التالية، سنميز غالبًا بين مقسّمات الرموز "البطيئة" و"السريعة". مقسّمات الرموز البطيئة هي تلك المكتوبة بلغة Python داخل مكتبة 🤗 Transformers، في حين أن الإصدارات السريعة هي تلك التي توفرها مكتبة 🤗 Tokenizers، والمكتوبة بلغة Rust. إذا كنت تتذكر الجدول من [الفصل 5](/course/chapter5/3) الذي أبلغ عن المدة التي استغرقها مقسّم رموز سريع وآخر بطيء لتقسيم مجموعة بيانات مراجعة الدواء، فيجب أن تكون لديك فكرة عن سبب تسميتنا لها بالسريعة والبطيئة:

|          | مقسّم الرموز السريع | مقسّم الرموز البطيء |
| :----: |:----: |:----: |
| `batched=True`  | 10.8s | 4min41s |
| `batched=False` | 59.2s | 5min3s |

|               | مقسّم الرموز السريع | مقسّم الرموز البطيء |
| :--------------:|:--------------:|:-------------:|
| `batched=True`  | 10.8s          | 4min41s |
| `batched=False` | 59.2s          | 5min3s |

تحذير: ⚠️ عند تقسيم جملة واحدة، لن تلاحظ دائمًا اختلافًا في السرعة بين الإصدارات السريعة والبطيئة من نفس مقسّم الرموز. في الواقع، قد يكون الإصدار السريع أبطأ بالفعل! لن تتمكن من رؤية الفرق بوضوح إلا عند تقسيم الكثير من النصوص بشكل متوازٍ في نفس الوقت.

## الترميز الدفعي

الناتج من مقسّم الرموز ليس عبارة عن قاموس Python بسيط؛ ما نحصل عليه في الواقع هو كائن `BatchEncoding` خاص. إنه فئة فرعية من قاموس (وهذا هو السبب في أننا تمكنا من الفهرسة في تلك النتيجة دون أي مشكلة من قبل)، ولكن مع أساليب إضافية تستخدمها بشكل أساسي مقسّمات الرموز السريعة.

بالإضافة إلى قدرات الموازاة، تتمثل الوظيفة الأساسية لمقسّمات الرموز السريعة في أنها تحتفظ دائمًا بتتبع النص الأصلي الذي تأتي منه الرموز النهائية - وهي ميزة نسميها *خريطة الإزاحة*. وهذا بدوره يفتح ميزات مثل رسم خريطة لكل كلمة إلى الرموز التي أنتجتها أو رسم خريطة لكل حرف من النص الأصلي إلى الرمز الذي يقع فيه، والعكس صحيح.

لنلقِ نظرة على مثال:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
encoding = tokenizer(example)
print(type(encoding))
```

كما ذكرنا سابقًا، نحصل على كائن `BatchEncoding` في إخراج مقسّم الرموز:

```python
<class 'transformers.tokenization_utils_base.BatchEncoding'>
```

نظرًا لأن فئة `AutoTokenizer` تختار مقسّم رموز سريعًا بشكل افتراضي، يمكننا استخدام الأساليب الإضافية التي يوفرها كائن `BatchEncoding` هذا. هناك طريقتان للتحقق مما إذا كان مقسّم الرموز الخاص بنا سريعًا أو بطيئًا. يمكننا إما التحقق من سمة `is_fast` من `tokenizer`:

```python
tokenizer.is_fast
```

```python
True
```

أو التحقق من السمة نفسها من `encoding`:

```python
encoding.is_fast
```

```python
True
```

دعونا نرى ما يمكّننا منه مقسّم الرموز السريع. أولاً، يمكننا الوصول إلى الرموز دون الحاجة إلى تحويل المعرّفات (IDs) مرة أخرى إلى رموز:

```py
encoding.tokens()
```

```python
['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',
'Brooklyn', '.', '[SEP]']
```

في هذه الحالة، الرمز في الفهرس 5 هو `##yl`، وهو جزء من كلمة "Sylvain" في الجملة الأصلية. يمكننا أيضًا استخدام طريقة `word_ids()` للحصول على فهرس الكلمة التي يأتي منها كل رمز:

```py
encoding.word_ids()
```

```python
[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]
```

يمكننا أن نرى أن الرموز الخاصة لمقسّم الرموز `[CLS]` و`[SEP]` يتم رسمها إلى `None`، ثم يتم رسم كل رمز إلى الكلمة التي نشأ منها. هذا مفيد بشكل خاص لتحديد ما إذا كان الرمز يقع في بداية كلمة أو إذا كان رمزان في نفس الكلمة. يمكننا الاعتماد على البادئة `##` لهذا الغرض، ولكنها تعمل فقط لمقسّمات رموز BERT-like؛ تعمل هذه الطريقة لأي نوع من مقسّمات الرموز طالما أنها سريعة. في الفصل التالي، سنرى كيف يمكننا استخدام هذه القدرة لتطبيق العلامات التي لدينا لكل كلمة بشكل صحيح على الرموز في مهام مثل التعرف على الكيانات المسماة (NER) ووضع العلامات النحوية (POS). يمكننا أيضًا استخدامه لإخفاء جميع الرموز التي تأتي من نفس الكلمة في النمذجة اللغوية المخفية (تقنية تسمى _whole word masking_).

تعتبر فكرة ما الكلمة معقدة. على سبيل المثال، هل "I'll" (اختصار لـ "I will") تعتبر كلمة واحدة أو كلمتين؟ في الواقع، يعتمد ذلك على مقسّم الرموز وعملية ما قبل التقسيم التي يطبقها. يقوم بعض مقسّمات الرموز بالتقسيم فقط حسب المسافات، لذا فهي ستعتبرها كلمة واحدة. يستخدم البعض الآخر علامات الترقيم بالإضافة إلى المسافات، لذا فسيتم اعتبارها كلمتين.

جربها بنفسك: قم بإنشاء مقسّم رموز من نقاط التفتيش `bert-base-cased` و`roberta-base` وقم بتقسيم "81s" باستخدامها. ماذا تلاحظ؟ ما هي معرّفات الكلمات؟

وبالمثل، هناك طريقة `sentence_ids()` يمكننا استخدامها لرسم خريطة للرمز إلى الجملة التي جاء منها (على الرغم من أنه في هذه الحالة، يمكن أن تعطينا `token_type_ids` التي يعيدها مقسّم الرموز نفس المعلومات).

أخيرًا، يمكننا رسم خريطة لأي كلمة أو رمز إلى الأحرف في النص الأصلي، والعكس صحيح، عبر طرق `word_to_chars()` أو `token_to_chars()` و`char_to_word()` أو `char_to_token()`. على سبيل المثال، أخبرتنا طريقة `word_ids()` أن `##yl` هي جزء من الكلمة في الفهرس 3، ولكن أي كلمة هي في الجملة؟ يمكننا معرفة ذلك بهذه الطريقة:

```py
start, end = encoding.word_to_chars(3)
example[start:end]
```

```python
Sylvain
```

كما ذكرنا سابقًا، يتم تشغيل كل ذلك من خلال حقيقة أن مقسّم الرموز السريع يحتفظ بتتبع النص الذي يأتي منه كل رمز في قائمة *الإزاحات*. لتوضيح استخدامها، سنريكم بعد ذلك كيفية إعادة إنتاج نتائج خط أنابيب `token-classification` يدويًا.

جربها بنفسك: قم بإنشاء مثال نصي خاص بك وانظر إذا كنت تستطيع فهم الرموز المرتبطة بمعرّف الكلمة، وكذلك كيفية استخراج نطاقات الأحرف لكلمة واحدة. للحصول على نقاط المكافأة، جرّب استخدام جملتين كإدخال وتحقق مما إذا كانت معرّفات الجمل منطقية بالنسبة لك.

## داخل خط أنابيب `token-classification`

في [الفصل 1](/course/chapter1)، حصلنا على أول تذوق لتطبيق NER - حيث تتمثل المهمة في تحديد الأجزاء من النص التي تتوافق مع الكيانات مثل الأشخاص أو المواقع أو المنظمات - باستخدام وظيفة `pipeline()` في مكتبة 🤗 Transformers. ثم في [الفصل 2](/course/chapter2)، رأينا كيف يقوم خط الأنابيب بتجميع المراحل الثلاث الضرورية للحصول على التنبؤات من نص خام: التقسيم إلى رموز، وتمرير الإدخالات عبر النموذج، والمعالجة اللاحقة. الخطوتان الأوليان في خط أنابيب `token-classification` هما نفس الخطوتين الموجودتين في أي خط أنابيب آخر، ولكن المعالجة اللاحقة أكثر تعقيدًا بعض الشيء - دعونا نرى كيف!

أولاً، دعونا نحصل على خط أنابيب تصنيف الرموز حتى نتمكن من الحصول على بعض النتائج لمقارنتها يدويًا. النموذج المستخدم بشكل افتراضي هو [`dbmdz/bert-large-cased-finetuned-conll03-english`](https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)؛ إنه يؤدي NER على الجمل:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
{'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
{'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
{'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
{'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
{'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
{'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
{'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

قام النموذج بتحديد كل رمز تم إنشاؤه بواسطة "Sylvain" بشكل صحيح على أنه شخص، وكل رمز تم إنشاؤه بواسطة "Hugging Face" على أنه منظمة، والرمز "Brooklyn" على أنه موقع. يمكننا أيضًا أن نطلب من خط الأنابيب تجميع الرموز التي تتوافق مع نفس الكيان:

```py
from transformers import pipeline

token_classifier = pipeline("token-classification", aggregation_strategy="simple")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
{'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
{'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

ستغير استراتيجية التجميع المحددة الدرجات المحسوبة لكل كيان مجمع. مع `"simple"`، يكون الدرجات ببساطة متوسط الدرجات لكل رمز في الكيان المعطى: على سبيل المثال، تكون درجة "Sylvain" هي متوسط الدرجات التي رأيناها في المثال السابق للرموز `S`، `##yl`، `##va`، و`##in`. الاستراتيجيات الأخرى المتاحة هي:

- `"first"`، حيث تكون درجة كل كيان هي درجة الرمز الأول من ذلك الكيان (لذا بالنسبة لـ "Sylvain" ستكون 0.993828، درجة الرمز `S`)
- `"max"`، حيث تكون درجة كل كيان هي الدرجة القصوى للرموز في ذلك الكيان (لذا بالنسبة لـ "Hugging Face" ستكون 0.98879766، درجة "Face")
- `"average"`، حيث تكون درجة كل كيان هي متوسط درجات الكلمات التي يتكون منها ذلك الكيان (لذا بالنسبة لـ "Sylvain" لن يكون هناك اختلاف عن استراتيجية `"simple"`، ولكن "Hugging Face" ستكون لها درجة 0.9819، متوسط الدرجات لـ "Hugging"، 0.975، و"Face"، 0.98879)

والآن دعونا نرى كيف نحصل على هذه النتائج دون استخدام وظيفة `pipeline()`!
### من المدخلات إلى التنبؤات

أولاً، نحتاج إلى تحويل مدخلاتنا إلى رموز مرورها عبر النموذج. يتم ذلك بالضبط كما في [الفصل 2](/course/chapter2)؛ نقوم بتنفيذ محول الرموز والنموذج باستخدام فئات `AutoXxx` ثم نستخدمها في مثالنا:

```py
from transformers import AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="pt")
outputs = model(**inputs)
```

نظرًا لأننا نستخدم `AutoModelForTokenClassification` هنا، فإننا نحصل على مجموعة واحدة من اللوغاريتمات لكل رمز في تسلسل الإدخال:

```py
print(inputs["input_ids"].shape)
print(outputs.logits.shape)
```

```python out
torch.Size([1, 19])
torch.Size([1, 19, 9])
```

نحن لدينا دفعة واحدة تتكون من تسلسل واحد من 19 رمزًا، وللنموذج 9 تسميات مختلفة، لذا فإن ناتج النموذج له شكل 1 × 19 × 9. كما هو الحال في خط أنابيب تصنيف النص، نستخدم دالة softmax لتحويل هذه اللوغاريتمات إلى احتمالات، ونأخذ argmax للحصول على تنبؤات (لاحظ أنه يمكننا أخذ argmax على اللوغاريتمات لأن softmax لا تغير الترتيب):

```py
import torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()
predictions = outputs.logits.argmax(dim=-1)[0].tolist()
print(predictions)
```

```python out
[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]
```

يحتوي atribut `model.config.id2label` على خريطة الفهارس إلى التسميات التي يمكننا استخدامها لفهم التنبؤات:

```py
model.config.id2label
```

```python out
{0: 'O',
1: 'B-MISC',
2: 'I-MISC',
3: 'B-PER',
4: 'I-PER',
5: 'B-ORG',
6: 'I-ORG',
7: 'B-LOC',
8: 'I-LOC'}
```

كما رأينا سابقًا، هناك 9 تسميات: `O` هي التسمية للرموز التي لا توجد في أي كيان مسمى (تقف على "خارج")، ثم لدينا تسميتان لكل نوع من الكيانات (متنوعة، شخص، منظمة، وموقع). تشير التسمية `B-XXX` إلى أن الرمز يقع في بداية كيان `XXX` وتشير التسمية `I-XXX` إلى أن الرمز يقع داخل الكيان `XXX`. على سبيل المثال، في المثال الحالي، نتوقع أن يصنف النموذج الرمز `S` على أنه `B-PER` (بداية كيان الشخص) والرموز `##yl`، `##va` و`##in` كـ `I-PER` (داخل كيان الشخص).

قد تعتقد أن النموذج كان خاطئًا في هذه الحالة لأنه أعطى التسمية `I-PER` لجميع هذه الرموز الأربعة، لكن هذا ليس صحيحًا تمامًا. في الواقع، هناك نسقان لهذه التسميات `B-` و`I-`: *IOB1* و*IOB2*. تنسيق IOB2 (باللون الوردي أدناه)، هو التنسيق الذي قدمناه في حين أنه في تنسيق IOB1 (باللون الأزرق)، لا تُستخدم التسميات التي تبدأ بـ `B-` مطلقًا إلا لفصل كيانين متجاورين من نفس النوع. تم ضبط النموذج الذي نستخدمه على مجموعة بيانات باستخدام هذا التنسيق، وهو السبب في تعيينه التسمية `I-PER` إلى الرمز `S`.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg" alt="IOB1 مقابل IOB2 format"/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg" alt="IOB1 مقابل IOB2 format"/>
</div>

مع هذه الخريطة، نحن مستعدون لتكرار (بأكملها تقريبًا) نتائج خط الأنابيب الأول - يمكننا فقط الحصول على النتيجة والتسمية لكل رمز لم يتم تصنيفه على أنه `O`:

```py
results = []
tokens = inputs.tokens()

for idx, pred in enumerate(predictions):
label = model.config.id2label[pred]
if label != "O":
results.append(
{"entity": label, "score": probabilities[idx][pred], "word": tokens[idx]}
)

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},
{'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},
{'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},
{'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},
{'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},
{'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},
{'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},
{'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]
```

هذا مشابه جدًا لما كان لدينا من قبل، باستثناء واحد: قدم لنا خط الأنابيب أيضًا معلومات حول `start` و`end` لكل كيان في الجملة الأصلية. هنا يأتي دور خريطة الإزاحة. للحصول على الإزاحات، ما علينا سوى تعيين `return_offsets_mapping=True` عند تطبيق محول الرموز على مدخلاتنا:

```py
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"]
```

```python out
[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),
(33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]
```

كل زوج هو امتداد النص المقابل لكل رمز، حيث يتم حجز `(0، 0)` للرموز الخاصة. رأينا سابقًا أن الرمز في الفهرس 5 هو `##yl`، والذي له `(12، 14)` كإزاحات هنا. إذا حصلنا على الشريحة المقابلة في مثالنا:

```py
example[12:14]
```

نحصل على امتداد النص الصحيح بدون `##`:

```python out
yl
```

باستخدام هذا، يمكننا الآن استكمال النتائج السابقة:

```py
results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

for idx, pred in enumerate(predictions):
label = model.config.id2label[pred]
if label != "O":
start, end = offsets[idx]
results.append(
{
"entity": label,
"score": probabilities[idx][pred],
"word": tokens[idx],
"start": start,
"end": end,
}
)

print(results)
```

```python out
[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
{'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
{'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
{'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
{'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
{'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
{'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
{'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

هذا هو نفسه ما حصلنا عليه من خط الأنابيب الأول!

### تجميع الكيانات

إن استخدام الإزاحات لتحديد مفاتيح البداية والنهاية لكل كيان أمر مفيد، ولكن هذه المعلومات ليست ضرورية تمامًا. ومع ذلك، عندما نريد تجميع الكيانات معًا، فإن الإزاحات ستوفر علينا الكثير من التعليمات البرمجية غير المرتبة. على سبيل المثال، إذا أردنا تجميع الرموز `Hu`، `##gging`، و`Face`، فيمكننا وضع قواعد خاصة تنص على أنه يجب إرفاق الأولين أثناء إزالة `##`، ويجب إضافة `Face` مع مسافة لأنه لا يبدأ بـ `##` - ولكن ذلك لن ينجح إلا لهذا النوع المحدد من محول الرموز. سيتعين علينا كتابة مجموعة أخرى من القواعد لمحول جمل أو محول ترميز Byte-Pair (سيتم مناقشته لاحقًا في هذا الفصل).

مع الإزاحات، تختفي كل هذه التعليمات البرمجية المخصصة: يمكننا ببساطة أخذ الامتداد في النص الأصلي الذي يبدأ بالرمز الأول وينتهي بالرمز الأخير. لذلك، في حالة الرموز `Hu`، `##gging`، و`Face`، يجب أن نبدأ عند حرف 33 (بداية `Hu`) وننتهي قبل حرف 45 (نهاية `Face`):

```py
example[33:45]
```

```python out
Hugging Face
```

للكتابة التعليمات البرمجية التي تقوم بمعالجة التنبؤات بعد تجميع الكيانات، سنقوم بتجميع الكيانات التي تكون متتالية وموسومة بـ `I-XXX`، باستثناء الأول، والذي يمكن أن يكون موسومًا بـ `B-XXX` أو `I-XXX` (لذا، نتوقف عن تجميع الكيان عندما نحصل على `O`، أو نوع جديد من الكيان، أو `B-XXX` الذي يخبرنا بأن كيانًا من نفس النوع بدأ للتو):

```py
import numpy as np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

idx = 0
while idx < len(predictions):
pred = predictions[idx]
label = model.config.id2label[pred]
if label != "O":
# Remove the B- or I-
label = label[2:]
start, _ = offsets[idx]

# Grab all the tokens labeled with I-label
all_scores = []
while (
idx < len(predictions)
and model.config.id2label[predictions[idx]] == f"I-{label}"
):
all_scores.append(probabilities[idx][pred])
_, end = offsets[idx]
idx += 1

# The score is the mean of all the scores of the tokens in that grouped entity
score = np.mean(all_scores).item()
word = example[start:end]
results.append(
{
"entity_group": label,
"score": score,
"word": word,
"start": start,
"end": end,
}
)
idx += 1

print(results)
```

ونحصل على نفس النتائج كما هو الحال مع خط أنابيبنا الثاني!

```python out
[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
{'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
{'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

مثال آخر على مهمة تكون فيها هذه الإزاحات مفيدة للغاية هو الإجابة على الأسئلة. الغوص في خط أنابيب الإجابة على الأسئلة، والذي سنقوم به في القسم التالي، سيمكننا أيضًا من إلقاء نظرة على ميزة محولات الرموز الأخيرة في مكتبة 🤗 Transformers: التعامل مع الرموز المتدفقة عند اقتطاع إدخال إلى طول معين.