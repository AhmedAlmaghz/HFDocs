# تكوين الرموز الاحادية [[unigram-tokenization]]

يستخدم خوارزم Unigram غالباً في SentencePiece، وهو خوارزمية التجزئة التي تستخدمها نماذج مثل AlBERT و T5 و mBART و Big Bird و XLNet.

## خوارزمية التدريب [ [training-algorithm]]

بالمقارنة مع BPE و WordPiece، يعمل Unigram في الاتجاه المعاكس: فهو يبدأ من مفردات كبيرة ويزيل الرموز منها حتى يصل إلى الحجم المرغوب للمفردات. هناك عدة خيارات يمكن استخدامها لبناء تلك المفردات الأساسية: يمكننا أخذ أكثر السلاسل الفرعية شيوعًا في الكلمات المقطعة مسبقًا، على سبيل المثال، أو تطبيق BPE على المجموعة الأولية بمفردات كبيرة.

في كل خطوة من التدريب، يحسب خوارزم Unigram خسارة على المجموعة المعطاة للمفردات الحالية. ثم، بالنسبة لكل رمز في المفردات، يحسب الخوارزم كيف ستزداد الخسارة الإجمالية إذا تم إزالة الرمز، ويبحث عن الرموز التي ستزيد منها الأقل. هذه الرموز لها تأثير أقل على الخسارة الإجمالية على المجموعة، لذلك فهي "غير مطلوبة" بشكل أقل وهي أفضل المرشحين للإزالة.

هذه عملية مكلفة للغاية، لذلك لا نقوم فقط بإزالة الرمز الفردي المرتبط بأقل زيادة في الخسارة، ولكن أيضًا \\ (ع) (حيث \\ (ع) هو فرط معلمة يمكنك التحكم فيها، عادة 10 أو 20) في المئة من الرموز المرتبطة بأقل زيادة في الخسارة. تكرر هذه العملية حتى تصل المفردات إلى الحجم المطلوب.

لاحظ أننا لا نزيل أبدًا الأحرف الأساسية، للتأكد من إمكانية تجزئة أي كلمة.

الآن، هذا لا يزال غامضًا بعض الشيء: الجزء الرئيسي من الخوارزمية هو حساب الخسارة على المجموعة ورؤية كيف تتغير عند إزالة بعض الرموز من المفردات، لكننا لم نشرح كيفية القيام بذلك بعد. تعتمد هذه الخطوة على خوارزمية التجزئة لنموذج Unigram، لذا سنغوص في هذا الأمر التالي.

سنعيد استخدام المجموعة من الأمثلة السابقة:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

وبالنسبة لهذا المثال، سنأخذ جميع السلاسل الفرعية الصارمة للمفردات الأولية:

```
["h", "u", "g", "hu", "ug", "p", "pu", "n", "un", "b", "bu", "s", "hug", "gs", "ugs"]
```

## خوارزمية التجزئة [ [tokenization-algorithm]]

نموذج Unigram هو نوع من نماذج اللغة التي تعتبر كل رمز مستقلًا عن الرموز التي سبقته. إنه أبسط نموذج للغة، بمعنى أن احتمال الرمز X بالنظر إلى السياق السابق هو ببساطة احتمال الرمز X. لذا، إذا استخدمنا نموذج لغة Unigram لتوليد نص، فسنقوم دائمًا بالتنبؤ بالرمز الأكثر شيوعًا.

احتمال رمز معين هو تكراره (عدد المرات التي نجده فيها) في المجموعة الأصلية، مقسومًا على مجموع جميع تكرارات جميع الرموز في المفردات (للتأكد من أن الاحتمالات تصل إلى 1). على سبيل المثال، يوجد "ug" في "hug" و "pug" و "hugs"، لذلك يبلغ تكراره 20 في مجموعتنا.

فيما يلي تكرارات جميع الرموز الفرعية المحتملة في المفردات:

```
("h", 15) ("u", 36) ("g", 20) ("hu", 15) ("ug", 20) ("p", 17) ("pu", 17) ("n", 16)
("un", 16) ("b", 4) ("bu", 4) ("s", 5) ("hug", 15) ("gs", 5) ("ugs", 5)
```

لذلك، فإن مجموع جميع التكرارات هو 210، واحتمال الرمز الفرعي "ug" هو 20/210.

✏️ **الآن دورك!** اكتب الكود لحساب التكرارات أعلاه وتحقق من أن النتائج المعروضة صحيحة، وكذلك المجموع الكلي.

الآن، لتجزئة كلمة معينة، ننظر في جميع التجزئات المحتملة إلى رموز ونحسب احتمال كل منها وفقًا لنموذج Unigram. نظرًا لأن جميع الرموز تعتبر مستقلة، فإن هذا الاحتمال هو ببساطة حاصل ضرب احتمال كل رمز. على سبيل المثال، فإن تجزئة ["p"، "u"، "g"] لـ "pug" لها الاحتمال:

$$P ([`` p "،" u "،" g "]) = P (`` p ") \ times P (`` u ") \ times P (`` g ") = \ frac {5} {210} \ times \ frac {36} {210} \ times \ frac {20} {210} = 0.000389 $$

بالمقارنة، فإن التجزئة ["pu"، "g"] لها الاحتمال:

$$P ([`` pu "،" g "]) = P (`` pu ") \ times P (`` g ") = \ frac {5} {210} \ times \ frac {20} {210} = 0.0022676 $$

لذا فإن هذا أكثر احتمالًا بكثير. بشكل عام، سيكون للتجزئات التي تحتوي على أقل عدد ممكن من الرموز أعلى احتمال (بسبب ذلك القسمة على 210 المتكررة لكل رمز)، وهو ما يتطابق مع ما نريده بداهة: تقسيم كلمة إلى أقل عدد ممكن من الرموز.

تجزئة كلمة باستخدام نموذج Unigram هي التجزئة ذات الاحتمال الأعلى. في مثال "pug"، فيما يلي الاحتمالات التي سنحصل عليها لكل تجزئة محتملة:

```
["p", "u", "g"] : 0.000389
["p", "ug"] : 0.0022676
["pu", "g"] : 0.0022676
```

لذلك، سيتم تجزئة "pug" على أنها ["p"، "ug"] أو ["pu"، "g"]، اعتمادًا على أي من هذه التجزئات يتم مواجهتها أولاً (لاحظ أنه في مجموعة أكبر، ستكون حالات المساواة مثل هذه نادرة).

في هذه الحالة، كان من السهل العثور على جميع التجزئات المحتملة وحساب احتمالاتها، ولكن بشكل عام سيكون الأمر أكثر صعوبة. هناك خوارزمية كلاسيكية تستخدم لهذا الغرض، تسمى خوارزمية Viterbi. في الأساس، يمكننا بناء رسم بياني للكشف عن التجزئات المحتملة لكلمة معينة من خلال القول بوجود فرع من حرف _a_ إلى حرف _b_ إذا كان الرمز الفرعي من _a_ إلى _b_ موجودًا في المفردات، ونعزو إلى ذلك الفرع احتمال الرمز الفرعي.

للعثور على المسار في هذا الرسم البياني الذي سيكون له أفضل نتيجة، تحدد خوارزمية Viterbi، لكل موضع في الكلمة، التجزئة ذات أفضل نتيجة التي تنتهي عند هذا الموضع. نظرًا لأننا ننتقل من البداية إلى النهاية، يمكن العثور على أفضل نتيجة عن طريق الحلقة عبر جميع الرموز الفرعية التي تنتهي عند الموضع الحالي ثم استخدام أفضل نتيجة للتجزئة من الموضع الذي يبدأ فيه هذا الرمز الفرعي. بعد ذلك، ما علينا سوى فك مسار الوصول إلى النهاية.

دعونا نلقي نظرة على مثال باستخدام مفرداتنا وكلمة "unhug". بالنسبة لكل موضع، فإن الرموز الفرعية ذات أفضل النتائج التي تنتهي هناك هي كما يلي:

```
Character 0 (u): "u" (score 0.171429)
Character 1 (n): "un" (score 0.076191)
Character 2 (h): "un" "h" (score 0.005442)
Character 3 (u): "un" "hu" (score 0.005442)
Character 4 (g): "un" "hug" (score 0.005442)
```

لذلك سيتم تجزئة "unhug" على أنها ["un"، "hug"].

✏️ **الآن دورك!** حدد تجزئة الكلمة "huggun"، ونتيجتها.

## العودة إلى التدريب [ [back-to-training]]

الآن بعد أن رأينا كيف تعمل التجزئة، يمكننا أن نتعمق قليلاً في الخسارة المستخدمة أثناء التدريب. في أي مرحلة معينة، يتم حساب هذه الخسارة عن طريق تجزئة كل كلمة في المجموعة، باستخدام المفردات الحالية ونموذج Unigram الذي تحدده تكرارات كل رمز في المجموعة (كما رأينا سابقًا).

تحتوي كل كلمة في المجموعة على نتيجة، والخسارة هي اللوغاريتم الطبيعي السلبي لهذه النتائج - أي مجموع جميع الكلمات في المجموعة من جميعها `-log(P(word))`.

دعونا نعود إلى مثالنا مع المجموعة التالية:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

تجزئة كل كلمة مع نتائجها هي:

```
"hug": ["hug"] (score 0.071428)
"pug": ["pu", "g"] (score 0.007710)
"pun": ["pu", "n"] (score 0.006168)
"bun": ["bu", "n"] (score 0.001451)
"hugs": ["hug", "s"] (score 0.001701)
```

لذلك فإن الخسارة هي:

```
10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8
```

الآن نحتاج إلى حساب كيف يؤثر إزالة كل رمز على الخسارة. هذا مرهق للغاية، لذا سنقوم بذلك فقط لرموزين هنا ونحفظ العملية بأكملها عندما يكون لدينا رمز لمساعدتنا. في هذه الحالة (المحددة جدًا)، كان لدينا تجزئتان متساويتان لجميع الكلمات: كما رأينا سابقًا، على سبيل المثال، يمكن تجزئة "pug" على أنها ["p"، "ug"] بنفس النتيجة. لذلك، فإن إزالة الرمز "pu" من المفردات سيعطي نفس الخسارة بالضبط.

من ناحية أخرى، فإن إزالة "hug" ستجعل الخسارة أسوأ، لأن تجزئة "hug" و "hugs" ستصبح:

```
"hug": ["hu", "g"] (score 0.006802)
"hugs": ["hu", "gs"] (score 0.001701)
```

ستتسبب هذه التغييرات في ارتفاع الخسارة إلى:

```
- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5
```

لذلك، من المحتمل أن يتم إزالة الرمز "pu" من المفردات، ولكن ليس "hug".
## تطبيق Unigram

الآن دعونا نطبق كل ما رأيناه حتى الآن في الكود. مثل BPE وWordPiece، هذا ليس تطبيقًا فعالًا لخوارزمية Unigram (على العكس تمامًا)، ولكنه يجب أن يساعدك على فهمها بشكل أفضل قليلاً.

سنستخدم نفس المجموعة من النصوص كمثال:

```python
corpus = [
"This is the Hugging Face Course.",
"This chapter is about tokenization.",
"This section shows several tokenizer algorithms.",
"Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

هذه المرة، سنستخدم `xlnet-base-cased` كنموذجنا:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("xlnet-base-cased")
```

كما هو الحال مع BPE وWordPiece، نبدأ بعدد مرات ظهور كل كلمة في المجموعة:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
new_words = [word for word, offset in words_with_offsets]
for word in new_words:
word_freqs[word] += 1

word_freqs
```

بعد ذلك، نحتاج إلى تهيئة مفرداتنا إلى شيء أكبر من حجم المفردات الذي نريده في النهاية. يجب أن ندرج جميع الأحرف الأساسية (وإلا فلن نتمكن من تقسيم كل كلمة إلى رموز)، ولكن بالنسبة إلى السلاسل الفرعية الأكبر، فسنحتفظ فقط بأكثرها شيوعًا، لذا سنقوم بفرزها حسب التردد:

```python
char_freqs = defaultdict(int)
subwords_freqs = defaultdict(int)
for word, freq in word_freqs.items():
for i in range(len(word)):
char_freqs[word[i]] += freq
# Loop through the subwords of length at least 2
for j in range(i + 2, len(word) + 1):
subwords_freqs[word[i:j]] += freq

# Sort subwords by frequency
sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)
sorted_subwords[:10]
```

```python out
[('▁t', 7), ('is', 5), ('er', 5), ('▁a', 5), ('