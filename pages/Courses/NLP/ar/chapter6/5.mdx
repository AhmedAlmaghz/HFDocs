# ترميز الترميز المزدوج Byte-Pair 

Byte-Pair Encoding (BPE) هو في الأصل خوارزمية تم تطويرها لضغط النصوص، ثم استخدمتها OpenAI للترميز المسبق عند التدريب المسبق لنموذج GPT. ويستخدمه العديد من نماذج Transformer، بما في ذلك GPT وGPT-2 وRoBERTa وBART وDeBERTa.

## خوارزمية التدريب

يبدأ التدريب على BPE بحساب مجموعة فريدة من الكلمات المستخدمة في الفيلق (بعد اكتمال خطوات التوحيد والتمهيد)، ثم بناء المفردات من خلال أخذ جميع الرموز المستخدمة لكتابة تلك الكلمات. كمثال بسيط جدًا، دعنا نقول أن فيلقنا يستخدم هذه الكلمات الخمس:

```
"hug", "pug", "pun", "bun", "hugs"
```

ستكون المفردات الأساسية هي إذن `["b"، "g"، "h"، "n"، "p"، "s"، "u"]`. بالنسبة للحالات الواقعية، ستتضمن هذه المفردات الأساسية جميع أحرف ASCII، على الأقل، وربما بعض أحرف Unicode أيضًا. إذا استخدمت مثالًا تقوم بترميزه حرفًا غير موجود في فيلق التدريب، فسيتم تحويل هذا الحرف إلى رمز غير معروف. هذا هو أحد الأسباب التي تجعل العديد من نماذج NLP سيئة للغاية في تحليل المحتوى باستخدام الرموز التعبيرية، على سبيل المثال.

لدى GPT-2 وRoBERTa tokenizer (وهما متشابهان إلى حد ما) طريقة ذكية للتعامل مع هذا: فهي لا تنظر إلى الكلمات على أنها مكتوبة بحروف Unicode، ولكن بحروف بايت. بهذه الطريقة، يكون حجم المفردات الأساسي صغيرًا (256)، ولكن سيظل كل حرف يمكنك التفكير فيه مدرجًا ولن ينتهي به الأمر إلى التحويل إلى الرمز المجهول. تُعرف هذه الحيلة باسم *BPE على مستوى البايت*.

بعد الحصول على هذه المفردات الأساسية، نضيف رموزًا جديدة حتى يتم الوصول إلى حجم المفردات المرغوب من خلال تعلم *عمليات الدمج*، والتي هي قواعد لدمج عنصرين من المفردات الموجودة في عنصر جديد. لذلك، في البداية، ستقوم عمليات الدمج هذه بإنشاء رموز ذات حرفين، ثم، مع تقدم التدريب، كلمات فرعية أطول.

في أي خطوة أثناء تدريب المحلل اللغوي، ستبحث خوارزمية BPE عن أكثر أزواج الرموز الموجودة شيوعًا (بـ "زوج"، نعني هنا رمزين متتاليين في كلمة). هذا الزوج الأكثر شيوعًا هو الذي سيتم دمجه، ونكرر ذلك للخطوة التالية.

بالعودة إلى مثالنا السابق، دعنا نفترض أن الكلمات كانت لها الترددات التالية:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

مما يعني أن `"hug"` كان موجودًا 10 مرات في الفيلق، و`"pug"` 5 مرات، و`"pun"` 12 مرة، و`"bun"` 4 مرات، و`"hugs"` 5 مرات. نبدأ التدريب عن طريق تقسيم كل كلمة إلى أحرف (التي تشكل مفرداتنا الأولية) حتى نتمكن من رؤية كل كلمة على أنها قائمة من الرموز:

```
("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
```

ثم ننظر إلى الأزواج. الزوج `("h"، "u")` موجود في الكلمات `"hug"` و`"hugs"`، لذا فهي موجودة 15 مرة في المجموع في الفيلق. ومع ذلك، فإنها ليست الزوج الأكثر شيوعًا: ينتمي هذا الشرف إلى `("u"، "g")`، والذي يوجد في `"hug"` و`"pug"` و`"hugs"`، بإجمالي 20 مرة في المفردات.

وهكذا، فإن أول قاعدة دمج يتعلمها المحلل اللغوي هي `("u"، "g") -> "ug"`، والتي تعني أن `"ug"` سيتم إضافتها إلى المفردات، ويجب دمج الزوج في جميع كلمات الفيلق. في نهاية هذه المرحلة، تبدو المفردات والفيلق على النحو التالي:

```
مفردات: ["b"، "g"، "h"، "n"، "p"، "s"، "u"، "ug"]
فيلق: ("h" "ug"، 10)، ("p" "ug"، 5)، ("p" "u" "n"، 12)، ("b" "u" "n"، 4)، ("h" "ug" "s"، 5)
```

الآن لدينا بعض الأزواج التي ينتج عنها رمز أطول من حرفين: الزوج `("h"، "ug")`، على سبيل المثال (موجود 15 مرة في الفيلق). ومع ذلك، فإن الزوج الأكثر شيوعًا في هذه المرحلة هو `("u"، "n")`، الموجود 16 مرة في الفيلق، لذا فإن قاعدة الدمج الثانية التي تم تعلمها هي `("u"، "n") -> "un"`. يؤدي إضافة ذلك إلى المفردات ودمج جميع حالات التكرار الموجودة إلى ما يلي:

```
مفردات: ["b"، "g"، "h"، "n"، "p"، "s"، "u"، "ug"، "un"]
فيلق: ("h" "ug"، 10)، ("p" "ug"، 5)، ("p" "un"، 12)، ("b" "un"، 4)، ("h" "ug" "s"، 5)
```

الآن الزوج الأكثر شيوعًا هو `("h"، "ug")`، لذا فإننا نتعلم قاعدة الدمج `("h"، "ug") -> "hug"`، والتي تعطينا أول رمز مكون من ثلاثة أحرف. بعد الدمج، يبدو الفيلق على النحو التالي:

```
مفردات: ["b"، "g"، "h"، "n"، "p"، "s"، "u"، "ug"، "un"، "hug"]
فيلق: ("hug"، 10)، ("p" "ug"، 5)، ("p" "un"، 12)، ("b" "un"، 4)، ("hug" "s"، 5)
```

ونستمر على هذا المنوال حتى نصل إلى حجم المفردات المرغوب.

## خوارزمية الترميز

يتبع الترميز عملية التدريب عن كثب، بمعنى أن المدخلات الجديدة يتم ترميزها من خلال تطبيق الخطوات التالية:

1. التوحيد
2. ما قبل الترميز
3. تقسيم الكلمات إلى أحرف فردية
4. تطبيق قواعد الدمج التي تم تعلمها بالترتيب على تلك الانقسامات

لنأخذ مثالنا الذي استخدمناه أثناء التدريب، مع قواعد الدمج الثلاثة التي تم تعلمها:

```
("u"، "g") -> "ug"
("u"، "n") -> "un"
("h"، "ug") -> "hug"
```

سيتم ترميز الكلمة `"bug"` على أنها `["b"، "ug"]`. ومع ذلك، سيتم ترميز كلمة `"mug"` على أنها `["[UNK]"، "ug"]` لأن الحرف `"m"` لم يكن في المفردات الأساسية. وبالمثل، سيتم ترميز كلمة `"thug"` على أنها `["[UNK]"، "hug"]`: الحرف `"t"` غير موجود في المفردات الأساسية، ونتيجة لتطبيق قواعد الدمج، يتم دمج `"u"` و`"g"` أولاً، ثم `"h"` و`"ug"`.

✏️ **الآن دورك!** كيف تعتقد أن الكلمة `"unhug"` سيتم ترميزها؟
## تطبيق BPE

والآن، دعونا نلقي نظرة على تطبيق لخوارزمية BPE. لن يكون هذا الإصدار الأمثل الذي يمكنك استخدامه بالفعل في مجموعة بيانات كبيرة؛ فنحن نريد فقط أن نريك الشفرة البرمجية حتى تتمكن من فهم الخوارزمية بشكل أفضل قليلًا.

أولًا، نحتاج إلى مجموعة بيانات، لذا دعونا ننشئ واحدة بسيطة ببضع جمل:

```python
corpus = [
"This is the Hugging Face Course.",
"This chapter is about tokenization.",
"This section shows several tokenizer algorithms.",
"Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

بعد ذلك، نحتاج إلى تقسيم مجموعة البيانات هذه مبدئيًا إلى كلمات. نظرًا لأننا نكرر محللًا نحويًا من نوع BPE (مثل GPT-2)، فسنستخدم المحلل النحوي "gpt2" للتقسيم المبدئي:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

بعد ذلك، نحسب تكرارات كل كلمة في مجموعة البيانات أثناء قيامنا بالتقسيم المبدئي:

```python
from collections import defaultdict

word_freqs = defaultdict(int)

for text in corpus:
words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
new_words = [word for word, offset in words_with_offsets]
for word in new_words:
word_freqs[word] += 1

print(word_freqs)
```

```python out
defaultdict(int, {'This': 3, 'Ġis': 2, 'Ġthe': 1, 'ĠHugging': 1, 'ĠFace': 1, 'ĠCourse': 1, '.': 4, 'Ġchapter': 1,
'Ġabout': 1, 'Ġtokenization': 1, 'Ġsection': 1, 'Ġshows': 1, 'Ġseveral': 1, 'Ġtokenizer': 1, 'Ġalgorithms': 1,
'Hopefully': 1, ',': 1, 'Ġyou': 1, 'Ġwill': 1, 'Ġbe': 1, 'Ġable': 1, 'Ġto': 1, 'Ġunderstand': 1, 'Ġhow': 1,
'Ġthey': 1, 'Ġare': 1, 'Ġtrained': 1, 'Ġand': 1, 'Ġgenerate': 1, 'Ġtokens': 1})
```

الخطوة التالية هي حساب مجموعة المحارف الأساسية، المشكلة من جميع المحارف المستخدمة في مجموعة البيانات:

```python
alphabet = []

for word in word_freqs.keys():
for letter in word:
if letter not in alphabet:
alphabet.append(letter)
alphabet.sort()

print(alphabet)
```

```python out
[ ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's',
't', 'u', 'v', 'w', 'y', 'z', 'Ġ']
```

نضيف أيضًا الرموز الخاصة التي يستخدمها النموذج في بداية مجموعة المحارف هذه. في حالة GPT-2، الرمز الخاص الوحيد هو "<|endoftext|>":

```python
vocab = ["<|endoftext|>"] + alphabet.copy()
```

الآن، نحتاج إلى تقسيم كل كلمة إلى محارف فردية، لنتمكن من البدء في التدريب:

```python
splits = {word: [c for c in word] for word in word_freqs.keys()}
```

الآن بعد أن أصبحنا جاهزين للتدريب، دعونا نكتب دالة تحسب تكرار كل زوج من المحارف. سنحتاج إلى استخدام هذه الدالة في كل خطوة من خطوات التدريب:

```python
def compute_pair_freqs(splits):
pair_freqs = defaultdict(int)
for word, freq in word_freqs.items():
split = splits[word]
if len(split) == 1:
continue
for i in range(len(split) - 1):
pair = (split[i], split[i + 1])
pair_freqs[pair] += freq
return pair_freqs
```

دعونا نلقي نظرة على جزء من هذا القاموس بعد الانقسامات الأولية:

```python
pair_freqs = compute_pair_freqs(splits)

for i, key in enumerate(pair_freqs.keys()):
print(f"{key}: {pair_freqs[key]}")
if i >= 5:
break
```

```python out
('T', 'h'): 3
('h', 'i'): 3
('i', 's'): 5
('Ġ', 'i'): 2
('Ġ', 't'): 7
('t', 'h'): 3
```

الآن، لا يستغرق العثور على أكثر الأزواج تكرارًا سوى حلقة سريعة:

```python
best_pair = ""
max_freq = None

for pair, freq in pair_freqs.items():
if max_freq is None or max_freq < freq:
best_pair = pair
max_freq = freq

print(best_pair, max_freq)
```

```python out
('Ġ', 't') 7
```

لذا فإن أول دمج نتعلمه هو `('Ġ', 't') -> 'Ġt'`، ونضيف `'Ġt'` إلى مجموعة المحارف:

```python
merges = {("Ġ", "t"): "Ġt"}
vocab.append("Ġt")
```

لمواصلة ذلك، نحتاج إلى تطبيق هذا الدمج في قاموس `splits`. دعونا نكتب دالة أخرى لهذا الغرض:

```python
def merge_pair(a, b, splits):
for word in word_freqs:
split = splits[word]
if len(split) == 1:
continue

i = 0
while i < len(split) - 1:
if split[i] == a and split[i + 1] == b:
split = split[:i] + [a + b] + split[i + 2 :]
else:
i += 1
splits[word] = split
return splits
```

ويمكننا إلقاء نظرة على نتيجة الدمج الأول:

```py
splits = merge_pair("Ġ", "t", splits)
print(splits["Ġtrained"])
```

```python out
['Ġt', 'r', 'a', 'i', 'n', 'e', 'd']
```

الآن لدينا كل ما نحتاج إليه لتنفيذ حلقة حتى نتعلم جميع عمليات الدمج التي نريد. دعونا نهدف إلى حجم مجموعة محارف يبلغ 50:

```python
vocab_size = 50

while len(vocab) < vocab_size:
pair_freqs = compute_pair_freqs(splits)
best_pair = ""
max_freq = None
for pair, freq in pair_freqs.items():
if max_freq is None or max_freq < freq:
best_pair = pair
max_freq = freq
splits = merge_pair(*best_pair, splits)
merges[best_pair] = best_pair[0] + best_pair[1]
vocab.append(best_pair[0] + best_pair[1])
```

ونتيجة لذلك، تعلمنا 19 قاعدة دمج (كان حجم مجموعة المحارف الأولية 31 - 30 محرفًا في الأبجدية، بالإضافة إلى الرمز الخاص):

```py
print(merges)
```

```python out
{('Ġ', 't'): 'Ġt', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ġ', 'a'): 'Ġa', ('Ġt', 'o'): 'Ġto', ('e', 'n'): 'en',
('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ġto', 'k'): 'Ġtok',
('Ġtok', 'en'): 'Ġtoken', ('n', 'd'): 'nd', ('Ġ', 'is'): 'Ġis', ('Ġt', 'h'): 'Ġth', ('Ġth', 'e'): 'Ġthe',
('i', 'n'): 'in', ('Ġa', 'b'): 'Ġab', ('Ġtoken', 'i'): 'Ġtokeni'}
```

وتتكون مجموعة المحارف من الرمز الخاص، والأبجدية الأولية، ونتائج جميع عمليات الدمج:

```py
print(vocab)
```

```python out
['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',
'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ', 'Ġt', 'is', 'er', 'Ġa', 'Ġto', 'en', 'Th', 'This', 'ou', 'se',
'Ġtok', 'Ġtoken', 'nd', 'Ġis', 'Ġth', 'Ġthe', 'in', 'Ġab', 'Ġtokeni']
```

<Tip>
💡 لن يؤدي استخدام `train_new_from_iterator()` على نفس مجموعة البيانات إلى الحصول على نفس مجموعة المحارف بالضبط. ويرجع ذلك إلى أنه عندما يكون هناك خيار لأكثر الأزواج تكرارًا، فقد اخترنا الأول الذي صادفناه، في حين أن مكتبة 🤗 Tokenizers تختار الأول بناءً على معرّفاتها الداخلية.
</Tip>

لتحليل نص جديد نحليله مبدئيًا، ثم نقسمه، ثم نطبق جميع قواعد الدمج التي تعلمناها:

```python
def tokenize(text):
pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
pre_tokenized_text = [word for word, offset in pre_tokenize_result]
splits = [[l for l in word] for word in pre_tokenized_text]
for pair, merge in merges.items():
for idx, split in enumerate(splits):
i = 0
while i < len(split) - 1:
if split[i] == pair[0] and split[i + 1] == pair[1]:
split = split[:i] + [merge] + split[i + 2 :]
else:
i += 1
splits[idx] = split

return sum(splits, [])
```

يمكننا تجربة ذلك على أي نص يتكون من محارف في الأبجدية:

```py
tokenize("This is not a token.")
```

```python out
['This', 'Ġis', 'Ġ', 'n', 'o', 't', 'Ġa', 'Ġtoken', '.']
```

<Tip warning={true}>
⚠️ سينتج عن تنفيذنا خطأ إذا كان هناك محرف غير معروف لأننا لم نفعل شيئًا للتعامل معه. لا يحتوي GPT-2 في الواقع على رمز غير معروف (من المستحيل الحصول على محرف غير معروف عند استخدام BPE على مستوى البايت)، ولكن هذا قد يحدث هنا لأننا لم ندرج جميع البايتات الممكنة في مجموعة المحارف الأولية. وهذا الجانب من BPE يتجاوز نطاق هذا القسم، لذا فقد تركنا التفاصيل.
</Tip>

هذا كل شيء عن خوارزمية BPE! بعد ذلك، سنلقي نظرة على WordPiece.