# التطبيع والتمهيد للتجزيء 

قبل أن نغوص بشكل أعمق في خوارزميات التجزيء الثلاث الأكثر شيوعًا المستخدمة مع نماذج المحول (الترميز ثنائي الأحرف، WordPiece، والأحادي)، سنلقي أولاً نظرة على المعالجة المسبقة التي يطبقها كل مجزئ على النص. فيما يلي نظرة عامة رفيعة المستوى على الخطوات في خط أنابيب التجزيء: 

قبل تقسيم النص إلى رموز فرعية (وفقًا لنموذجه)، يقوم المجزئ بتنفيذ خطوتين: التطبيع والتمهيد للتجزيء.

## التطبيع 

تشمل خطوة التطبيع بعض التنظيف العام، مثل إزالة المسافات غير الضرورية، والتحويل إلى أحرف صغيرة، و/أو إزالة التشديد. إذا كنت على دراية بالتطبيع Unicode (مثل NFC أو NFKC)، فقد يكون هذا أيضًا شيء يطبقه المجزئ. 

يحتوي مجزئ 🤗 Transformers على سمة تسمى `backend_tokenizer` التي توفر الوصول إلى المجزئ الأساسي من مكتبة 🤗 Tokenizers: 

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
print(type(tokenizer.backend_tokenizer))
```

```python out
<class 'tokenizers.Tokenizer'>
```

لدى سمة `normalizer` لشيء `tokenizer` طريقة `normalize_str()` التي يمكننا استخدامها لمعرفة كيفية تنفيذ التطبيع: 

```py
print(tokenizer.backend_tokenizer.normalizer.normalize_str("Héllò hôw are ü?"))
```

```python out
'hello how are u?'
```

في هذا المثال، نظرًا لأننا اخترنا نقطة تفتيش `bert-base-uncased`، فقد قام التطبيق بالتحويل إلى أحرف صغيرة وإزالة التشديد. 

✏️ **جربه!** قم بتحميل مجزئ من نقطة تفتيش `bert-base-cased` ومرر نفس المثال إليه. ما هي الاختلافات الرئيسية التي يمكنك ملاحظتها بين إصداري المجزئ المميز بحالة الأحرف وغير المميز بحالة الأحرف؟ 

## التمهيد للتجزيء 

كما سنرى في الأقسام التالية، لا يمكن تدريب المجزئ على النص الخام وحده. بدلاً من ذلك، نحتاج أولاً إلى تقسيم النصوص إلى كيانات صغيرة، مثل الكلمات. وهنا يأتي دور خطوة التمهيد للتجزيء. كما رأينا في [الفصل 2](/course/chapter2)، يمكن لمجزئ قائم على الكلمات ببساطة تقسيم النص الخام إلى كلمات بناءً على المسافات والفواصل. ستكون هذه الكلمات هي حدود الرموز الفرعية التي يمكن للمجزئ تعلمها أثناء التدريب. 

ولرؤية كيف يؤدي المجزئ السريع التمهيد للتجزيء، يمكننا استخدام طريقة `pre_tokenize_str()` لسمة `pre_tokenizer` لشيء `tokenizer`: 

```py
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python out
[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]
```

لاحظ كيف يحتفظ المجزئ بالفعل بتعقب الإزاحات، والتي يمكنه من خلالها منحنا تعيين الإزاحة الذي استخدمناه في القسم السابق. هنا يتجاهل المجزئ المسافتين ويستبدلهما بواحدة فقط، ولكن الإزاحة تقفز بين "are" و"you" لمراعاة ذلك. 

نظرًا لأننا نستخدم مجزئ BERT، فإن التمهيد للتجزيء ينطوي على التقسيم بناءً على المسافات والفواصل. وقد تكون لدى المجزئات الأخرى قواعد مختلفة لهذه الخطوة. على سبيل المثال، إذا استخدمنا مجزئ GPT-2: 

```py
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

سيقوم بالتقسيم بناءً على المسافات والفواصل، ولكنه سيحتفظ بالمسافات ويستبدلها برمز `Ġ`، مما يمكنه من استرداد المسافات الأصلية إذا قمنا بفك تشفير الرموز: 

```python out
[('Hello', (0, 5)), (',', (5, 6)), ('Ġhow', (6, 10)), ('Ġare', (10, 14)), ('Ġ', (14, 15)), ('Ġyou', (15, 19)),
('?', (19, 20))]
```

لاحظ أيضًا أن هذا المجزئ، على عكس مجزئ BERT، لا يتجاهل المسافة المزدوجة. 

ولكن كمثال أخير، دعنا نلقي نظرة على مجزئ T5، والذي يعتمد على خوارزمية SentencePiece: 

```py
tokenizer = AutoTokenizer.from_pretrained("t5-small")
tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, how are  you?")
```

```python out
[('▁Hello,', (0, 6)), ('▁how', (7, 10)), ('▁are', (11, 14)), ('▁you?', (16, 20))]
```

مثل مجزئ GPT-2، يحتفظ هذا المجزئ بالمسافات ويستبدلها برمز محدد (`_`)، ولكن مجزئ T5 يقسم فقط بناءً على المسافات، وليس الفواصل. لاحظ أيضًا أنه أضاف مسافة افتراضية في بداية الجملة (قبل "Hello") وتجاهل المسافة المزدوجة بين "are" و"you". 

الآن بعد أن رأينا كيف تقوم بعض المجزئات المختلفة بمعالجة النص، يمكننا البدء في استكشاف الخوارزميات الأساسية نفسها. سنبدأ بإلقاء نظرة سريعة على خوارزمية SentencePiece القابلة للتطبيق على نطاق واسع؛ ثم، في الأقسام الثلاثة التالية، سنفحص كيفية عمل خوارزميات التجزيء الفرعي الثلاث الرئيسية.

## SentencePiece 

[SentencePiece](https://github.com/google/sentencepiece) هي خوارزمية تجزيء لمعالجة النص مسبقًا يمكنك استخدامها مع أي من النماذج التي سنراها في الأقسام الثلاثة التالية. فهو يعتبر النص تسلسلًا من أحرف Unicode، ويستبدل المسافات برمز خاص، `▁`. عند استخدامه بالاقتران مع خوارزمية Unigram (انظر [القسم 7](/course/chapter7/7))، فإنه لا يتطلب حتى خطوة التمهيد للتجزيء، وهو أمر مفيد للغاية للغات التي لا يتم فيها استخدام حرف المسافة (مثل الصينية أو اليابانية). 

والسمة الرئيسية الأخرى لخوارزمية SentencePiece هي *إمكانية عكس التجزيء*: نظرًا لعدم وجود معاملة خاصة للمسافات، يتم فك تشفير الرموز ببساطة عن طريق ضمها واستبدال `_`s بمسافات - مما يؤدي إلى النص المطبق. كما رأينا سابقًا، يقوم مجزئ BERT بإزالة المسافات المتكررة، لذلك لا يمكن عكس تجزيئه. 

## نظرة عامة على الخوارزمية 

في الأقسام التالية، سنغوص في خوارزميات التجزيء الفرعي الثلاث الرئيسية: BPE (يستخدمها GPT-2 وغيرها)، WordPiece (يستخدمها BERT، على سبيل المثال)، والأحادي (يستخدمها T5 وغيرها). قبل أن نبدأ، إليك نظرة عامة سريعة عن كيفية عمل كل منها. لا تتردد في العودة إلى هذا الجدول بعد قراءة كل من الأقسام التالية إذا لم يكن واضحًا بعد. 

النموذج | BPE | WordPiece | أحادي
:----:|:---:|:---------:|:------:
التدريب | يبدأ من مفردات صغيرة ويتعلم قواعد دمج الرموز | يبدأ من مفردات صغيرة ويتعلم قواعد دمج الرموز | يبدأ من مفردات كبيرة ويتعلم قواعد إزالة الرموز
خطوة التدريب | دمج الرموز التي تتوافق مع أكثر الأزواج شيوعًا | دمج الرموز التي تتوافق مع الزوج الذي يحتوي على أفضل نتيجة بناءً على تكرار الزوج، مع إعطاء الأفضلية للأزواج حيث يكون كل رمز فردي أقل تكرارًا | إزالة جميع الرموز في المفردات التي ستحد من الخسارة المحسوبة على الفيلق بأكمله
يتعلم | قواعد الدمج ومفردات | مفردات فقط | مفردات مع نتيجة لكل رمز
الترميز | تقسيم كلمة إلى أحرف وتطبيق عمليات الدمج التي تم تعلمها أثناء التدريب | العثور على أطول رمز فرعي بدءًا من البداية الموجودة في المفردات، ثم القيام بالشيء نفسه بالنسبة لبقية الكلمة | العثور على الانقسام الأكثر احتمالًا إلى رموز، باستخدام النتائج التي تم تعلمها أثناء التدريب 

والآن، لنغوص في BPE!