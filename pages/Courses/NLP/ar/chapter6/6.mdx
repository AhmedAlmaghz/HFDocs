## WordPiece tokenization 

WordPiece هو خوارزمية التجزئة التي طورتها Google لتمهيد الطريق لبرنامج BERT. وقد أعيد استخدامه منذ ذلك الحين في عدد قليل من نماذج المحول المستندة إلى BERT، مثل DistilBERT وMobileBERT وFunnel Transformers وMPNET. إنه مشابه جدًا لـ BPE من حيث التدريب، ولكن تجزئة الكلمات الفعلية تتم بشكل مختلف. 

<Youtube id="qpv6ms_t_1A"/> 

<Tip> 

💡 يغطي هذا القسم WordPiece بشكل متعمق، ويصل إلى حد إظهار التنفيذ الكامل. يمكنك تخطي ما تبقى إذا كنت تريد فقط نظرة عامة على خوارزمية التجزئة. 

</Tip> 

## خوارزمية التدريب 

<Tip warning={true}> 

⚠️ لم تفتح Google مطلقًا مصدر تنفيذها لخوارزمية التدريب لـ WordPiece، لذا فإن ما يلي هو أفضل تخمين لدينا بناءً على الأدبيات المنشورة. قد لا يكون دقيقًا بنسبة 100%. 

</Tip> 

مثل BPE، يبدأ WordPiece من مفردات صغيرة بما في ذلك الرموز الخاصة التي يستخدمها النموذج والأبجدية الأولية. نظرًا لأنه يحدد الكلمات الفرعية عن طريق إضافة بادئة (مثل `##` لـ BERT)، يتم تقسيم كل كلمة في البداية عن طريق إضافة تلك البادئة إلى جميع الأحرف داخل الكلمة. لذلك، على سبيل المثال، يتم تقسيم "كلمة" على النحو التالي: 

``` 
w ##o ##r ##d
``` 

وبالتالي، تحتوي الأبجدية الأولية على جميع الأحرف الموجودة في بداية الكلمة والأحرف الموجودة داخل الكلمة التي تسبقها بادئة WordPiece. 

ثم، مثل BPE، يتعلم WordPiece قواعد الدمج. الفرق الرئيسي هو طريقة اختيار الزوج الذي سيتم دمجه. بدلاً من تحديد أكثر الأزواج تكرارًا، يحسب WordPiece درجة لكل زوج، باستخدام الصيغة التالية: 

$$\ mathrm {score} = (\ mathrm {freq_ {of_pair}}) / (\ mathrm {freq_ {of_first_element}} \ times \ mathrm {freq_ {of_second_element}})$$ 

من خلال قسمة تكرار الزوج على حاصل ضرب تكرارات كل جزء منه، تعطي الخوارزمية الأولوية لدمج الأزواج حيث تكون الأجزاء الفردية أقل تكرارًا في المفردات. على سبيل المثال، لن يقوم بالضرورة بدمج `("un"، "##able")` حتى إذا ظهر هذا الزوج بشكل متكرر جدًا في المفردات، لأن كلا الزوجين "un" و"##able" سيظهر على الأرجح في العديد من الكلمات الأخرى وسيكون لهما تكرار عالٍ. على النقيض من ذلك، من المحتمل أن يتم دمج زوج مثل `("hu"، "##gging")` بشكل أسرع (بافتراض أن كلمة "hugging" تظهر غالبًا في المفردات) نظرًا لأن "hu" و"##gging" من المحتمل أن يكونا أقل تكرارًا بشكل فردي. 

لنلقِ نظرة على نفس المفردات التي استخدمناها في مثال التدريب BPE: 

``` 
("hug"، 10)، ("pug"، 5)، ("pun"، 12)، ("bun"، 4)، ("hugs"، 5)
``` 

ستكون الانقسامات هنا: 

``` 
("h" "##u" "##g"، 10)، ("p" "##u" "##g"، 5)، ("p" "##u" "##n"، 12)، ("b" "##u" "##n"، 4)، ("h" "##u" "##g" "##s"، 5)
``` 

لذلك ستكون المفردات الأولية `["b"، "h"، "p"، "##g"، "##n"، "##s"، "##u"]` (إذا تجاهلنا الرموز الخاصة الآن). الزوج الأكثر تكرارًا هو `("##u"، "##g")` (يظهر 20 مرة)، ولكن التردد الفردي لـ `"##u"` مرتفع جدًا، لذا فإن درجته ليست الأعلى (1 / 36). جميع الأزواج مع `"##u"` لها في الواقع نفس الدرجة (1 / 36)، لذا فإن أفضل درجة تذهب إلى الزوج `("##g"، "##s")` - الوحيد بدون `"##u"` - في 1/20، والقاعدة الأولى التي يتم تعلمها هي `("##g"، "##s") -> ("##gs")`. 

لاحظ أنه عند الدمج، نقوم بإزالة `##` بين الرمزين، لذا فإننا نضيف `"##gs"` إلى المفردات ونطبق الدمج في كلمات المجموعة: 

``` 
مفردات: ["b"، "h"، "p"، "##g"، "##n"، "##s"، "##u"، "##gs"]
المجموعة: ("h" "##u" "##g"، 10)، ("p" "##u" "##g"، 5)، ("p" "##u" "##n"، 12)، ("b" "##u" "##n"، 4)، ("h" "##u" "##gs"، 5)
``` 

في هذه المرحلة، يظهر `"##u"` في جميع الأزواج المحتملة، لذا فإنها جميعًا تنتهي بنفس الدرجة. دعنا نقول أنه في هذه الحالة، يتم دمج الزوج الأول، لذا `("h"، "##u") -> "hu"`. هذا يأخذنا إلى: 

``` 
مفردات: ["b"، "h"، "p"، "##g"، "##n"، "##s"، "##u"، "##gs"، "hu"]
المجموعة: ("hu" "##g"، 10)، ("p" "##u" "##g"، 5)، ("p" "##u" "##n"، 12)، ("b" "##u" "##n"، 4)، ("hu" "##gs"، 5)
``` 

ثم يتم مشاركة الدرجة التالية من قبل `("hu"، "##g")` و`("hu"، "##gs")` (مع 1/15، مقارنة بـ 1/21 لجميع الأزواج الأخرى)، لذا فإن أول زوج له الدرجة الأكبر هو الذي يتم دمجه: 

``` 
مفردات: ["b"، "h"، "p"، "##g"، "##n"، "##s"، "##u"، "##gs"، "hu"، "hug"]
المجموعة: ("hug"، 10)، ("p" "##u" "##g"، 5)، ("p" "##u" "##n"، 12)، ("b" "##u" "##n"، 4)، ("hu" "##gs"، 5)
``` 

ونحن نواصل مثل هذا حتى نصل إلى حجم المفردات المطلوب. 

<Tip> 

✏️ **الآن دورك!** ما هي قاعدة الدمج التالية؟ 

</Tip> 

## خوارزمية التجزئة 

تختلف التجزئة في WordPiece وBPE في أن WordPiece يحفظ فقط المفردات النهائية، وليس قواعد الدمج التي تم تعلمها. بدءًا من الكلمة التي سيتم تجزئتها، يجد WordPiece أطول كلمة فرعية موجودة في المفردات، ثم يقوم بالتقسيم عليها. على سبيل المثال، إذا استخدمنا المفردات التي تم تعلمها في المثال أعلاه، فبالنسبة لكلمة "احتضان" أطول كلمة فرعية تبدأ من البداية موجودة في المفردات هي "احتضان"، لذا نقوم بالتقسيم هناك ونحصل على `["احتضان"، "##s"]`. ثم نواصل مع `"##s"`، والتي هي في المفردات، لذا فإن تجزئة كلمة "احتضان" هي `["احتضان"، "##s"]`. 

مع BPE، لكنا قد طبقنا عمليات الدمج التي تعلمناها بالترتيب وقمنا بتجزئة هذا على أنه `["hu"، "##gs"]`، لذا فإن الترميز مختلف. 

وكمثال آخر، دعنا نرى كيف تتم تجزئة كلمة "bugs". "b" هي أطول كلمة فرعية تبدأ في بداية الكلمة موجودة في المفردات، لذا نقوم بالتقسيم هناك ونحصل على `["b"، "##ugs"]`. ثم `"##u"` هي أطول كلمة فرعية تبدأ في بداية "##ugs" موجودة في المفردات، لذا نقوم بالتقسيم هناك ونحصل على `["b"، "##u"، "##gs"]`. أخيرًا، `"##gs"` موجود في المفردات، لذا فإن هذه القائمة الأخيرة هي تجزئة كلمة "bugs". 

عندما تصل التجزئة إلى مرحلة لا يمكن فيها العثور على كلمة فرعية في المفردات، يتم تجزئة الكلمة بأكملها على أنها غير معروفة - لذا، على سبيل المثال، يتم تجزئة كلمة "mug" على أنها `["[UNK]"]`، كما هو الحال مع كلمة "bum" (حتى إذا كان بإمكاننا البدء بـ "b" و"##u"، فإن "##m" غير موجود في المفردات، وستكون التجزئة الناتجة ببساطة `["[UNK]"]`، وليس `["b"، "##u"، "[UNK]"]`). هذا اختلاف آخر عن BPE، والذي سيصنف الأحرف الفردية غير الموجودة في المفردات فقط على أنها غير معروفة. 

<Tip> 

✏️ **الآن دورك!** كيف سيتم تجزئة كلمة "pugs"؟ 

</Tip>
## تطبيق WordPiece

الآن، دعونا نلقي نظرة على تطبيق لخوارزمية WordPiece. مثل BPE، هذا فقط لأغراض تعليمية، ولن تتمكن من استخدامه على مجموعة بيانات كبيرة.

سنستخدم نفس مجموعة البيانات كما في مثال BPE:

```python
corpus = [
"This is the Hugging Face Course.",
"This chapter is about tokenization.",
"This section shows several tokenizer algorithms.",
"Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

أولاً، نحتاج إلى تقسيم مجموعة البيانات مسبقًا إلى كلمات. نظرًا لأننا نكرر عملية توكينيزر WordPiece (مثل BERT)، فسنستخدم توكينيزر "bert-base-cased" للتقسيم المسبق:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

بعد ذلك، نحسب تكرارات كل كلمة في مجموعة البيانات أثناء التقسيم المسبق:

```python
from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
new_words = [word for word, offset in words_with_offsets]
for word in new_words:
word_freqs[word] += 1

word_freqs
```

```python out
defaultdict(
int, {'This': 3, 'is': 2, 'the': 1, 'Hugging': 1, 'Face': 1, 'Course': 1, '.': 4, 'chapter': 1, 'about': 1,
'tokenization': 1, 'section': 1, 'shows': 1, 'several': 1, 'tokenizer': 1, 'algorithms': 1, 'Hopefully': 1,
',': 1, 'you': 1, 'will': 1, 'be': 1, 'able': 1, 'to': 1, 'understand': 1, 'how': 1, 'they': 1, 'are': 1,
'trained': 1, 'and': 1, 'generate': 1, 'tokens': 1})
```

كما رأينا سابقًا، الأبجدية هي المجموعة الفريدة المكونة من جميع الأحرف الأولى من الكلمات، وجميع الأحرف الأخرى التي تظهر في الكلمات التي تسبقها "##":

```python
alphabet = []
for word in word_freqs.keys():
if word[0] not in alphabet:
alphabet.append(word[0])
for letter in word[1:]:
if f"##{letter}" not in alphabet:
alphabet.append(f"##{letter}")

alphabet.sort()
alphabet

print(alphabet)
```

```python out
['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s',
'##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u',
'w', 'y']
```

نضيف أيضًا الرموز الخاصة التي يستخدمها النموذج في بداية هذا القاموس. في حالة BERT، تكون القائمة كما يلي: "["[PAD]"، "[UNK]"، "[CLS]"، "[SEP]"، "[MASK]"]:

```python
vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy()
```

بعد ذلك، نحتاج إلى تقسيم كل كلمة، مع إضافة "##" قبل جميع الأحرف ما عدا الحرف الأول:

```python
splits = {
word: [c if i == 0 else f"##{c}" for i, c in enumerate(word)]
for word in word_freqs.keys()
}
```

الآن بعد أن أصبحنا جاهزين للتدريب، دعونا نكتب دالة تقوم بحساب درجة كل زوج. سنحتاج إلى استخدام هذا في كل خطوة من التدريب:

```python
def compute_pair_scores(splits):
letter_freqs = defaultdict(int)
pair_freqs = defaultdict(int)
for word, freq in word_freqs.items():
split = splits[word]
if len(split) == 1:
letter_freqs[split[0]] += freq
continue
for i in range(len(split) - 1):
pair = (split[i], split[i + 1])
letter_freqs[split[i]] += freq
pair_freqs[pair] += freq
letter_freqs[split[-1]] += freq

scores = {
pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
for pair, freq in pair_freqs.items()
}
return scores
```

دعونا نلقي نظرة على جزء من هذا القاموس بعد الانقسامات الأولية:

```python
pair_scores = compute_pair_scores(splits)
for i, key in enumerate(pair_scores.keys()):
print(f"{key}: {pair_scores[key]}")
if i >= 5:
break
```

```python out
('T', '##h'): 0.125
('##h', '##i'): 0.03409090909090909
('##i', '##s'): 0.02727272727272727
('i', '##s'): 0.1
('t', '##h'): 0.03571428571428571
('##h', '##e'): 0.011904761904761904
```

الآن، لا يستغرق العثور على الزوج ذو الدرجة الأعلى سوى حلقة سريعة:

```python
best_pair = ""
max_score = None
for pair, score in pair_scores.items():
if max_score is None or max_score < score:
best_pair = pair
max_score = score

print(best_pair, max_score)
```

```python out
('a', '##b') 0.2
```

لذا فإن أول دمج نتعلمه هو "('a', '##b') -> 'ab'"، ونضيف "ab'" إلى القاموس:

```python
vocab.append("ab")
```

لمواصلة ذلك، نحتاج إلى تطبيق هذا الدمج في قاموس "splits". دعونا نكتب دالة أخرى لهذا الغرض:

```python
def merge_pair(a, b, splits):
for word in word_freqs:
split = splits[word]
if len(split) == 1:
continue
i = 0
while i < len(split) - 1:
if split[i] == a and split[i + 1] == b:
merge = a + b[2:] if b.startswith("##") else a + b
split = split[:i] + [merge] + split[i + 2 :]
else:
i += 1
splits[word] = split
return splits
```

يمكننا إلقاء نظرة على نتيجة الدمج الأول:

```py
splits = merge_pair("a", "##b", splits)
splits["about"]
```

```python out
['ab', '##o', '##u', '##t']
```

الآن بعد أن أصبح لدينا كل ما نحتاج إليه، يمكننا الاستمرار في الحلقة حتى نتعلم جميع عمليات الدمج التي نريد. دعونا نهدف إلى حجم قاموس يبلغ 70:

```python
vocab_size = 70
while len(vocab) < vocab_size:
scores = compute_pair_scores(splits)
best_pair, max_score = "", None
for pair, score in scores.items():
if max_score is None or max_score < score:
best_pair = pair
max_score = score
splits = merge_pair(*best_pair, splits)
new_token = (
best_pair[0] + best_pair[1][2:]
if best_pair[1].startswith("##")
else best_pair[0] + best_pair[1]
)
vocab.append(new_token)
```

بعد ذلك، يمكننا إلقاء نظرة على القاموس الناتج:

```py
print(vocab)
```

```python out
['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k',
'##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H',
'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully',
'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat',
'##ut']
```

كما نرى، مقارنة بـ BPE، يتعلم هذا التوكنيزر أجزاء الكلمات كرموز بشكل أسرع قليلاً.

<Tip>
💡 استخدام "train_new_from_iterator()" على نفس مجموعة البيانات لن يؤدي إلى نفس القاموس بالضبط. ويرجع ذلك إلى أن مكتبة 🤗 Tokenizers لا تنفذ WordPiece للتدريب (نظرًا لأننا لسنا متأكدين تمامًا من تفاصيلها الداخلية)، ولكنها تستخدم BPE بدلاً من ذلك.
</Tip>

لتوكينيز نص جديد، نقوم بتقسيمه مسبقًا، ثم نقسمه، ثم نطبق خوارزمية التوكنيز على كل كلمة. وهذا يعني أننا نبحث عن أكبر كلمة فرعية تبدأ من بداية الكلمة الأولى ونقسمها، ثم نكرر العملية على الجزء الثاني، وهكذا لبقية تلك الكلمة والكلمات التالية في النص:

```python
def encode_word(word):
tokens = []
while len(word) > 0:
i = len(word)
while i > 0 and word[:i] not in vocab:
i -= 1
if i == 0:
return ["[UNK]"]
tokens.append(word[:i])
word = word[i:]
if len(word) > 0:
word = f"##{word}"
return tokens
```

دعونا نختبرها على كلمة موجودة في القاموس، وأخرى غير موجودة:

```python
print(encode_word("Hugging"))
print(encode_word("HOgging"))
```

```python out
['Hugg', '##i', '##n', '##g']
['[UNK]']
```

الآن، دعونا نكتب دالة تقوم بتوكينيز نص:

```python
def tokenize(text):
pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
pre_tokenized_text = [word for word, offset in pre_tokenize_result]
encoded_words = [encode_word(word) for word in pre_tokenized_text]
return sum(encoded_words, [])
```

يمكننا تجربتها على أي نص:

```python
tokenize("This is the Hugging Face course!")
```

```python out
['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s',
'##e', '[UNK]']
```

هذا كل شيء عن خوارزمية WordPiece! الآن دعونا نلقي نظرة على Unigram.