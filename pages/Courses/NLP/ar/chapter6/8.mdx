# بناء أداة تحليل نصي، خطوة بخطوة

كما رأينا في الأقسام السابقة، تتكون عملية تحليل النص من عدة خطوات:

- التوحيد القياسي (أي تنظيف النص الذي يعتبر ضروريًا، مثل إزالة المسافات أو علامات الترقيم، وتوحيد يونيكود، وما إلى ذلك)

- ما قبل التحليل (تقسيم الإدخال إلى كلمات)

- تشغيل الإدخال من خلال النموذج (باستخدام الكلمات التي تم تحليلها مسبقًا لإنتاج تسلسل من الرموز)

- ما بعد المعالجة (إضافة الرموز الخاصة لأداة تحليل النص، وتوليد قناع الاهتمام ومعرفات نوع الرمز)

على سبيل التذكير، إليك نظرة أخرى على العملية الشاملة:

تُستخدم مكتبة 🤗 Tokenizers لتوفير عدة خيارات لكل من هذه الخطوات، والتي يمكنك مزجها ومطابقتها معًا. في هذا القسم، سنرى كيف يمكننا بناء أداة لتحليل النص من الصفر، على عكس تدريب أداة تحليل نصي جديدة من أداة قديمة كما فعلنا في [القسم 2](/course/chapter6/2). بعد ذلك، ستتمكن من بناء أي نوع من أدوات تحليل النصوص التي يمكنك التفكير فيها!

بشكل أكثر دقة، تم بناء المكتبة حول فئة `Tokenizer` مركزية مع كتل البناء المعاد تجميعها في وحدات فرعية:

- يحتوي `normalizers` على جميع أنواع `Normalizer` الممكنة التي يمكنك استخدامها (القائمة الكاملة [هنا](https://huggingface.co/docs/tokenizers/api/normalizers))

- يحتوي `pre_tokenizers` على جميع أنواع `PreTokenizer` الممكنة التي يمكنك استخدامها (القائمة الكاملة [هنا](https://huggingface.co/docs/tokenizers/api/pre-tokenizers))

- يحتوي `models` على الأنواع المختلفة من `Model` التي يمكنك استخدامها، مثل `BPE` و`WordPiece` و`Unigram` (القائمة الكاملة [هنا](https://huggingface.co/docs/tokenizers/api/models))

- يحتوي `trainers` على جميع الأنواع المختلفة من `Trainer` التي يمكنك استخدامها لتدريب نموذجك على مجموعة من البيانات (واحد لكل نوع من النماذج؛ القائمة الكاملة [هنا](https://huggingface.co/docs/tokenizers/api/trainers))

- يحتوي `post_processors` على الأنواع المختلفة من `PostProcessor` التي يمكنك استخدامها (القائمة الكاملة [هنا](https://huggingface.co/docs/tokenizers/api/post-processors))

- يحتوي `decoders` على الأنواع المختلفة من `Decoder` التي يمكنك استخدامها لفك تشفير مخرجات التحليل (القائمة الكاملة [هنا](https://huggingface.co/docs/tokenizers/components#decoders))

يمكنك العثور على القائمة الكاملة لكتل البناء [هنا](https://huggingface.co/docs/tokenizers/components).

## الحصول على مجموعة من البيانات

لتدريب أداة تحليل النص الجديدة الخاصة بنا، سنستخدم مجموعة بيانات نصية صغيرة (حتى تعمل الأمثلة بسرعة). خطوات الحصول على مجموعة البيانات مماثلة لتلك التي اتخذناها في [بداية هذا الفصل](/course/chapter6/2)، ولكن هذه المرة سنستخدم مجموعة بيانات [WikiText-2](https://huggingface.co/datasets/wikitext):

```python
from datasets import load_dataset

dataset = load_dataset("wikitext", name="wikitext-2-raw-v1", split="train")


def get_training_corpus():
    for i in range(0, len(dataset), 1000):
        yield dataset[i : i + 1000]["text"]
```

وظيفة `get_training_corpus()` هي مولد سيعطي دفعات من 1000 نص، والتي سنستخدمها لتدريب أداة تحليل النص.

يمكن أيضًا تدريب مكتبة 🤗 Tokenizers على ملفات نصية مباشرة. فيما يلي كيفية إنشاء ملف نصي يحتوي على جميع النصوص/الإدخالات من WikiText-2 التي يمكننا استخدامها محليًا:

```python
with open("wikitext-2.txt", "w", encoding="utf-8") as f:
    for i in range(len(dataset)):
        f.write(dataset[i]["text"] + "\n")
```

بعد ذلك، سنريكم كيفية بناء أداة تحليل نصي BERT وGPT-2 وXLNet الخاصة بكم، خطوة بخطوة. سيعطينا ذلك مثالًا على كل من خوارزميات التحليل الثلاثة الرئيسية: WordPiece وBPE وUnigram. دعونا نبدأ مع BERT!
## بناء محلل نحوي WordPiece من الصفر

للبناء محلل نحوي باستخدام مكتبة 🤗 Tokenizers، نبدأ بتحديد كائن `Tokenizer` مع نموذج `model`، ثم نحدد خصائصه `normalizer` و`pre_tokenizer` و`post_processor` و`decoder` بالقيم التي نريدها.

في هذا المثال، سنقوم بإنشاء كائن `Tokenizer` مع نموذج WordPiece:

```python
from tokenizers import (
decoders,
models,
normalizers,
pre_tokenizers,
processors,
trainers,
Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]"))
```

يجب علينا تحديد `unk_token` حتى يعرف النموذج ما الذي يجب إعادته عندما يواجه أحرفًا لم يرها من قبل. هناك حجج أخرى يمكننا تحديدها هنا، بما في ذلك `vocab` من النموذج (سنقوم بتدريب النموذج، لذلك لا نحتاج إلى تحديده) و`max_input_chars_per_word`، الذي يحدد طولًا أقصى لكل كلمة (سيتم تقسيم الكلمات الأطول من القيمة الممررة).

الخطوة الأولى من التحليل النحوي هي التوحيد القياسي، لذلك دعنا نبدأ بذلك. نظرًا لأن BERT يستخدم على نطاق واسع، هناك `BertNormalizer` مع الخيارات الكلاسيكية التي يمكننا تحديدها لـ BERT: `lowercase` و`strip_accents`، والتي لا تحتاج إلى شرح؛ `clean_text` لإزالة جميع أحرف التحكم واستبدال المسافات المتكررة بواحدة؛ و`handle_chinese_chars`، الذي يضع مسافات حول الأحرف الصينية. لتكرار المحلل النحوي `bert-base-uncased`، يمكننا فقط تحديد هذا الموحد القياسي:

```python
tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)
```

ومع ذلك، بشكل عام، عند بناء محلل نحوي جديد، لن يكون لديك حق الوصول إلى مثل هذا الموحد القياسي المفيد الذي تم تنفيذه بالفعل في مكتبة 🤗 Tokenizers - لذلك دعنا نرى كيفية إنشاء الموحد القياسي BERT يدويًا. توفر المكتبة موحدًا قياسيًا `Lowercase` وموحدًا قياسيًا `StripAccents`، ويمكنك تكوين عدة موحدين قياسيين باستخدام `Sequence`:

```python
tokenizer.normalizer = normalizers.Sequence(
[normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)
```

نستخدم أيضًا موحدًا قياسيًا Unicode `NFD`، وإلا فلن يتعرف الموحد القياسي `StripAccents` بشكل صحيح على الأحرف المنقطة ولن يقوم بإزالتها.

كما رأينا من قبل، يمكننا استخدام طريقة `normalize_str()` للموحد القياسي للتحقق من التأثيرات التي تحدثها على نص معين:

```python
print(tokenizer.normalizer.normalize_str("Héllò hôw are ü?"))
```

```python out
hello how are u?
```

<Tip>

**للمضي قدمًا** إذا قمت باختبار الإصدارين السابقين من الموحدين القياسيين على سلسلة تحتوي على حرف Unicode `u"\u0085"`، فستلاحظ بالتأكيد أن هذين الموحدين القياسيين غير متطابقين تمامًا.

للحفاظ على بساطة الإصدار باستخدام `normalizers.Sequence`، لم نقم بتضمين استبدالات Regex التي تتطلبها `BertNormalizer` عندما تكون حجة `clean_text` مضبوطة على `True` - وهو السلوك الافتراضي. ولكن لا تقلق: من الممكن الحصول على التوحيد القياسي نفسه تمامًا دون استخدام الموحد القياسي `BertNormalizer` المفيد عن طريق إضافة استبدالين `normalizers.Replace` إلى تسلسل الموحدين القياسيين.

</Tip>

الخطوة التالية هي مرحلة ما قبل التحليل النحوي. مرة أخرى، هناك `BertPreTokenizer` مسبق البناء يمكننا استخدامه:

```python
tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()
```

أو يمكننا بناؤه من الصفر:

```python
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
```

لاحظ أن المحلل النحوي `Whitespace` يقسم على المسافات البيضاء وجميع الأحرف التي ليست حروفًا أو أرقامًا أو حرف تسطير سفلي، لذا فهو يقسم من الناحية الفنية على المسافات البيضاء وعلامات الترقيم:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

إذا كنت تريد فقط تقسيمه على المسافات البيضاء، فيجب عليك استخدام المحلل النحوي `WhitespaceSplit` بدلاً من ذلك:

```python
pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[("Let's", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))]
```

كما هو الحال مع الموحدين القياسيين، يمكنك استخدام `Sequence` لتكوين عدة محللين نحويين مسبقين:

```python
pre_tokenizer = pre_tokenizers.Sequence(
[pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

الخطوة التالية في خط أنابيب التحليل النحوي هي تشغيل المدخلات عبر النموذج. لقد حددنا نموذجنا بالفعل في التهيئة، لكننا ما زلنا بحاجة إلى تدريبه، والذي سيتطلب `WordPieceTrainer`. الشيء الرئيسي الذي يجب تذكره عند إنشاء مدرب في 🤗 Tokenizers هو أنه يجب عليك تمرير جميع الرموز الخاصة التي تنوي استخدامها - وإلا فلن يقوم بإضافتها إلى المفردات، نظرًا لعدم وجودها في مجموعة البيانات التدريبية:

```python
special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]
trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)
```

بالإضافة إلى تحديد `vocab_size` و`special_tokens`، يمكننا تعيين `min_frequency` (عدد المرات التي يجب أن تظهر فيها الرموز لإدراجها في المفردات) أو تغيير `continuing_subword_prefix` (إذا كنا نريد استخدام شيء مختلف عن `##`).

لتدريب نموذجنا باستخدام الدالة iterator التي حددناها سابقًا، كل ما علينا فعله هو تنفيذ هذا الأمر:

```python
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

يمكننا أيضًا استخدام ملفات نصية لتدريب المحلل النحوي الخاص بنا، والتي ستكون على النحو التالي (نقوم بإعادة تهيئة النموذج باستخدام WordPiece فارغ مسبقًا):

```python
tokenizer.model = models.WordPiece(unk_token="[UNK]")
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

في كلتا الحالتين، يمكننا بعد ذلك اختبار المحلل النحوي على نص عن طريق استدعاء طريقة `encode()` :

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']
```

إن `encoding` الذي تم الحصول عليه هو `Encoding`، والذي يحتوي على جميع الإخراجات اللازمة للمحلل النحوي في سماته المختلفة: `ids`، و`type_ids`، و`tokens`، و`offsets`، و`attention_mask`، و`special_tokens_mask`، و`overflowing`.

الخطوة الأخيرة في خط أنابيب التحليل النحوي هي المعالجة اللاحقة. نحن بحاجة إلى إضافة الرمز `[CLS]` في البداية ورمز `[SEP]` في النهاية (أو بعد كل جملة، إذا كان لدينا زوج من الجمل). سنستخدم `TemplateProcessor` لهذا الغرض، ولكن أولاً نحتاج إلى معرفة معرفات الرموز `[CLS]` و`[SEP]` في المفردات:

```python
cls_token_id = tokenizer.token_to_id("[CLS]")
sep_token_id = tokenizer.token_to_id("[SEP]")
print(cls_token_id, sep_token_id)
```

```python out
(2, 3)
```

لإنشاء القالب لـ `TemplateProcessor`، يجب علينا تحديد كيفية معالجة جملة واحدة وزوج من الجمل. بالنسبة لكليهما، نكتب الرموز الخاصة التي نريد استخدامها؛ الجملة الأولى (أو الوحيدة) ممثلة بـ `$A`، بينما الجملة الثانية (إذا تم تشفير زوج) ممثلة بـ `$B`. لكل من هذه (الرموز الخاصة والجمل)، نقوم أيضًا بتحديد معرف نوع الرمز المناسب بعد علامة النقطتين.

يتم تعريف القالب الكلاسيكي BERT على النحو التالي:

```python
tokenizer.post_processor = processors.TemplateProcessing(
single=f"[CLS]:0 $A:0 [SEP]:0",
pair=f"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1",
special_tokens=[("[CLS]", cls_token_id), ("[SEP]", sep_token_id)],
)
```

لاحظ أننا نحتاج إلى تمرير معرفات الرموز الخاصة، بحيث يمكن للمحلل النحوي تحويلها إلى معرفاتها بشكل صحيح.

بمجرد إضافة ذلك، عند العودة إلى مثالنا السابق، سنحصل على ما يلي:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']
```

وعلى زوج من الجمل، نحصل على النتيجة الصحيحة:

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences.")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]
```

لقد انتهينا تقريبًا من بناء هذا المحلل النحوي من الصفر - الخطوة الأخيرة هي تضمين محلل ترميز:

```python
tokenizer.decoder = decoders.WordPiece(prefix="##")
```

دعنا نختبرها على `encoding` السابق:

```python
tokenizer.decode(encoding.ids)
```

```python out
"let's test this tokenizer... on a pair of sentences."
```

رائع! يمكننا حفظ محللنا النحوي في ملف JSON واحد على النحو التالي:

```python
tokenizer.save("tokenizer.json")
```

يمكننا بعد ذلك إعادة تحميل هذا الملف في كائن `Tokenizer` باستخدام طريقة `from_file()` :

```python
new_tokenizer = Tokenizer.from_file("tokenizer.json")
```

لاستخدام هذا المحلل النحوي في 🤗 Transformers، يجب علينا لفها في `PreTrainedTokenizerFast`. يمكننا إما استخدام الفئة العامة أو، إذا كان محللنا النحوي يتوافق مع نموذج موجود، استخدم تلك الفئة (هنا، `BertTokenizerFast`). إذا كنت تطبق هذا الدرس لبناء محلل نحوي جديد تمامًا، فستضطر إلى استخدام الخيار الأول.

للف المحلل النحوي في `PreTrainedTokenizerFast`، يمكننا إما تمرير المحلل النحوي الذي بنيناه كـ `tokenizer_object` أو تمرير ملف المحلل النحوي الذي تم حفظه كـ `tokenizer_file`. الشيء الرئيسي الذي يجب تذكره هو أنه يجب علينا يدويًا تعيين جميع الرموز الخاصة، نظرًا لأن هذه الفئة لا يمكنها الاستدلال من كائن `tokenizer` والذي يعد رمز القناع، ورمز `[CLS]`، وما إلى ذلك:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
tokenizer_object=tokenizer,
# tokenizer_file="tokenizer.json", # يمكنك التحميل من ملف المحلل النحوي، بدلاً من ذلك
unk_token="[UNK]",
pad_token="[PAD]",
cls_token="[CLS]",
sep_token="[SEP]",
mask_token="[MASK]",
)
```

إذا كنت تستخدم فئة محلل نحوي محددة (مثل `BertTokenizerFast`)، فستحتاج فقط إلى تحديد الرموز الخاصة المختلفة عن الرموز الافتراضية (هنا، لا شيء):

```python
from transformers import BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)
```

يمكنك بعد ذلك استخدام هذا المحلل النحوي مثل أي محلل نحوي آخر من 🤗 Transformers. يمكنك حفظه باستخدام طريقة `save_pretrained()`، أو تحميله إلى Hub باستخدام طريقة `push_to_hub()` .

الآن بعد أن رأينا كيفية بناء محلل نحوي WordPiece، دعنا نفعل الشيء نفسه لمحلل نحوي BPE. سنكون أسرع قليلاً لأنك تعرف جميع الخطوات، وسنلقي الضوء فقط على الاختلافات.
## بناء محلل BPE من الصفر [[building-a-bpe-tokenizer-from-scratch]]

الآن، دعنا نبني محلل رموز GPT-2. وكما هو الحال مع محلل رموز BERT، نبدأ بتحديد واجهة برمجة التطبيقات API `Tokenizer` بنموذج BPE:

```python
tokenizer = Tokenizer(models.BPE())
```

وكما هو الحال مع BERT، يمكننا تهيئة هذا النموذج بقاموس إذا كان لدينا واحدًا (سنحتاج إلى تمرير `vocab` و`merges` في هذه الحالة)، ولكن نظرًا لأننا سنقوم بالتدريب من الصفر، فلا يلزم القيام بذلك. كما لا يلزم تحديد `unk_token` لأن GPT-2 يستخدم BPE على مستوى البايت، والذي لا يتطلب ذلك.

لا يستخدم GPT-2 أداة تحليل قواعد اللغة، لذا فإننا نتخطى تلك الخطوة وننتقل مباشرة إلى مرحلة ما قبل التحليل:

```python
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
```

الخيار الذي أضفناه إلى `ByteLevel` هنا هو عدم إضافة مسافة في بداية الجملة (وهو الإعداد الافتراضي في الحالات الأخرى). يمكننا إلقاء نظرة على مرحلة ما قبل التحليل لنص مثال كما هو موضح سابقًا:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test pre-tokenization!")
```

```python out
[('Let', (0, 3)), ("'s", (3, 5)), ('Ġtest', (5, 10)), ('Ġpre', (10, 14)), ('-', (14, 15)),
('tokenization', (15, 27)), ('!', (27, 28))]
```

الخطوة التالية هي النموذج، الذي يحتاج إلى التدريب. بالنسبة لـ GPT-2، فإن الرمز الخاص الوحيد هو رمز نهاية النص:

```python
trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=["<|endoftext|>"])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

وكما هو الحال مع `WordPieceTrainer`، بالإضافة إلى `vocab_size` و`special_tokens`، يمكننا تحديد `min_frequency` إذا أردنا ذلك، أو إذا كان لدينا لاحقة نهاية الكلمة (مثل `</w>`)، فيمكننا تعيينها باستخدام `end_of_word_suffix`.

يمكن أيضًا تدريب هذا المحلل على ملفات نصية:

```python
tokenizer.model = models.BPE()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

دعونا نلقي نظرة على تحليل عينة نص:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['L', 'et', "'", 's', 'Ġtest', 'Ġthis', 'Ġto', 'ken', 'izer', '.']
```

نطبق مرحلة ما بعد المعالجة على مستوى البايت لمحلل رموز GPT-2 على النحو التالي:

```python
tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)
```

يشير خيار `trim_offsets = False` إلى مرحلة ما بعد المعالجة أنه يجب علينا ترك إزاحات الرموز التي تبدأ بـ 'Ġ' كما هي: بهذه الطريقة، ستشير بداية الإزاحات إلى المسافة قبل الكلمة، وليس الحرف الأول من الكلمة (نظرًا لأن المسافة تعتبر جزءًا فنيًا من الرمز). دعونا نلقي نظرة على النتيجة مع النص الذي قمنا بتشفيره للتو، حيث `'Ġtest'` هو الرمز في الفهرس 4:

```python
sentence = "Let's test this tokenizer."
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[4]
sentence[start:end]
```

```python out
' test'
```

أخيرًا، نضيف محلل رموز على مستوى البايت:

```python
tokenizer.decoder = decoders.ByteLevel()
```

ويمكننا التأكد من أنه يعمل بشكل صحيح:

```python
tokenizer.decode(encoding.ids)
```

```python out
"Let's test this tokenizer."
```

رائع! الآن بعد أن انتهينا، يمكننا حفظ محلل الرموز كما فعلنا من قبل، وتغليفه في `PreTrainedTokenizerFast` أو `GPT2TokenizerFast` إذا أردنا استخدامه في مكتبة 🤗 Transformers:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
tokenizer_object=tokenizer,
bos_token="<|endoftext|>",
eos_token="<|endoftext|>",
)
```

أو:

```python
from transformers import GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)
```

وكمثال أخير، سنريكم كيفية بناء محلل رموز Unigram من الصفر.

## بناء محلل رموز Unigram من الصفر [[building-a-unigram-tokenizer-from-scratch]]

دعونا الآن نبني محلل رموز XLNet. وكما هو الحال مع محللات الرموز السابقة، نبدأ بتحديد واجهة برمجة التطبيقات API `Tokenizer` بنموذج Unigram:

```python
tokenizer = Tokenizer(models.Unigram())
```

مرة أخرى، يمكننا تهيئة هذا النموذج بقاموس إذا كان لدينا واحد.

بالنسبة للتحليل اللغوي، يستخدم XLNet بعض الاستبدالات (التي تأتي من SentencePiece):

```python
from tokenizers import Regex

tokenizer.normalizer = normalizers.Sequence(
[
normalizers.Replace("``", '"'),
normalizers.Replace("''", '"'),
normalizers.NFKD(),
normalizers.StripAccents(),
normalizers.Replace(Regex(" {2,}"), " "),
]
)
```

يستبدل هذا الرمز `<code>``</code>` و`<code>''</code>` بـ `<code>"</code>` وأي تسلسل لمسافتين أو أكثر بمسافة واحدة، بالإضافة إلى إزالة علامات الترقيم من النصوص التي سيتم تحليلها.

أداة ما قبل التحليل التي تستخدم لأي محلل رموز SentencePiece هي `Metaspace`:

```python
tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()
```

يمكننا إلقاء نظرة على مرحلة ما قبل التحليل لنص مثال كما هو موضح سابقًا:

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test the pre-tokenizer!")
```

```python out
[("▁Let's", (0, 5)), ('▁test', (5, 10)), ('▁the', (10, 14)), ('▁pre-tokenizer!', (14, 29))]
```

الخطوة التالية هي النموذج، الذي يحتاج إلى التدريب. يحتوي XLNet على عدد قليل من الرموز الخاصة:

```python
special_tokens = ["<cls>", "<sep>", "<unk>", "<pad>", "<mask>", "<s>", "</s>"]
trainer = trainers.UnigramTrainer(
vocab_size=25000, special_tokens=special_tokens, unk_token="<unk>"
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

هناك حجة مهمة للغاية يجب عدم نسيانها بالنسبة لـ `UnigramTrainer` وهي `unk_token`. يمكننا أيضًا تمرير الحجج الأخرى المحددة لخوارزمية Unigram، مثل `shrinking_factor` لكل خطوة نقوم فيها بإزالة الرموز (القيمة الافتراضية هي 0.75) أو `max_piece_length` لتحديد الطول الأقصى لرموز معينة (القيمة الافتراضية هي 16).

يمكن أيضًا تدريب هذا المحلل على ملفات نصية:

```python
tokenizer.model = models.Unigram()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

دعونا نلقي نظرة على تحليل عينة نص:

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['▁Let', "'", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.']
```

من خصائص XLNet أنه يضع رمز `<cls>` في نهاية الجملة، مع معرف نوع 2 (لتمييزه عن الرموز الأخرى). يتم التعبئة على اليسار، نتيجة لذلك. يمكننا التعامل مع جميع الرموز الخاصة ومعرفات نوع الرموز باستخدام قالب، كما هو الحال مع BERT، ولكن يجب علينا أولاً الحصول على معرفات رموز `<cls>` و`<sep>`:

```python
cls_token_id = tokenizer.token_to_id("<cls>")
sep_token_id = tokenizer.token_to_id("<sep>")
print(cls_token_id, sep_token_id)
```

```python out
0 1
```

يبدو القالب على النحو التالي:

```python
tokenizer.post_processor = processors.TemplateProcessing(
single="$A:0 <sep>:0 <cls>:2",
pair="$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2",
special_tokens=[("<sep>", sep_token_id), ("<cls>", cls_token_id)],
)
```

ويمكننا اختباره للتأكد من أنه يعمل عن طريق تشفير زوج من الجمل:

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences!")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['▁Let', "'", 's', '▁test', '▁this', '▁to', 'ken', 'izer', '.', '.', '.', '<sep>', '▁', 'on', '▁', 'a', '▁pair',
'▁of', '▁sentence', 's', '!', '<sep>', '<cls>']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]
```

أخيرًا، نضيف محلل رموز `Metaspace`:

```python
tokenizer.decoder = decoders.Metaspace()
```

والآن انتهينا من محلل الرموز هذا! يمكننا حفظ محلل الرموز كما فعلنا من قبل، وتغليفه في `PreTrainedTokenizerFast` أو `XLNetTokenizerFast` إذا أردنا استخدامه في مكتبة 🤗 Transformers. أحد الأمور التي يجب ملاحظتها عند استخدام `PreTrainedTokenizerFast` هي أنه بالإضافة إلى الرموز الخاصة، يجب علينا إخبار مكتبة 🤗 Transformers بالتعبئة على اليسار:

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
tokenizer_object=tokenizer,
bos_token="<s>",
eos_token="</s>",
unk_token="<unk>",
pad_token="<pad>",
cls_token="<cls>",
sep_token="<sep>",
mask_token="<mask>",
padding_side="left",
)
```

أو كبديل:

```python
from transformers import XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)
```

الآن بعد أن رأيت كيف يتم استخدام كتل البناء المختلفة لبناء محللات الرموز الموجودة، يجب أن تكون قادرًا على كتابة أي محلل رموز تريده باستخدام مكتبة 🤗 Tokenizers واستخدامه في مكتبة 🤗 Transformers.