# الملخص

في هذا القسم، سنلقي نظرة على كيفية استخدام نماذج المحول لاختصار الوثائق الطويلة في ملخصات، وهي مهمة تُعرف باسم _تلخيص النص_. تعد هذه المهمة واحدة من أكثر مهام معالجة اللغات الطبيعية تحديًا لأنها تتطلب مجموعة من القدرات، مثل فهم المقاطع الطويلة وتوليد نص متماسك يلخص الموضوعات الرئيسية في الوثيقة. ومع ذلك، عندما يتم تنفيذه بشكل جيد، يعد تلخيص النص أداة قوية يمكنها تسريع مختلف العمليات التجارية عن طريق تخفيف عبء خبراء المجال من قراءة الوثائق الطويلة بالتفصيل.

على الرغم من وجود العديد من النماذج المعدلة مسبقًا للتلخيص على [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=summarization&sort=downloads)، إلا أن جميعها تقريبًا مناسبة للوثائق الإنجليزية فقط. لذا، لإضافة لمسة تحدي في هذا القسم، سنقوم بتدريب نموذج ثنائي اللغة للغة الإنجليزية والإسبانية. وبنهاية هذا القسم، سيكون لديك [نموذج](https://huggingface.co/huggingface-course/mt5-small-finetuned-amazon-en-es) يمكنه تلخيص مراجعات العملاء مثل المراجعة الموضحة هنا:

كما سنرى، هذه الملخصات موجزة لأنها مستفادة من العناوين التي يقدمها العملاء في مراجعاتهم للمنتجات. دعونا نبدأ بتجميع مجموعة بيانات ثنائية اللغة مناسبة لهذه المهمة.
## إعداد مجموعة بيانات متعددة اللغات 

سنستخدم مجموعة بيانات "Multilingual Amazon Reviews Corpus" لإنشاء ملخص ثنائي اللغة. تتكون هذه المجموعة من مراجعات منتجات Amazon بست لغات، وتُستخدم عادةً لمعايرة المصنفات متعددة اللغات. ومع ذلك، نظرًا لأن كل مراجعة مصحوبة بعنوان قصير، فيمكننا استخدام العناوين كملخصات مستهدفة لتعلم النموذج الخاص بنا! للبدء، دعنا نقوم بتنزيل المجموعات الفرعية الإنجليزية والإسبانية من منصة Hugging Face Hub:

```python
from datasets import load_dataset

spanish_dataset = load_dataset("amazon_reviews_multi", "es")
english_dataset = load_dataset("amazon_reviews_multi", "en")
english_dataset
```

كما ترى، هناك 200,000 مراجعة لكل لغة لمجموعة التدريب، و5,000 مراجعة لكل من مجموعات التحقق والاختبار. تتوفر معلومات المراجعة التي تهمنا في عمودي "review_body" و"review_title". دعنا نلقي نظرة على بعض الأمثلة من خلال إنشاء دالة بسيطة تأخذ عينة عشوائية من مجموعة التدريب باستخدام التقنيات التي تعلمناها في [الفصل 5](/course/chapter5):

```python
def show_samples(dataset, num_samples=3, seed=42):
    sample = dataset["train"].shuffle(seed=seed).select(range(num_samples))
    for example in sample:
        print(f"\n'>> العنوان: {example['review_title']}'")
        print(f"'>> المراجعة: {example['review_body']}'")

show_samples(english_dataset)
```

```python out
'>> العنوان: Worked in front position, not rear'
'>> المراجعة: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.'

'>> العنوان: meh'
'>> المراجعة: Does it’s job and it’s gorgeous but mine is falling apart, I had to basically put it together again with hot glue'

'>> العنوان: Can't beat these for the money'
'>> المراجعة: Bought this for handling miscellaneous aircraft parts and hanger "stuff" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn't get brittle and split like my older plastic drawers did. I like the all-plastic construction. It's heavy duty enough to hold metal parts, but being made of plastic it's not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can't beat it. Best one of these I've bought to date-- and I've been using some version of these for over forty years.'
```

<Tip>

✏️ **جربها!** قم بتغيير البذرة العشوائية في أمر `Dataset.shuffle()` لاستكشاف مراجعات أخرى في المجموعة. إذا كنت تتحدث الإسبانية، فتفقد بعض المراجعات في `spanish_dataset` لمعرفة ما إذا كانت العناوين تبدو أيضًا ملخصات معقولة.

</Tip>

تُظهر هذه العينة تنوع المراجعات التي عادةً ما نجدها عبر الإنترنت، والتي تتراوح من الإيجابية إلى السلبية (وكل شيء بينهما!). على الرغم من أن المثال الذي يحمل عنوان "meh" ليس مفيدًا جدًا، إلا أن العناوين الأخرى تبدو ملخصات جيدة للمراجعات نفسها. سيستغرق تدريب نموذج الملخص على جميع المراجعات البالغ عددها 400,000 وقتًا طويلاً جدًا على وحدة معالجة رسومات (GPU) واحدة، لذلك بدلاً من ذلك، سنركز على توليد ملخصات لمجال منتجات واحد. لمعرفة المجالات التي يمكننا الاختيار منها، دعنا نحول `english_dataset` إلى `pandas.DataFrame` ونحسب عدد المراجعات لكل فئة منتج:

```python
english_dataset.set_format("pandas")
english_df = english_dataset["train"][:]
# إظهار عدد المراجعات لأفضل 20 منتجًا
english_df["product_category"].value_counts()[:20]
```

```python out
home                      17679
apparel                   15951
wireless                  15717
other                     13418
beauty                    12091
drugstore                 11730
kitchen                   10382
toy                        8745
sports                     8277
automotive                 7506
lawn_and_garden            7327
home_improvement           7136
pet_products               7082
digital_ebook_purchase     6749
pc                         6401
electronics                6186
office_product             5521
shoes                      5197
grocery                    4730
book                       3756
Name: product_category, dtype: int64
```

المنتجات الأكثر شعبية في مجموعة البيانات الإنجليزية هي تلك المتعلقة بالمنزل والملابس والإلكترونيات اللاسلكية. ومع ذلك، للالتزام بموضوع Amazon، دعنا نركز على تلخيص مراجعات الكتب - ففي النهاية، هذه هي الشركة التي تأسست عليها! يمكننا أن نرى فئتين من المنتجات تناسب الفاتورة (`book` و`digital_ebook_purchase`)، لذلك دعنا نقوم بتصفية المجموعات الفرعية للغات الإنجليزية والإسبانية لهذه المنتجات فقط. كما رأينا في [الفصل 5](/course/chapter5)، تسمح لنا دالة `Dataset.filter()` بتقسيم مجموعة البيانات بكفاءة، لذلك يمكننا تعريف دالة بسيطة للقيام بذلك:

```python
def filter_books(example):
    return (
        example["product_category"] == "book"
        or example["product_category"] == "digital_ebook_purchase"
    )
```

الآن، عندما نطبق هذه الدالة على `english_dataset` و`spanish_dataset`، ستتضمن النتيجة فقط الصفوف المتعلقة بفئات الكتب. قبل تطبيق التصفية، دعنا نبدل تنسيق `english_dataset` من `"pandas"` إلى `"arrow"`:

```python
english_dataset.reset_format()
```

بعد ذلك، يمكننا تطبيق دالة التصفية، وكإجراء تحقق، دعنا نلقي نظرة على بعض المراجعات للتأكد من أنها تتحدث بالفعل عن الكتب:

```python
spanish_books = spanish_dataset.filter(filter_books)
english_books = english_dataset.filter(filter_books)
show_samples(english_books)
```

```python out
'>> العنوان: I'm dissapointed.'
'>> المراجعة: I guess I had higher expectations for this book from the reviews. I really thought I'd at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I'm dissapointed.'

'>> العنوان: Good art, good price, poor design'
'>> المراجعة: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it's less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar'

'>> العنوان: Helpful'
'>> المراجعة: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.'
```

حسنًا، يمكننا أن نرى أن المراجعات لا تتعلق بالكتب بشكل صارم وقد تشير إلى أشياء مثل التقاويم والتطبيقات الإلكترونية مثل OneNote. ومع ذلك، يبدو أن المجال مناسب لتدريب نموذج الملخص. قبل أن نلقي نظرة على النماذج المختلفة المناسبة لهذه المهمة، هناك خطوة تحضير بيانات أخيرة يجب القيام بها: وهي دمج المراجعات الإنجليزية والإسبانية في كائن `DatasetDict` واحد. توفر مكتبة 🤗 Datasets دالة `concatenate_datasets()` مفيدة (كما يوحي الاسم) والتي ستكدس كائني `Dataset` فوق بعضهما البعض. لذلك، لإنشاء مجموعة البيانات ثنائية اللغة الخاصة بنا، سنقوم بالحلق على كل مجموعة، ونقوم بدمج مجموعات البيانات لتلك المجموعة، ونخلط النتيجة لضمان عدم تعلم النموذج الخاص بنا بلغة واحدة فقط:

```python
from datasets import concatenate_datasets, DatasetDict

books_dataset = DatasetDict()

for split in english_books.keys():
    books_dataset[split] = concatenate_datasets(
        [english_books[split], spanish_books[split]]
    )
    books_dataset[split] = books_dataset[split].shuffle(seed=42)

# إلقاء نظرة على بعض الأمثلة
show_samples(books_dataset)
```

```python out
'>> العنوان: Easy to follow!!!!'
'>> المراجعة: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.'

'>> العنوان: PARCIALMENTE DAÑADO'
'>> المراجعة: Me llegó el día que tocaba, junto a otros libros que pedí, pero la caja llegó en mal estado lo cual dañó las esquinas de los libros porque venían sin protección (forro).'

'>> العنوان: no lo he podido descargar'
'>> المراجعة: igual que el anterior'
```

يبدو هذا بالتأكيد مزيجًا من المراجعات الإنجليزية والإسبانية! الآن بعد أن أصبح لدينا مجموعة بيانات تدريب، فإن الشيء الأخير الذي يجب التحقق منه هو توزيع الكلمات في المراجعات وعناوينها. هذا أمر مهم بشكل خاص لمهمة الملخص، حيث يمكن أن تحيز الملخصات المرجعية القصيرة جدًا في البيانات النموذج لإخراج كلمة واحدة أو كلمتين فقط في الملخصات المولدة. توضح الرسوم البيانية أدناه توزيعات الكلمات، ويمكننا أن نرى أن العناوين منحرفة بشدة نحو 1-2 كلمة فقط:

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths.svg" alt="توزيعات عدد الكلمات للعناوين والمراجعات."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths-dark.svg" alt="توزيعات عدد الكلمات للعناوين والمراجعات."/>
</div>

للتعامل مع هذا، سنقوم بتصفية الأمثلة التي تحتوي على عناوين قصيرة جدًا حتى يتمكن نموذجنا من إنتاج ملخصات أكثر إثارة للاهتمام. نظرًا لأننا نتعامل مع نصوص باللغة الإنجليزية والإسبانية، فيمكننا استخدام اختصار تقريبي لتقسيم العناوين على المسافات، ثم استخدام دالة `Dataset.filter()` الموثوقة كما يلي:

```python
books_dataset = books_dataset.filter(lambda x: len(x["review_title"].split()) > 2)
```

الآن بعد أن قمنا بإعداد مجموعة البيانات الخاصة بنا، دعنا نلقي نظرة على بعض نماذج Transformer المحتملة التي يمكن ضبط دقتها عليها!
## نماذج لتلخيص النص

إذا فكرت في الأمر، فإن تلخيص النص هو مهمة مشابهة إلى حد ما للترجمة الآلية: لدينا نص مثل مراجعة نرغب في "ترجمتها" إلى نسخة أقصر تلتقط الميزات البارزة للنص الأصلي. وبناءً على ذلك، تعتمد معظم نماذج المحول للتلخيص بنية الترميز فك الترميز التي واجهناها لأول مرة في [الفصل 1](/course/chapter1)، على الرغم من وجود بعض الاستثناءات مثل عائلة نماذج GPT التي يمكن استخدامها أيضًا للتلخيص في إعدادات قليلة التصوير. يسرد الجدول التالي بعض النماذج المسبقة التدريب الشائعة التي يمكن ضبط دقتها للتلخيص.

| نموذج المحول | الوصف                                                                                                                                                                                                  | متعدد اللغات؟ |
| :---------: | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-----------: |
|    [GPT-2](https://huggingface.co/gpt2-xl)    | على الرغم من تدريبه كنموذج لغة توليدي ذاتي، يمكنك جعل GPT-2 يقوم بتوليد ملخصات عن طريق إضافة "TL؛ DR" في نهاية نص الإدخال.                                                          |      ❌       |
|   [PEGASUS](https://huggingface.co/google/pegasus-large)   | يستخدم هدف التدريب المسبق للتنبؤ بالجمل المقنعة في نصوص متعددة الجمل. هدف التدريب المسبق هذا أقرب إلى التلخيص من نمذجة اللغة العادية ويحقق نتائج عالية في المعايير القياسية الشهيرة. |      ❌       |
|     [T5](https://huggingface.co/t5-base)      | بنية محول عالمي تصوغ جميع المهام في إطار نص إلى نص؛ على سبيل المثال، يكون تنسيق الإدخال للنموذج لتلخيص مستند هو `summarize: ARTICLE`.                                   |      ❌       |
|     [mT5](https://huggingface.co/google/mt5-base)     | إصدار متعدد اللغات من T5، تم التدريب المسبق عليه على مجموعة بيانات Common Crawl متعددة اللغات (mC4)، والتي تغطي 101 لغة.                                                  |      ✅       |
|    [BART](https://huggingface.co/facebook/bart-base)     | بنية محول جديدة بها مكدس ترميز وفك ترميز تم تدريبه لإعادة بناء الإدخال التالف الذي يجمع بين مخططات التدريب المسبق لـ BERT وGPT-2.                                  |      ❌       |
|  [mBART-50](https://huggingface.co/facebook/mbart-large-50)   | إصدار متعدد اللغات من BART، تم التدريب المسبق عليه على 50 لغة.                                                                                                                                 |      ✅       |

كما ترون من هذا الجدول، فإن غالبية نماذج المحول للتلخيص (وفي الواقع معظم مهام معالجة اللغات الطبيعية) أحادية اللغة. هذا رائع إذا كانت مهمتك بلغة "غنية بالموارد" مثل الإنجليزية أو الألمانية، ولكن ليس كثيرًا للآلاف من اللغات الأخرى المستخدمة في جميع أنحاء العالم. لحسن الحظ، هناك فئة من نماذج المحول متعددة اللغات، مثل mT5 وmBART، التي تأتي لإنقاذ الموقف. يتم التدريب المسبق لهذه النماذج باستخدام نمذجة اللغة، ولكن مع إضافة: بدلاً من التدريب على مجموعة بيانات بلغة واحدة، يتم تدريبها بشكل مشترك على نصوص بأكثر من 50 لغة في نفس الوقت!

سنركز على mT5، وهو بنية مثيرة للاهتمام تعتمد على T5 تم التدريب المسبق عليها في إطار نص إلى نص. في T5، يتم صياغة كل مهمة من مهام معالجة اللغات الطبيعية من حيث بادئة موجهة مثل `summarize:` والتي تجعل النموذج يتكيف مع النص المولد وفقًا للموجه. كما هو موضح في الشكل أدناه، يجعل هذا T5 متعدد الاستخدامات للغاية، حيث يمكنك حل العديد من المهام باستخدام نموذج واحد!

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5.svg" alt="المهام المختلفة التي تقوم بها بنية T5."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5-dark.svg" alt="المهام المختلفة التي تقوم بها بنية T5."/>
</div>

لا يستخدم mT5 البادئات، ولكنه يشارك الكثير من مرونة T5 ويتمتع بميزة كونه متعدد اللغات. الآن بعد أن اخترنا نموذجًا، دعنا نلقي نظرة على إعداد بياناتنا للتدريب.

<Tip>

✏️ **جربه!** بمجرد الانتهاء من هذا القسم، راجع مدى جودة أداء mT5 مقارنة بـ mBART عن طريق ضبط دقة الأخير باستخدام نفس التقنيات. للحصول على نقاط المكافأة، يمكنك أيضًا تجربة ضبط دقة T5 على مراجعات اللغة الإنجليزية فقط. نظرًا لأن T5 لديه موجه بادئة خاص، ستحتاج إلى إضافة بادئة `summarize:` إلى أمثلة الإدخال في خطوات المعالجة المسبقة أدناه.

</Tip>

## معالجة البيانات مسبقًا

<Youtube id="1m7BerpSq8A"/>

مهمتنا التالية هي توكيل مراجعنا وعناوينها وترميزها. كالعادة، نبدأ بتحميل الرامزة المرتبطة بنقطة التحقق من النموذج المسبق التدريب. سنستخدم `mt5-small` كنقطة مرجعية لدينا حتى نتمكن من ضبط دقة النموذج في غضون فترة زمنية معقولة:

```python
from transformers import AutoTokenizer

model_checkpoint = "google/mt5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

<Tip>

💡 في المراحل الأولى من مشاريع معالجة اللغات الطبيعية، من الجيد تدريب فئة من النماذج "الصغيرة" على عينة صغيرة من البيانات. يتيح لك ذلك تصحيح الأخطاء وتكرارها بشكل أسرع نحو سير عمل شامل. بمجرد أن تشعر بالثقة في النتائج، يمكنك دائمًا زيادة حجم النموذج عن طريق تغيير نقطة التحقق من النموذج ببساطة!

</Tip>

دعونا نجرب رموز mT5 على مثال صغير:

```python
inputs = tokenizer("I loved reading the Hunger Games!")
inputs
```

```python out
{'input_ids': [336, 259, 28387, 11807, 287, 62893, 295, 12507, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

هنا يمكننا أن نرى `input_ids` و`attention_mask` المألوفة التي واجهناها في أولى تجاربنا في الضبط الدقيق مرة أخرى في [الفصل 3](/course/chapter3). دعونا نقوم بفك ترميز معرفات الإدخال هذه باستخدام وظيفة `convert_ids_to_tokens()` الخاصة بالرامزة لمعرفة نوع الرامزة التي نتعامل معها:

```python
tokenizer.convert_ids_to_tokens(inputs.input_ids)
```

```python out
['▁I', '▁', 'loved', '▁reading', '▁the', '▁Hung', 'er', '▁Games', '</s>']
```

يشير الرمز الخاص يونيكود `▁` ورمز نهاية التسلسل `</s>` إلى أننا نتعامل مع رامزة SentencePiece، والتي تستند إلى خوارزمية تجزئة Unigram التي نوقشت في [الفصل 6](/course/chapter6). تعد Unigram مفيدة بشكل خاص لمجموعات البيانات متعددة اللغات لأنها تسمح لـ SentencePiece بأن تكون غير متحيزة تجاه علامات الترقيم والتشديد، وحقيقة أن العديد من اللغات، مثل اليابانية، لا تحتوي على أحرف مسافة.

لتوكيل مجموعتنا البيانات، يتعين علينا التعامل مع دقة مرتبطة بالتلخيص: نظرًا لأن علاماتنا أيضًا نص، فمن المحتمل أن تتجاوز حجم السياق الأقصى للنموذج. وهذا يعني أننا بحاجة إلى تطبيق الاقتطاع على كل من المراجع وعناوينها لضمان عدم تمرير إدخالات طويلة بشكل مفرط إلى نموذجنا. توفر الرامزات في مكتبة 🤗 Transformers حجة `text_target` مفيدة تسمح لك بتوكيل العلامات بالتوازي مع الإدخالات. فيما يلي مثال على كيفية معالجة الإدخالات والأهداف لـ mT5:

```python
max_input_length = 512
max_target_length = 30


def preprocess_function(examples):
    model_inputs = tokenizer(
    examples["review_body"],
    max_length=max_input_length,
    truncation=True,
    )
    labels = tokenizer(
    examples["review_title"], max_length=max_target_length, truncation=True
    )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```

دعونا نمر عبر هذا الكود لفهم ما يحدث. أول شيء قمنا به هو تحديد قيم لـ `max_input_length` و`max_target_length`، والتي تحدد الحدود العليا لطول مراجعنا وعناوينها. نظرًا لأن جسم المراجعة أكبر عادةً من العنوان، فقد قمنا بضبط هذه القيم وفقًا لذلك.

مع `preprocess_function()`، من السهل بعد ذلك توكيل المجموعة البيانات بأكملها باستخدام وظيفة `Dataset.map()` المفيدة التي استخدمناها على نطاق واسع طوال هذه الدورة التدريبية:

```python
tokenized_datasets = books_dataset.map(preprocess_function, batched=True)
```

الآن بعد أن تم معالجة مجموعة البيانات مسبقًا، دعنا نلقي نظرة على بعض المقاييس المستخدمة عادةً للتلخيص. كما سنرى، لا يوجد حل سحري عندما يتعلق الأمر بقياس جودة النص الذي تم إنشاؤه بواسطة الآلة.

<Tip>

💡 قد تكون لاحظت أننا استخدمنا `batched=True` في وظيفة `Dataset.map()` أعلاه. يقوم هذا بتشفير الأمثلة في دفعات من 1000 (افتراضيًا) ويسمح لك بالاستفادة من قدرات تعدد مؤشرات الترابط للرامزات السريعة في 🤗 Transformers. حيثما أمكن، حاول استخدام `batched=True` للاستفادة القصوى من معالجتك المسبقة!

</Tip>
## مقاييس لتقييم ملخص النص

بالمقارنة مع معظم المهام الأخرى التي غطيناها في هذه الدورة، فإن قياس أداء مهام توليد النص مثل الملخص أو الترجمة ليس مباشرًا بنفس القدر. على سبيل المثال، بالنظر إلى مراجعة مثل "أحببت قراءة مباريات الجوع"، هناك العديد من الملخصات الصحيحة، مثل "أحببت مباريات الجوع" أو "مباريات الجوع هي قراءة رائعة". من الواضح أن تطبيق نوع من المطابقة الدقيقة بين الملخص المولد والعلامة ليس حلاً جيدًا - حتى البشر سيؤدون أداءً سيئًا في ظل هذا المقياس، لأن لدينا جميعًا أسلوبنا الخاص في الكتابة.

بالنسبة للتلخيص، أحد أكثر المقاييس المستخدمة شيوعًا هو [درجة ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) (اختصار لـ Recall-Oriented Understudy for Gisting Evaluation). الفكرة الأساسية وراء هذا المقياس هي مقارنة ملخص مولد بمجموعة من الملخصات المرجعية التي يصنعها البشر عادةً. لجعل هذا أكثر دقة، دعنا نفترض أننا نريد مقارنة الملخصين التاليين:

```python
generated_summary = "I absolutely loved reading the Hunger Games"
reference_summary = "I loved reading the Hunger Games"
```

إحدى طرق مقارنتهما قد تكون بعدد الكلمات المتطابقة، والتي في هذه الحالة ستكون 6. ومع ذلك، فإن هذا الأمر بسيط بعض الشيء، لذا بدلاً من ذلك، تستند ROUGE إلى حساب درجات _الدقة_ و _التذكر_ للتداخل.

🙋 لا تقلق إذا كانت هذه هي المرة الأولى التي تسمع فيها عن الدقة والاستدعاء - فسوف نمر ببعض الأمثلة الصريحة معًا لتوضيح الأمر. تُواجه هذه المقاييس عادةً في مهام التصنيف، لذا إذا كنت تريد فهم كيفية تعريف الدقة والاستدعاء في هذا السياق، نوصي بالاطلاع على أدلة `scikit-learn` [هنا](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html).

بالنسبة لـ ROUGE، يقيس الاستدعاء مقدار الملخص المرجعي الذي تم التقاطه بواسطة الملخص المولد. إذا كنا نقارن فقط الكلمات، فيمكن حساب الاستدعاء وفقًا للمعادلة التالية:

$$ \mathrm{Recall} = \frac{\mathrm{Number\,of\,overlapping\, words}}{\mathrm{Total\, number\, of\, words\, in\, reference\, summary}} $$

بالنسبة لمثالنا البسيط أعلاه، تعطي هذه المعادلة استدعاءً مثاليًا يبلغ 6/6 = 1؛ أي أن جميع الكلمات في الملخص المرجعي أنتجها النموذج. قد يبدو هذا رائعًا، ولكن ماذا لو كان ملخصنا المولد هو "لقد أحببت حقًا قراءة مباريات الجوع طوال الليل". سيكون لهذا أيضًا استدعاء مثالي، ولكنه ملخص أسوأ لأنه مسرف في الكلام. لمعالجة هذه السيناريوهات، نحسب أيضًا الدقة، والتي تقيس في سياق ROUGE مدى ملاءمة الملخص المولد:

$$ \mathrm{Precision} = \frac{\mathrm{Number\,of\,overlapping\, words}}{\mathrm{Total\, number\, of\, words\, in\, generated\, summary}} $$

تطبيق هذا على ملخصنا المسرف يعطي دقة 6/10 = 0.6، وهو أسوأ بكثير من الدقة 6/7 = 0.86 التي حصلنا عليها في ملخصنا الأقصر. في الممارسة العملية، يتم عادةً حساب كل من الدقة والاستدعاء، ثم يتم الإبلاغ عن F1-score (المتوسط الهارموني للدقة والاستدعاء). يمكننا القيام بذلك بسهولة في 🤗 Datasets عن طريق تثبيت حزمة `rouge_score` أولاً:

```py
!pip install rouge_score
```

ثم تحميل مقياس ROUGE كما يلي:

```python
import evaluate

rouge_score = evaluate.load("rouge")
```

بعد ذلك، يمكننا استخدام وظيفة `rouge_score.compute()` لحساب جميع المقاييس مرة واحدة:

```python
scores = rouge_score.compute(
predictions=[generated_summary], references=[reference_summary]
)
scores
```

```python out
{'rouge1': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
'rouge2': AggregateScore(low=Score(precision=0.67, recall=0.8, fmeasure=0.73), mid=Score(precision=0.67, recall=0.8, fmeasure=0.73), high=Score(precision=0.67, recall=0.8, fmeasure=0.73)),
'rougeL': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
'rougeLsum': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92))}
```

يا له من أمر! هناك الكثير من المعلومات في هذا الإخراج - ماذا تعني كل هذه المعلومات؟ أولاً، يقوم 🤗 Datasets بالفعل بحساب فترات الثقة للدقة والاستدعاء وF1-score؛ هذه هي سمات `low` و`mid` و`high` التي يمكنك رؤيتها هنا. علاوة على ذلك، يحسب 🤗 Datasets مجموعة متنوعة من درجات ROUGE التي تستند إلى أنواع مختلفة من دقة النص عند مقارنة الملخصات المولدة والمرجعية. يمثل متغير `rouge1` تداخل أحادي الكلمة - هذه مجرد طريقة رائعة للقول بأن تداخل الكلمات هو بالضبط المقياس الذي ناقشناه أعلاه. للتحقق من ذلك، دعنا نستخرج قيمة `mid` من درجاتنا:

```python
scores["rouge1"].mid
```

```python out
Score(precision=0.86, recall=1.0, fmeasure=0.92)
```

رائع، تتوافق أرقام الدقة والاستدعاء! والآن ماذا عن تلك الدرجات ROUGE الأخرى؟ يقيس `rouge2` التداخل بين الكلمات الثنائية (فكر في تداخل أزواج الكلمات)، بينما يقيس `rougeL` و`rougeLsum` أطول تسلسلات متطابقة من الكلمات عن طريق البحث عن أطول السلاسل الفرعية المشتركة في الملخصات المولدة والمرجعية. تشير "المجموع" في `rougeLsum` إلى حقيقة أن هذا المقياس يتم حسابه على ملخص كامل، بينما يتم حساب `rougeL` كمتوسط على الجمل الفردية.

✏️ **جربه!** قم بإنشاء مثالك الخاص لملخص مولد ومرجعي وتحقق مما إذا كانت درجات ROUGE تتوافق مع الحساب اليدوي بناءً على الصيغ للدقة والاستدعاء. للحصول على نقاط المكافأة، قم بتقسيم النص إلى كلمات ثنائية وقارن الدقة والاستدعاء لمقياس `rouge2`.

سنستخدم هذه الدرجات ROUGE لتتبع أداء نموذجنا، ولكن قبل القيام بذلك، دعنا نفعل شيئًا يجب على كل ممارس NLP جيد فعله: إنشاء خط أساس قوي وبسيط!

### إنشاء خط أساس قوي

خط الأساس الشائع لتلخيص النص هو ببساطة أخذ الجمل الثلاث الأولى من المقال، والتي يشار إليها غالبًا باسم خط الأساس _lead-3_. يمكننا استخدام علامات الترقيم الكاملة لتتبع حدود الجملة، ولكن هذا سيفشل في المختصرات مثل "الولايات المتحدة" أو "الأمم المتحدة" - لذا بدلاً من ذلك، سنستخدم مكتبة `nltk`، والتي تتضمن خوارزمية أفضل للتعامل مع هذه الحالات. يمكنك تثبيت الحزمة باستخدام `pip` كما يلي:

```python
!pip install nltk
```

ثم قم بتنزيل قواعد علامات الترقيم:

```python
import nltk

nltk.download("punkt")
```

بعد ذلك، نقوم باستيراد محلل الجمل من `nltk` وإنشاء دالة بسيطة لاستخراج الجمل الثلاث الأولى في المراجعة. الاتفاقية في تلخيص النص هي فصل كل ملخص بسطر جديد، لذا دعنا نضمن ذلك ونجربه على مثال تدريبي:

```python
from nltk.tokenize import sent_tokenize


def three_sentence_summary(text):
return "\n".join(sent_tokenize(text)[:3])


print(three_sentence_summary(books_dataset["train"][1]["review_body"]))
```

```python out
'I grew up reading Koontz, and years ago, I stopped,convinced i had "outgrown" him.'
'Still,when a friend was looking for something suspenseful too read, I suggested Koontz.'
'She found Strangers.'
```

يبدو أن هذا يعمل، لذا دعنا الآن ننفذ دالة تستخرج هذه "الملخصات" من مجموعة بيانات وتحسب درجات ROUGE لخط الأساس:

```python
def evaluate_baseline(dataset, metric):
summaries = [three_sentence_summary(text) for text in dataset["review_body"]]
return metric.compute(predictions=summaries, references=dataset["review_title"])
```

بعد ذلك، يمكننا استخدام هذه الدالة لحساب درجات ROUGE على مجموعة التحقق من الصحة وتنسيقها قليلاً باستخدام Pandas:

```python
import pandas as pd

score = evaluate_baseline(books_dataset["validation"], rouge_score)
rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names)
rouge_dict
```

```python out
{'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96}
```

يمكننا أن نرى أن درجة `rouge2` أقل بكثير من الباقي؛ وهذا يعكس على الأرجح حقيقة أن عناوين المراجعة موجزة للغاية، وبالتالي فإن خط الأساس lead-3 مسرف في الكلام. الآن بعد أن أصبح لدينا خط أساس جيد للعمل منه، دعنا نوجه اهتمامنا نحو ضبط نموذج mT5!
## الضبط الدقيق لـ mT5 باستخدام Keras

إن ضبط نموذج بدقة لمهمة تلخيص النصوص يشبه إلى حد كبير المهام الأخرى التي تمت تغطيتها في هذا الفصل. أول شيء نحتاج إلى فعله هو تحميل النموذج المُدرب مسبقًا من نقطة تفتيش "mt5-small". نظرًا لأن التلخيص هو مهمة تسلسل إلى تسلسل، فيمكننا تحميل النموذج باستخدام فئة "TFAutoModelForSeq2SeqLM"، والتي ستقوم تلقائيًا بتنزيل الأوزان وتخزينها مؤقتًا:

```python
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

<Tip>
💡 إذا كنت تتساءل عن سبب عدم ظهور أي تحذيرات حول ضبط النموذج بدقة على مهمة أسفل النهر، فذلك لأننا نحتفظ بجميع أوزان الشبكة في المهام تسلسل إلى تسلسل. قارن ذلك بنموذج التصنيف النصي في [الفصل 3](/course/chapter3)، حيث تم استبدال رأس النموذج المُدرب مسبقًا بشبكة مُستهلة بشكل عشوائي.
</Tip>

الشيء التالي الذي نحتاج إلى فعله هو تسجيل الدخول إلى Hugging Face Hub. إذا كنت تشغل هذا الكود في دفتر ملاحظات، فيمكنك القيام بذلك باستخدام دالة المساعدة التالية:

```python
from huggingface_hub import notebook_login

notebook_login()
```

والتي ستعرض أداة يمكنك من خلالها إدخال بيانات اعتمادك. أو يمكنك تشغيل هذا الأمر في المحطة الطرفية الخاصة بك وتسجيل الدخول هناك:

```
huggingface-cli login
```

سنحتاج إلى توليد ملخصات من أجل حساب درجات ROUGE أثناء التدريب. لحسن الحظ، توفر مكتبة 🤗 Transformers فئات مخصصة `Seq2SeqTrainingArguments` و`Seq2SeqTrainer` يمكنها القيام بذلك تلقائيًا! ولرؤية كيفية عمل ذلك، دعونا نقوم أولاً بتعريف فرط المعاملات وحجج أخرى لتجاربنا:

```python
from transformers import Seq2SeqTrainingArguments

batch_size = 8
num_train_epochs = 8
# إظهار خسارة التدريب مع كل حقبة
logging_steps = len(tokenized_datasets["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

args = Seq2SeqTrainingArguments(
output_dir=f"{model_name}-finetuned-amazon-en-es",
evaluation_strategy="epoch"،
learning_rate=5.6e-5,
per_device_train_batch_size=batch_size,
per_device_eval_batch_size=batch_size,
weight_decay=0.01,
save_total_limit=3,
num_train_epochs=num_train_epochs,
predict_with_generate=True,
logging_steps=logging_steps,
push_to_hub=True,
)
```

هنا، تم تعيين وسيط `predict_with_generate` للإشارة إلى أنه يجب علينا توليد ملخصات أثناء التقييم حتى نتمكن من حساب درجات ROUGE لكل حقبة. كما نوقش في [الفصل 1](/course/chapter1)، يقوم فك الترميز بالاستدلال عن طريق التنبؤ بالرموز واحدًا تلو الآخر، وهذا يتم تنفيذه بواسطة طريقة `generate()` للنموذج. يخبر تعيين `predict_with_generate=True` المدرب `Seq2SeqTrainer` باستخدام تلك الطريقة للتقييم. لقد قمنا أيضًا بتعديل بعض فرط المعلمات الافتراضية، مثل معدل التعلم وعدد الحقبات وتناقص الوزن، وقمنا بتعيين خيار `save_total_limit` لحفظ ما يصل إلى 3 نقاط تفتيش أثناء التدريب - وذلك لأن حتى الإصدار "الصغير" من mT5 يستخدم حوالي جيجابايت من مساحة القرص الصلب، ويمكننا توفير بعض المساحة عن طريق الحد من عدد النسخ التي نحفظها.

وسيسمح لنا وسيط `push_to_hub=True` بدفع النموذج إلى Hub بعد التدريب؛ وستجد المستودع ضمن ملفك الشخصي في الموقع الذي حدده `output_dir`. لاحظ أنه يمكنك تحديد اسم المستودع الذي تريد دفعه باستخدام وسيط `hub_model_id` (على وجه الخصوص، سيتعين عليك استخدام هذا الوسيط للدفع إلى منظمة). على سبيل المثال، عندما قمنا بدفع النموذج إلى منظمة [`huggingface-course`](https://huggingface.co/huggingface-course)، أضفنا `hub_model_id="huggingface-course/mt5-finetuned-amazon-en-es"` إلى `Seq2SeqTrainingArguments`.

الشيء التالي الذي نحتاج إلى فعله هو تزويد المدرب بوظيفة `compute_metrics()` حتى نتمكن من تقييم نموذجنا أثناء التدريب. بالنسبة للتلخيص، فإن هذا الأمر أكثر تعقيدًا من مجرد استدعاء `rouge_score.compute()` على تنبؤات النموذج، نظرًا لأننا نحتاج إلى فك تشفير المخرجات والعلامات إلى نص قبل أن نتمكن من حساب درجات ROUGE. وتقوم الوظيفة التالية بذلك بالضبط، كما تستفيد من وظيفة `sent_tokenize()` من `nltk` لفصل جمل الملخص بأسطر جديدة:

```python
import numpy as np


def compute_metrics(eval_pred):
predictions, labels = eval_pred
# فك تشفير الملخصات المولدة إلى نص
decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
# استبدل -100 في العلامات لأننا لا نستطيع فك تشفيرها
labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
# فك تشفير الملخصات المرجعية إلى نص
decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
# تتوقع ROUGE وجود سطر جديد بعد كل جملة
decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
# حساب درجات ROUGE
result = rouge_score.compute(
predictions=decoded_preds, references=decoded_labels, use_stemmer=True
)
# استخراج الدرجات المتوسطة
result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
return {k: round(v, 4) for k, v in result.items()}
```

بعد ذلك، نحتاج إلى تعريف جامع بيانات لمهمة تسلسل إلى تسلسل. نظرًا لأن mT5 هو نموذج محول ترميز فك ترميز، فإن إحدى الدقائق في إعداد دفعاتنا هي أنه أثناء فك الترميز، نحتاج إلى تحويل العلامات إلى اليمين. هذا مطلوب لضمان أن فك الترميز لا يرى سوى العلامات الأرضية الحقيقية السابقة وليس الحالية أو المستقبلية، والتي سيكون من السهل على النموذج حفظها. هذا مشابه لكيفية تطبيق الاهتمام الذاتي المقنع على المدخلات في مهمة مثل [نمذجة اللغة السببية](/course/chapter7/6).

لحسن الحظ، توفر مكتبة 🤗 Transformers جامع بيانات `DataCollatorForSeq2Seq` الذي سيقوم تلقائيًا بتبطين المدخلات والعلامات بالنسبة لنا. لتهيئة جامع البيانات هذا، كل ما نحتاج إلى توفيره هو `tokenizer` و`model`:

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

دعونا نرى ما ينتجه جامع البيانات هذا عند إطعامه بمجموعة صغيرة من الأمثلة. أولاً، نحتاج إلى إزالة الأعمدة ذات السلاسل النصية لأن جامع البيانات لن يعرف كيفية تبطين هذه العناصر:

```python
tokenized_datasets = tokenized_datasets.remove_columns(
books_dataset["train"].column_names
)
```

نظرًا لأن جامع البيانات يتوقع قائمة من القواميس، حيث يمثل كل قاموس مثالًا واحدًا في مجموعة البيانات، فنحن بحاجة أيضًا إلى تنظيم البيانات في التنسيق المتوقع قبل تمريرها إلى جامع البيانات:

```python
features = [tokenized_datasets["train"][i] for i in range(2)]
data_collator(features)
```

```python out
{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[  1494,    259,   8622,    390,    259,    262,   2316,   3435,    955,
772,    281,    772,   1617,    263,    305,  14701,    260,   1385,
3031,    259,  24146,    332,   1037,    259,  43906,    305,    336,
260,      1,      0,      0,      0,      0,      0,      0],
[   259,  27531,  13483,    259,   7505,    260, 112240,  15192,    305,
53198,    276,    259,  74060,    263,    260,    459,  25640,    776,
2119,    336,    259,   2220,    259,  18896,    288,   4906,    288,
1037,   3931,    260,   7083, 101476,   1143,    260,      1]]), 'labels': tensor([[ 7483,   259,  2364, 15695,     1,  -100],
[  259, 27531, 13483,   259,  7505,     1]]), 'decoder_input_ids': tensor([[    0,  7483,   259,  2364, 15695,     1],
[    0,   259, 27531, 13483,   259,  7505]])}
```

الشيء الرئيسي الذي يجب ملاحظته هنا هو أن المثال الأول أطول من الثاني، لذلك تم تبطين `input_ids` و`attention_mask` للمثال الثاني على اليمين باستخدام رمز `[PAD]` (الذي يحمل الرقم التعريفي 0). وبالمثل، يمكننا أن نرى أن `labels` تم تبطينها باستخدام -100s، للتأكد من أن رموز التبطين يتم تجاهلها بواسطة دالة الخسارة. وأخيرًا، يمكننا أن نرى `decoder_input_ids` جديدًا تم فيه تحويل العلامات إلى اليمين عن طريق إدراج رمز `[PAD]` في الإدخال الأول.

أخيرًا، لدينا جميع المكونات التي نحتاجها للتدريب! كل ما نحتاج إلى فعله الآن هو تهيئة المدرب بالحجج القياسية:

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
model,
args,
train_dataset=tokenized_datasets["train"],
eval_dataset=tokenized_datasets["validation"],
data_collator=data_collator,
tokenizer=tokenizer,
compute_metrics=compute_metrics,
)
```

وإطلاق عملية التدريب:

```python
trainer.train()
```

أثناء التدريب، يجب أن تشاهد انخفاض خسارة التدريب وزيادة درجات ROUGE مع كل حقبة. بمجرد اكتمال التدريب، يمكنك معرفة درجات ROUGE النهائية عن طريق تشغيل `Trainer.evaluate()`:

```python
trainer.evaluate()
```

```python out
{'eval_loss': 3.028524398803711,
'eval_rouge1': 16.9728,
'eval_rouge2': 8.2969,
'eval_rougeL': 16.8366,
'eval_rougeLsum': 16.851,
'eval_gen_len': 10.1597,
'eval_runtime': 6.1054,
'eval_samples_per_second': 38.982,
'eval_steps_per_second': 4.914}
```

من الدرجات، يمكننا أن نرى أن نموذجنا قد تفوق بسهولة على خط الأساس الخاص بنا - رائع! والشيء الأخير الذي يجب فعله هو دفع أوزان النموذج إلى Hub، كما يلي:

```
trainer.push_to_hub(commit_message="Training complete", tags="summarization")
```

```python out
'https://huggingface.co/huggingface-course/mt5-finetuned-amazon-en-es/commit/aa0536b829b28e73e1e4b94b8a5aacec420d40e0'
```

سيؤدي هذا إلى حفظ ملفات نقطة التفتيش وملفات التكوين في `output_dir`، قبل تحميل جميع الملفات إلى Hub. من خلال تحديد وسيط `tags`، نضمن أيضًا أن الأداة الإضافية في Hub ستكون أداة تلخيص بدلاً من الأداة الإضافية الافتراضية لتوليد النصوص المرتبطة بمعمارية mT5 (لمزيد من المعلومات حول علامات النموذج، راجع [وثائق 🤗 Hub](https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-determined)). والناتج من `trainer.push_to_hub()` هو عنوان URL لالتزام Git، لذلك يمكنك بسهولة رؤية التغييرات التي تم إجراؤها على مستودع النموذج!

لإنهاء هذا القسم، دعونا نلقي نظرة على كيفية ضبط mT5 بدقة باستخدام الميزات منخفضة المستوى التي توفرها 🤗 Accelerate.

نحن على استعداد للتدريب! كل ما نحتاجه هو تحويل مجموعات البيانات الخاصة بنا إلى `tf.data.Dataset`s باستخدام جامع البيانات الذي حددناه أعلاه، ثم `compile()` و`fit()` النموذج. أولاً، مجموعات البيانات:

```python
tf_train_dataset = model.prepare_tf_dataset(
tokenized_datasets["train"],
collate_fn=data_collator,
shuffle=True,
batch_size=8,
)
tf_eval_dataset = model.prepare_tf_dataset(
tokenized_datasets["validation"],
collate_fn=data_collator,
shuffle=False,
batch_size=8,
)
```

الآن، نقوم بتعريف فرط معلمات التدريب الخاصة بنا و`compile()`:

```python
from transformers import create_optimizer
import tensorflow as tf

# عدد خطوات التدريب هو عدد العينات في مجموعة البيانات، مقسومًا على حجم الدفعة ثم مضروبًا
# بعدد الحقبات الإجمالية. لاحظ أن tf_train_dataset هنا عبارة عن مجموعة بيانات TensorFlow ذات
## ضبط نموذج mT5 باستخدام Accelerate من 🤗

إن ضبط نموذجنا باستخدام Accelerate من 🤗 يشبه إلى حد كبير مثال تصنيف النص الذي واجهناه في [الفصل 3](/course/chapter3). الفرق الرئيسي سيكون الحاجة إلى إنشاء ملخصاتنا صراحة أثناء التدريب وتحديد كيفية حساب درجات ROUGE (تذكر أن `Seq2SeqTrainer` تولى عملية الإنشاء نيابة عنا). دعونا نلقي نظرة على كيفية تنفيذ هذين المطلبين ضمن Accelerate من 🤗!

### إعداد كل شيء للتدريب

أول شيء نحتاج إلى فعله هو إنشاء `DataLoader` لكل من مجموعات البيانات الخاصة بنا. نظرًا لأن برامج PyTorch DataLoaders تتوقع دفعات من المنسوجات، فنحن بحاجة إلى تعيين التنسيق إلى `"torch"` في مجموعات البيانات الخاصة بنا:

```python
tokenized_datasets.set_format("torch")
```

الآن بعد أن حصلنا على مجموعات بيانات تتكون فقط من المنسوجات، فإن الشيء التالي هو إنشاء مثيل لـ `DataCollatorForSeq2Seq` مرة أخرى. لهذا، نحتاج إلى توفير نسخة جديدة من النموذج، لذا دعونا نحمله مرة أخرى من ذاكرة التخزين المؤقت الخاصة بنا:

```python
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

بعد ذلك، يمكننا إنشاء مثيل لمجمع البيانات واستخدامه لتحديد برامجنا DataLoaders:

```python
from torch.utils.data import DataLoader

batch_size = 8
train_dataloader = DataLoader(
tokenized_datasets["train"],
shuffle=True,
collate_fn=data_collator,
batch_size=batch_size,
)
eval_dataloader = DataLoader(
tokenized_datasets["validation"], collate_fn=data_collator, batch_
size=batch_size
)
```

الشيء التالي الذي يجب فعله هو تحديد المحسن الذي نريد استخدامه. كما هو الحال في الأمثلة الأخرى، سنستخدم `AdamW`، والذي يعمل بشكل جيد لمعظم المشكلات:

```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

أخيرًا، نقوم بتغذية نموذجنا ومحسننا وبرامجنا DataLoaders إلى طريقة `accelerator.prepare()` :

```python
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

🚨 إذا كنت تتدرب على وحدة معالجة الرسومات (TPU)، فستحتاج إلى نقل كل التعليمات البرمجية أعلاه إلى دالة تدريب مخصصة. راجع [الفصل 3](/course/chapter3) لمزيد من التفاصيل.

</Tip>

الآن بعد أن قمنا بإعداد كائناتنا، هناك ثلاثة أشياء متبقية يجب فعلها:

* تحديد جدول معدل التعلم.
* تنفيذ دالة لمعالجة الملخصات للتقييم.
* إنشاء مستودع على Hub يمكننا دفع نموذجنا إليه.

بالنسبة لجدول معدل التعلم، سنستخدم الجدول الخطي القياسي من الأقسام السابقة:

```python
from transformers import get_scheduler

num_train_epochs = 10
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
"linear",
optimizer=optimizer,
num_warmup_steps=0,
num_training_steps=num_training_steps,
)
```

بالنسبة للمعالجة المسبقة، نحتاج إلى دالة تقوم بتقسيم الملخصات المولدة إلى جمل مفصولة بأسطر جديدة. هذا هو التنسيق الذي يتوقعه مقياس ROUGE، ويمكننا تحقيق ذلك باستخدام الجزء التالي من التعليمات البرمجية:

```python
def postprocess_text(preds, labels):
preds = [pred.strip() for pred in preds]
labels = [label.strip() for label in labels]

# ROUGE expects a newline after each sentence
preds = ["\n".join(nltk.sent_tokenize(pred)) for pred in preds]
labels = ["\n".join(nltk.sent_tokenize(label)) for label in labels]

return preds, labels
```

يجب أن يبدو هذا مألوفًا بالنسبة لك إذا كنت تتذكر كيفية تحديد دالة `compute_metrics()` لـ `Seq2SeqTrainer`.

أخيرًا، نحتاج إلى إنشاء مستودع نموذج على Hub من Hugging Face. لهذا، يمكننا استخدام مكتبة Hub من 🤗 المناسبة. كل ما نحتاج إلى فعله هو تحديد اسم لمستودعنا، ولدى المكتبة دالة مساعدة لدمج معرف المستودع مع ملف تعريف المستخدم:

```python
from huggingface_hub import get_full_repo_name

model_name = "test-bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'lewtun/mt5-finetuned-amazon-en-es-accelerate'
```

الآن يمكننا استخدام اسم المستودع هذا لاستنساخ نسخة محلية إلى دليل النتائج الخاص بنا والذي سيخزن نتائج التدريب:

```python
from huggingface_hub import Repository

output_dir = "results-mt5-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

سيسمح لنا هذا بإعادة دفع القطع الأثرية إلى Hub عن طريق استدعاء طريقة `repo.push_to_hub()` أثناء التدريب! دعونا الآن ننهي تحليلنا من خلال كتابة حلقة التدريب.

### حلقة التدريب

إن حلقة التدريب للتلخيص مشابهة جدًا لأمثلة Accelerate الأخرى من 🤗 التي واجهناها وتنقسم تقريبًا إلى أربع خطوات رئيسية:

1. تدريب النموذج عن طريق التكرار خلال جميع الأمثلة في `train_dataloader` لكل حقبة.
2. قم بتوليد ملخصات النموذج في نهاية كل حقبة، عن طريق إنشاء الرموز أولاً ثم فك ترميزها (والملخصات المرجعية) إلى نص.
3. احسب درجات ROUGE باستخدام نفس التقنيات التي رأيناها سابقًا.
4. احفظ نقاط التفتيش وادفع كل شيء إلى Hub. نعتمد هنا على الحجة المفيدة `blocking=False` لكائن `Repository` حتى نتمكن من دفع نقاط التفتيش لكل حقبة _بشكل غير متزامن_. يسمح لنا هذا بالاستمرار في التدريب دون الاضطرار إلى الانتظار للتحميل البطيء إلى حد ما المرتبط بنموذج بحجم جيجابايت!

يمكن رؤية هذه الخطوات في كتلة التعليمات البرمجية التالية:

```python
from tqdm.auto import tqdm
import torch
import numpy as np

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
# Training
model.train()
for step, batch in enumerate(train_dataloader):
outputs = model(**batch)
loss = outputs.loss
accelerator.backward(loss)

optimizer.step()
lr_scheduler.step()
optimizer.zero_grad()
progress_bar.update(1)

# Evaluation
model.eval()
for step, batch in enumerate(eval_dataloader):
with torch.no_grad():
generated_tokens = accelerator.unwrap_model(model).generate(
batch["input_ids"],
attention_mask=batch["attention_mask"],
)

generated_tokens = accelerator.pad_across_processes(
generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
)
labels = batch["labels"]

# If we did not pad to max length, we need to pad the labels too
labels = accelerator.pad_across_processes(
batch["labels"], dim=1, pad_index=tokenizer.pad_token_id
)

generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()
labels = accelerator.gather(labels).cpu().numpy()

# Replace -100 in the labels as we can't decode them
labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
if isinstance(generated_tokens, tuple):
generated_tokens = generated_tokens[0]
decoded_preds = tokenizer.batch_decode(
generated_tokens, skip_special_tokens=True
)
decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

decoded_preds, decoded_labels = postprocess_text(
decoded_preds, decoded_labels
)

rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)

# Compute metrics
result = rouge_score.compute()
# Extract the median ROUGE scores
result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
result = {k: round(v, 4) for k, v in result.items()}
print(f"Epoch {epoch}:", result)

# Save and upload
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
if accelerator.is_main_process:
tokenizer.save_pretrained(output_dir)
repo.push_to_hub(
commit_message=f"Training in progress epoch {epoch}", blocking=False
)
```

```python out
Epoch 0: {'rouge1': 5.6351, 'rouge2': 1.1625, 'rougeL': 5.4866, 'rougeLsum': 5.5005}
Epoch 1: {'rouge1': 9.8646, 'rouge2': 3.4106, 'rougeL': 9.9439, 'rougeLsum': 9.9306}
Epoch 2: {'rouge1': 11.0872, 'rouge2': 3.3273, 'rougeL': 11.0508, 'rougeLsum': 10.9468}
Epoch 3: {'rouge1': 11.8587, 'rouge2': 4.8167, 'rougeL': 11.7986, 'rougeLsum': 11.7518}
Epoch 4: {'rouge1': 12.9842, 'rouge2': 5.5887, 'rougeL': 12.7546, 'rougeLsum': 12.7029}
Epoch 5: {'rouge1': 13.4628, 'rouge2': 6.4598, 'rougeL': 13.312, 'rougeLsum': 13.2913}
Epoch 6: {'rouge1': 12.9131, 'rouge2': 5.8914, 'rougeL': 12.6896, 'rougeLsum': 12.5701}
Epoch 7: {'rouge1': 13.3079, 'rouge2': 6.2994, 'rougeL': 13.1536, 'rougeLsum': 13.1194}
Epoch 8: {'rouge1': 13.96, 'rouge2': 6.5998, 'rougeL': 13.9123, 'rougeLsum': 13.7744}
Epoch 9: {'rouge1': 14.1192, 'rouge2': 7.0059, 'rougeL': 14.1172, 'rougeLsum': 13.9509}
```

وهذا كل شيء! بمجرد تشغيل هذا، سيكون لديك نموذج ونتائج مشابهة جدًا لتلك التي حصلنا عليها مع `Trainer`.

{/if}

## استخدام نموذجك المضبوط

بمجرد دفع النموذج إلى Hub، يمكنك اللعب به إما عبر أداة الاستدلال التفاعلية أو باستخدام كائن `pipeline`، كما يلي:

```python
from transformers import pipeline

hub_model_id = "huggingface-course/mt5-small-finetuned-amazon-en-es"
summarizer = pipeline("summarization", model=hub_model_id)
```

يمكننا إطعام بعض الأمثلة من مجموعة الاختبار (التي لم يرها النموذج) إلى خط أنابيبنا للحصول على فكرة عن جودة الملخصات. أولاً، دعونا ننفذ دالة بسيطة لعرض المراجعة والعنوان والملخص المولد معًا:

```python
def print_summary(idx):
review = books_dataset["test"][idx]["review_body"]
title = books_dataset["test"][idx]["review_title"]
summary = summarizer(books_dataset["test"][idx]["review_body"])[0]["summary_text"]
print(f"'>>> Review: {review}'")
print(f"\n'>>> Title: {title}'")
print(f"\n'>>> Summary: {summary}'")
```

دعونا نلقي نظرة على أحد الأمثلة الإنجليزية التي نحصل عليها:

```python
print_summary(100)
```

```python out
'>>> Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesn’t come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. It’s also really expensive for what it is.'

'>>> Title: Not impressed at all... buy something else'

'>>> Summary: Nothing special at all about this product'
```

هذا ليس سيئًا! يمكننا أن نرى أن نموذجنا تمكن بالفعل من أداء تلخيص _استخلاصي_ عن طريق إضافة كلمات جديدة إلى أجزاء من المراجعة. وربما الجانب الأكثر روعة في نموذجنا هو أنه ثنائي اللغة، لذا يمكننا أيضًا توليد ملخصات لمراجعات باللغة الإسبانية:

```python
print_summary(0)
```

```python out
'>>> Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada'

'>>> Title: Buena literatura para adolescentes'

'>>> Summary: Muy facil de leer'
```

تترجم الملخصات إلى "من السهل جدًا القراءة" باللغة الإنجليزية، والتي يمكننا أن نرى في هذه الحالة تم استخراجها مباشرة من المراجعة. ومع ذلك، فإن هذا يظهر مرونة نموذج mT5 وقد أعطاك فكرة عن التعامل مع مجموعة بيانات متعددة اللغات!

بعد ذلك، سنوجه اهتمامنا إلى مهمة أكثر تعقيدًا قليلاً: تدريب نموذج اللغة من الصفر.