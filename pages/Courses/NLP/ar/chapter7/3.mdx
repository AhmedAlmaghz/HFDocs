## ุถุจุท ุฏููู ููููุฐุฌ ุงููุบุฉ ุงููููุน

ููุชุทุจููุงุช ุงููุบููุฉ ุงูุนุตุจูุฉ ุงูุนุฏูุฏุฉ ุงูุชู ุชุชุถูู ููุงุฐุฌ ุงููุญููุ ููููู ุจุจุณุงุทุฉ ุงุณุชุฎุฏุงู ูููุฐุฌ ูุฏุฑุจ ูุณุจููุง ูู ููุตุฉ ูุฌูู ููุณ ูุงูุถุจุท ุงูุฏููู ูู ูุจุงุดุฑุฉ ุนูู ุจูุงูุงุชู ุงูุฎุงุตุฉ ุจุงููููุฉ ุงูุชู ุจูู ูุฏูู. ุจุดุฑุท ุฃูุง ูุฎุชูู ุงููููู ุงููุณุชุฎุฏู ููุชุนููู ุงููุณุจู ูุซูุฑูุง ุนู ุงููููู ุงููุณุชุฎุฏู ููุถุจุท ุงูุฏูููุ ูุฅู ุงูุชุนูู ุงูุชุญูููู ุณููุชุฌ ุนุงุฏุฉ ูุชุงุฆุฌ ุฌูุฏุฉ.

ููุน ุฐููุ ููุงู ุจุนุถ ุงูุญุงูุงุช ุงูุชู ุชุฑูุฏ ูููุง ุฃููุงู ุถุจุท ูููุฐุฌ ุงููุบุฉ ุงูุฏููู ูุจูุงูุงุชูุ ูุจู ุชุฏุฑูุจ ุฑุฃุณ ุฎุงุต ุจูููุฉ ูุนููุฉ. ุนูู ุณุจูู ุงููุซุงูุ ุฅุฐุง ูุงูุช ูุฌููุนุฉ ุจูุงูุงุชู ุชุญุชูู ุนูู ุนููุฏ ูุงููููุฉ ุฃู ููุงูุงุช ุนูููุฉุ ูุนุงุฏุฉู ูุง ุชุนุงูู ูููุฐุฌ ุงููุญูู ุงูุฃุณุงุณู ูุซู BERT ุงููููุงุช ุงูุฎุงุตุฉ ุจุงููุฌุงู ูู ููููู ุนูู ุฃููุง ุฑููุฒ ูุงุฏุฑุฉุ ููุฏ ูููู ุงูุฃุฏุงุก ุงููุงุชุฌ ุฃูู ูู ุงููุซุงูู. ูู ุฎูุงู ุงูุถุจุท ุงูุฏููู ููููุฐุฌ ุงููุบุฉ ุนูู ุงูุจูุงูุงุช ุฏุงุฎู ุงููุฌุงูุ ููููู ุชุนุฒูุฒ ุฃุฏุงุก ุงูุนุฏูุฏ ูู ุงูููุงู ุงููุงุญูุฉุ ููุง ูุนูู ุฃูู ุนุงุฏุฉ ูุง ุชุญุชุงุฌ ููุท ุฅูู ุชูููุฐ ูุฐู ุงูุฎุทูุฉ ูุฑุฉ ูุงุญุฏุฉ!

ุชูุนุฑู ุนูููุฉ ุงูุถุจุท ุงูุฏููู ููููุฐุฌ ุงููุบุฉ ุงููุฏุฑุจ ูุณุจููุง ุนูู ุงูุจูุงูุงุช ุฏุงุฎู ุงููุฌุงู ุนุงุฏุฉู ุจุงุณู _ุชูููู ุงููุฌุงู_. ููุฏ ุงุดุชูุฑุช ูู ุนุงู 2018 ูู ุฎูุงู [ULMFiT](https://arxiv.org/abs/1801.06146)ุ ูุงูุชู ูุงูุช ูุงุญุฏุฉ ูู ุฃููู ุงูุจูู ุงูุนุตุจูุฉ (ุจูุงุกู ุนูู LSTMs) ูุฌุนู ุงูุชุนูู ุงูุชุญูููู ูุนูู ุญููุง ูู NLP. ูุชู ุฅุธูุงุฑ ูุซุงู ุนูู ุชูููู ุงููุฌุงู ูุน ULMFiT ูู ุงูุตูุฑุฉ ุฃุฏูุงูุ ูู ูุฐุง ุงููุณูุ ุณูููู ุจุดูุก ูุดุงุจูุ ูููู ุจุงุณุชุฎุฏุงู ูุญูู ุจุฏูุงู ูู LSTM!

ุจุญููู ููุงูุฉ ูุฐุง ุงููุณูุ ุณูููู ูุฏูู [ูููุฐุฌ ูุบุฉ ูููุน](https://huggingface.co/huggingface-course/distilbert-base-uncased-finetuned-imdb?text=This+is+a+great+%5BMASK%5D.) ุนูู ุงููุญุงูุฑ ููููู ุฅููุงู ุงูุฌูู ููุง ูู ููุถุญ ุฃุฏูุงู:

ุฏุนููุง ูุบูุต ูู ุฐูู!

๐ ุฅุฐุง ูุงูุช ูุตุทูุญุงุช "ููุฐุฌุฉ ุงููุบุฉ ุงููููุนุฉ" ู "ุงููููุฐุฌ ุงููุฏุฑุจ ูุณุจููุง" ุชุจุฏู ุบูุฑ ูุฃูููุฉ ุจุงููุณุจุฉ ููุ ูุงูุชูู ุฅูู [ุงููุตู 1](/course/chapter1)ุ ุญูุซ ูุดุฑุญ ูู ูุฐู ุงูููุงููู ุงูุฃุณุงุณูุฉุ ูุน ููุงุทุน ุงูููุฏูู!

## ุงุฎุชูุงุฑ ูููุฐุฌ ูุฏุฑุจ ูุณุจููุง ูููุฐุฌุฉ ุงููุบุฉ ุงููููุนุฉ

ููุจุฏุกุ ุฏุนูุง ูุฎุชุงุฑ ูููุฐุฌูุง ูุฏุฑุจูุง ูุณุจููุง ููุงุณุจูุง ูููุฐุฌุฉ ุงููุบุฉ ุงููููุนุฉ. ููุง ูู ููุถุญ ูู ููุทุฉ ุงูุดุงุดุฉ ุฃุฏูุงูุ ููููู ุงูุนุซูุฑ ุนูู ูุงุฆูุฉ ุจุงููุฑุดุญูู ูู ุฎูุงู ุชุทุจูู ูุฑุดุญ "Fill-Mask" ุนูู [ููุตุฉ ูุฌูู ููุณ](https://huggingface.co/modelsุpipeline_tag=fill-mask&sort=downloads):

ุนูู ุงูุฑุบู ูู ุฃู ุนุงุฆูุฉ BERT ู RoBERTa ูู ุงูููุงุฐุฌ ูู ุงูุฃูุซุฑ ุชูุฒูููุงุ ุฅูุง ุฃููุง ุณูุณุชุฎุฏู ูููุฐุฌูุง ูุณูู [DistilBERT](https://huggingface.co/distilbert-base-uncased)
ุงูุฐู ูููู ุชุฏุฑูุจู ุจุดูู ุฃุณุฑุน ุจูุซูุฑ ูุน ุงููููู ูู ููุฏุงู ุงูุฃุฏุงุก ูู ุงููููุฉ ุงููุงุญูุฉ ุฃู ุจุฏูู ููุฏุงู. ุชู ุชุฏุฑูุจ ูุฐุง ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู ุชูููุฉ ุฎุงุตุฉ ุชุณูู [_ุงูุชูุทูุฑ ุงููุนุฑูู_](https://en.wikipedia.org/wiki/Knowledge_distillation)ุ ุญูุซ ูุชู ุงุณุชุฎุฏุงู "ูููุฐุฌ ุงููุนูู" ุงููุจูุฑ ูุซู BERT ูุชูุฌูู ุชุฏุฑูุจ "ูููุฐุฌ ุงูุทุงูุจ" ุงูุฐู ูุญุชูู ุนูู ุนุฏุฏ ุฃูู ุจูุซูุฑ ูู ุงููุนููุงุช. ูู ุดุฃู ุดุฑุญ ุชูุงุตูู ุงูุชูุทูุฑ ุงููุนุฑูู ุฃู ูุฃุฎุฐูุง ุจุนูุฏูุง ุฌุฏูุง ูู ูุฐุง ุงููุณูุ ูููู ุฅุฐุง ููุช ููุชููุงุ ูููููู ูุฑุงุกุฉ ูู ุดูุก ุนูู ูู [_ูุนุงูุฌุฉ ุงููุบุงุช ุงูุทุจูุนูุฉ ุจุงุณุชุฎุฏุงู ุงููุญููุงุช_](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/) (ุงููุนุฑูู ุจุงุณู ูุชุงุจ ุงููุญููุงุช ุงููุฏุฑุณู).

ุฏุนููุง ูููู ุจุชูุฒูู DistilBERT ุจุงุณุชุฎุฏุงู ูุฆุฉ `AutoModelForMaskedLM`:

```python
from transformers import AutoModelForMaskedLM

model_checkpoint = "distilbert-base-uncased"
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)
```

ูููููุง ูุนุฑูุฉ ุนุฏุฏ ุงููุนููุงุช ุงูุชู ูุญุชูู ุนูููุง ูุฐุง ุงููููุฐุฌ ุนู ุทุฑูู ุงุณุชุฏุนุงุก ุทุฑููุฉ `num_parameters()`:

```python
distilbert_num_parameters = model.num_parameters() / 1_000_000
print(f"'>>> ุนุฏุฏ ูุนููุงุช DistilBERT: {round(distilbert_num_parameters)}M'")
print(f"'>>> ุนุฏุฏ ูุนููุงุช BERT: 110M'")
```

```python out
'>>> ุนุฏุฏ ูุนููุงุช DistilBERT: 67M'
'>>> ุนุฏุฏ ูุนููุงุช BERT: 110M'
```

ูุฏู DistilBERT ุญูุงูู 67 ููููู ูุนููุฉุ ุฃู ูุง ููุฑุจ ูู ุถุนู ุญุฌู ูููุฐุฌ BERT ุงูุฃุณุงุณูุ ูุงูุฐู ูุชุฑุฌู ุชูุฑูุจูุง ุฅูู ุชุณุฑูุน ุซูุงุฆู ูู ุงูุชุฏุฑูุจ - ุฑุงุฆุน! ุฏุนููุง ุงูุขู ูุฑู ุฃููุงุน ุงูุฑููุฒ ุงูุชู ูุชููุนูุง ูุฐุง ุงููููุฐุฌ ูู ุงูุฃูุซุฑ ุงูุชูุงููุง ูุนููุฉ ูุตูุฉ ุตุบูุฑุฉ:

```python
text = "This is a great [MASK]."
```

ุจุงุนุชุจุงุฑูุง ุจุดุฑูุงุ ูููููุง ุชุฎูู ุงูุนุฏูุฏ ูู ุงูุงุญุชูุงูุงุช ูุฑููุฒ `[MASK]`ุ ูุซู "ููู" ุฃู "ุฑููุจ" ุฃู "ููุญุฉ". ุจุงููุณุจุฉ ููููุงุฐุฌ ุงููุฏุฑุจุฉ ูุณุจููุงุ ุชุนุชูุฏ ุงูุชููุนุงุช ุนูู ุงููููู ุงูุฐู ุชู ุชุฏุฑูุจ ุงููููุฐุฌ ุนูููุ ุญูุซ ูุชุนูู ุงูุชูุงุท ุงูุฃููุงุท ุงูุฅุญุตุงุฆูุฉ ุงูููุฌูุฏุฉ ูู ุงูุจูุงูุงุช. ูุซู BERTุ ุชู ุชุฏุฑูุจ DistilBERT ุนูู ูุฌููุนุงุช ุจูุงูุงุช [English Wikipedia](https://huggingface.co/datasets/wikipedia) ู [BookCorpus](https://huggingface.co/datasets/bookcorpus)ุ ูุฐูู ูุชููุน ุฃู ุชุนูุณ ุงูุชููุนุงุช ูู `[MASK]` ูุฐู ุงููุฌุงูุงุช. ููุชูุจุค ุจุงูููุงุนุ ูุญุชุงุฌ ุฅูู ูุญูู ุฑููุฒ DistilBERT ูุฅูุชุงุฌ ุงููุฏุฎูุงุช ูููููุฐุฌุ ูุฐุง ุฏุนูุง ูููู ุจุชูุฒูู ุฐูู ูู ุงููุญุงูุฑ ุฃูุถูุง:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

ูุน ูุญูู ุงูุฑููุฒ ูุงููููุฐุฌุ ูููููุง ุงูุขู ุชูุฑูุฑ ูุซุงู ุงููุต ุงูุฎุงุต ุจูุง ุฅูู ุงููููุฐุฌุ ูุงุณุชุฎุฑุงุฌ logitsุ ูุทุจุงุนุฉ ุฃูุถู 5 ูุฑุดุญูู:

```python
import torch

inputs = tokenizer(text, return_tensors="pt")
token_logits = model(**inputs).logits
# Find the location of [MASK] and extract its logits
mask_token_index = torch.where(inputs["input_ids"] == tokenizer.mask_token_id)[1]
mask_token_logits = token_logits[0, mask_token_index, :]
# Pick the [MASK] candidates with the highest logits
top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()

for token in top_5_tokens:
print(f"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'")
```

```python out
'>>> This is a great deal.'
'>>> This is a great success.'
'>>> This is a great adventure.'
'>>> This is a great idea.'
'>>> This is a great feat.'
```

ูููููุง ุฃู ูุฑู ูู ุงููุฎุฑุฌุงุช ุฃู ุชูุจุคุงุช ุงููููุฐุฌ ุชุดูุฑ ุฅูู ุงููุตุทูุญุงุช ุงูููููุฉุ ุงูุฃูุฑ ุงูุฐู ูุฏ ูุง ูููู ููุงุฌุฆูุง ุจุงููุธุฑ ุฅูู ุฃุณุงุณ ููููุจูุฏูุง ุงูุฅูุฌููุฒูุฉ. ุฏุนููุง ูุฑู ููู ูููููุง ุชุบููุฑ ูุฐุง ุงููุฌุงู ุฅูู ุดูุก ุฃูุซุฑ ุชุฎุตุตูุง - ูุฑุงุฌุนุงุช ุงูุฃููุงู ุงููุทุจูุฉ ููุบุงูุฉ!
## ูุฌููุนุฉ ุงูุจูุงูุงุช

ูุฅุธูุงุฑ ุชููู ุงููุฌุงูุ ุณูุณุชุฎุฏู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุดููุฑุฉ [ูุฌููุนุฉ ูุฑุงุฌุนุงุช ุงูุฃููุงู ุงููุจูุฑุฉ](https://huggingface.co/datasets/imdb) (ุฃู IMDb ุงุฎุชุตุงุฑูุง)ุ ูุงูุชู ูู ุนุจุงุฑุฉ ุนู ูุฌููุนุฉ ูู ูุฑุงุฌุนุงุช ุงูุฃููุงู ุงูุชู ุชุณุชุฎุฏู ุบุงูุจูุง ูููุงุณ ููุงุฐุฌ ุชุญููู ุงููุดุงุนุฑ. ูู ุฎูุงู ุถุจุท ูููุฐุฌ DistilBERT ุงูุฏููู ุนูู ูุฐุง ุงููููุ ูุชููุน ุฃู ูููู ูููุฐุฌ ุงููุบุฉ ุจุชูููู ููุฑุฏุงุชู ูู ุงูุจูุงูุงุช ุงููุงูุนูุฉ ูู Wikipedia ุงูุชู ุชู ุชุฏุฑูุจู ูุณุจููุง ุนูููุง ุฅูู ุงูุนูุงุตุฑ ุงูุฃูุซุฑ ุฐุงุชูุฉ ูู ูุฑุงุฌุนุงุช ุงูุฃููุงู. ูููููุง ุงูุญุตูู ุนูู ุงูุจูุงูุงุช ูู Hub Hugging Face ุจุงุณุชุฎุฏุงู ุงูุฏุงูุฉ `load_dataset()` ูู Datasets ๐ค:

```python
from datasets import load_dataset

imdb_dataset = load_dataset("imdb")
imdb_dataset
```

```python out
DatasetDict({
train: Dataset({
features: ['text', 'label'],
num_rows: 25000
})
test: Dataset({
features: ['text', 'label'],
num_rows: 25000
})
unsupervised: Dataset({
features: ['text', 'label'],
num_rows: 50000
})
})
```

ูููููุง ุฃู ูุฑู ุฃู ุงูุงููุณุงูุงุช `train` ู`test` ุชุชููู ูู ูููุง ูู 25000 ูุฑุงุฌุนุฉุ ูู ุญูู ุฃู ููุงู ูุณู ุบูุฑ ูุนููู ูุณูู `unsupervised` ูุญุชูู ุนูู 50000 ูุฑุงุฌุนุฉ. ุฏุนููุง ูููู ูุธุฑุฉ ุนูู ุจุนุถ ุงูุนููุงุช ููุญุตูู ุนูู ููุฑุฉ ุนู ููุน ุงููุต ุงูุฐู ูุชุนุงูู ูุนู. ููุง ูุนููุง ูู ุงููุตูู ุงูุณุงุจูุฉ ูู ุงูุฏูุฑุฉุ ุณูููู ุจุชุณูุณู ูุธุงุฆู `Dataset.shuffle()` ู`Dataset.select()` ูุฅูุดุงุก ุนููุฉ ุนุดูุงุฆูุฉ:

```python
sample = imdb_dataset["train"].shuffle(seed=42).select(range(3))

for row in sample:
print(f"\n'>>> Review: {row['text']}'")
print(f"'>>> Label: {row['label']}'")
```

```python out
'>>> Review: ูุฐุง ูู ูููู Priyadarshan ุงููููุฐุฌู - ูุฌููุนุฉ ูู ุงูุดุฎุตูุงุช ุงููุฌูููุฉ ุงูุชู ุชููู ุจูููุฉ ุณุฎููุฉ. ูุชุถูู ุชูููุนู ูู ุงูุฐุฑูุฉ ุธููุฑ ุฌููุน ุฃูุฑุงุฏ ูุฑูู ุงูุชูุซูู ูู ุงููููู ูุงููุชุงู ูุน ุจุนุถูู ุงูุจุนุถ ูู ุจุนุถ ุงููุนุงุฑู ุงููุฌูููุฉ ุญูู ุงููุงู ุงููุฎุจุฃ. ุณูุงุก ูุงูุช ุชุฐูุฑุฉ ูุงูุตูุจ ุฑุงุจุญุฉ ูู Malamaal Weeklyุ ุฃู ุฃููุงู ุณูุฏุงุก ูู Hera Pheriุ ุฃู "kodokoo" ูู Phir Hera Pheriุ ููุง ุฅูู ุฐููุ ุฃุตุจุญ ุงููุฎุฑุฌ ูุชููุนูุง ุจุดูู ุณุฎูู. ูุง ุชูููููู ุฎุทุฃูุ ุจุบุถ ุงููุธุฑ ุนู ูุฏู ุงุจุชุฐุงู ุฃููุงูู ูุฌููููุงุ ุนุงุฏุฉ ูุง ุฃุณุชูุชุน ุจุงูููููุฏูุง. ููุน ุฐููุ ูู ูุนุธู ุฃููุงูู ุงูุณุงุจูุฉุ ูุงู ููุงู ูู ุงููุงูุน ุจุนุถ ุงูููุงูุฉ ุงูุฌูุฏุฉุ (Hungama ู Hera Pheri ูู ุฃููุงู ุฌุฏูุฑุฉ ุจุงูููุงุญุธุฉ). ุงูุขูุ ุชุชูุงุดู ุทุฑุงูุฉ ุฃููุงูู ูุฃูู ูุณุชุฎุฏู ููุณ ุงูุตูุบุฉ ูุฑุงุฑูุง ูุชูุฑุงุฑูุง. <br /><br /> ุงูุฃุบุงูู ุฌูุฏุฉ. ุชุจุฏู Tanushree Datta ุฑุงุฆุนุฉ. Rajpal Yadav ูุฒุนุฌุ ูTusshar ููุณ ุฃูุถู ุจูุซูุฑ. ูููู ูููู ุนูู ูุง ูุฑุงูุ ูุดุงุฑูุงู ุฌูุดู ูู ุงูุฃูุถู.'
'>>> Label: 0'

'>>> Review: ุญุณููุงุ ุงููุตุฉ ูุง ูุนูู ููุงุ ูุงูุดุฎุตูุงุช ุชูุชูุฑ ุฅูู ุงูุจุนุฏุ ูุฃูุถู ุญูุงุฑ ูู ุงุฑุชุฌุงู ุญูู ุฌูุฏุฉ ุงููููู ุงูููุฎูุถุฉุ ูุงูุชุตููุฑ ุงูุณูููุงุฆู ูุฆูุจุ ูุงูุชุญุฑูุฑ ูุญุฏู ูููุฐ ุงูููุถู ุจุนุถ ุงูุดูุกุ ููู ุณุงู ุจููููุจุงู ุฃุฎุฑุฌ ุงููููู. ุจุทุฑููุฉ ูุงุ ูู ููู ุฅุฎุฑุงุฌู ูุงูููุง. ุจุงููุณุจุฉ ูุฃููุฆู ุงูุฐูู ููุฏุฑูู ุจููููุจุงู ูุนููู ุงูุนุธููุ ูุฐุง ุงููููู ูุฎูุจ ููุขูุงู. ุญุชู ุทุงูู ุงูุนูู ุงูุฑุงุฆุน ูุง ูุณุชุทูุน ุฅููุงุฐ ุงูููุช ุงูุฐู ูุถูุนู ุงููุดุงูุฏ ูู ูุฐุง ุงูุฌูุฏ ุงูุถุฆูู. <br /><br /> ุงูุงุณุชุฌุงุจุฉ ุงูููุงุณุจุฉ ูููููู ูู ุงูุงุญุชูุงุฑ ุงูุฐู ุฌูุจู ุงููุฎุฑุฌ ุณุงู ุจููููุจุงูุ ูุฌููุณ ูุงูุ ูุฑูุจุฑุช ุฏููุงูุ ูุจูุฑุช ูููุบุ ูุจู ููุจููุฒุ ูุขุฑุซุฑ ูููุ ูุญุชู ุฌูุฌ ูููุบ ุฅูู ุนูููู. ุดุงูุฏ ุฃููุงู ุจููููุจุงู ุงูุฑุงุฆุนุฉ. ุชุฎุทู ูุฐู ุงูููุถู.'
'>>> Label: 0'

'>>> Review: ุดุงูุฏุช ูุฐุง ุงููููู ูู ุงูุณูููุง ุนูุฏูุง ููุช ูู ุงูุณุงุฏุณุฉ ุฃู ุงูุณุงุจุนุฉ ูู ุนูุฑู. ุฃุญุจุจุชู ุขูุฐุงูุ ููุฏ ุญุตูุช ูุคุฎุฑูุง ุนูู ูุณุฎุฉ VHS. <br /><br /> ูุญุจ ุฃุทูุงูู ุงูุฐูู ุชุชุฑุงูุญ ุฃุนูุงุฑูู ุจูู 4 ู6 ุณููุงุช ูุฐุง ุงููููู ููุฏ ุธููุง ูุณุฃููู ุนู ูุดุงูุฏุชู ูุฑุงุฑูุง ูุชูุฑุงุฑูุง. <br /><br /> ููุฏ ุงุณุชูุชุนุช ุจูุดุงูุฏุชู ูุฑุฉ ุฃุฎุฑู ุฃูุถูุง. ุนูู ุงูุฑุบู ูู ุฃูู ูุฌุจ ุฃู ุฃุนุชุฑู ุฃูู ููุณ ุฌูุฏูุง ุนูู ุดุงุดุฉ ุงูุชููุฒููู ุงูุตุบูุฑุฉ. <br /><br /> ูุง ุฃููู ุฃุทูุงููุง ุฃูุจุฑุ ูุฐูู ูุง ุฃุนุฑู ูุง ุงูุฐู ุณูููุฑูู ููู. <br /><br /> ุงูุฃุบุงูู ูุทููุฉ ููุบุงูุฉ. ุงุจูุชู ุชุธู ุชุบูููุง ูุฑุงุฑูุง ูุชูุฑุงุฑูุง. <br /><br /> ุขูู ุฃู ูููู ูุฐุง ูููุฏูุง.'
'>>> Label: 1'
```

ูุนูุ ูุฐู ูู ุจุงูุชุฃููุฏ ูุฑุงุฌุนุงุช ุงูุฃููุงูุ ูุฅุฐุง ููุช ูุจูุฑูุง ุจูุง ููููุ ููุฏ ุชููู ุงูุชุนููู ูู ุงููุฑุงุฌุนุฉ ุงูุฃุฎูุฑุฉ ุญูู ุงูุชูุงู ูุณุฎุฉ VHS ๐! ุนูู ุงูุฑุบู ูู ุฃููุง ูู ูุญุชุงุฌ ุฅูู ุงูุชุตูููุงุช ูู ุฃุฌู ููุฐุฌุฉ ุงููุบุฉุ ุฅูุง ุฃููุง ูุฑู ุจุงููุนู ุฃู "0" ุชุดูุฑ ุฅูู ูุฑุงุฌุนุฉ ุณูุจูุฉุ ูู ุญูู ุฃู "1" ุชุชูุงูู ูุน ูุฑุงุฌุนุฉ ุฅูุฌุงุจูุฉ.

<Tip>

โ๏ธ **ุฌุฑุจู!** ูู ุจุฅูุดุงุก ุนููุฉ ุนุดูุงุฆูุฉ ูู ุงูุงููุณุงู `unsupervised` ูุชุญูู ูู ุฃู ุงูุชุตูููุงุช ููุณุช `0` ููุง `1`. ุฃุซูุงุก ููุงูู ุจุฐููุ ููููู ุฃูุถูุง ุงูุชุญูู ูู ุฃู ุงูุชุตูููุงุช ูู ุงูุงููุณุงูุงุช `train` ู`test` ูู ูู ุงููุงูุน `0` ุฃู `1` - ููุฐุง ูุญุต ุฌูุฏ ูุฌุจ ุนูู ูู ููุงุฑุณ NLP ุงูููุงู ุจู ูู ุจุฏุงูุฉ ุฃู ูุดุฑูุน ุฌุฏูุฏ!

</Tip>

ุงูุขู ุจุนุฏ ุฃู ุฃููููุง ูุธุฑุฉ ุณุฑูุนุฉ ุนูู ุงูุจูุงูุงุชุ ุฏุนููุง ูุบูุต ูู ุฅุนุฏุงุฏูุง ูููุฐุฌุฉ ุงููุบุฉ ุงููููุนุฉ. ููุง ุณูุฑูุ ููุงู ุจุนุถ ุงูุฎุทูุงุช ุงูุฅุถุงููุฉ ุงูุชู ูุฌุจ ุงุชุฎุงุฐูุง ููุงุฑูุฉ ุจููุงู ุชุตููู ุงูุชุณูุณูุงุช ุงูุชู ุฑุฃููุงูุง ูู [ุงููุตู 3](/course/chapter3). ููุง ุจูุง!
## ูุนุงูุฌุฉ ุงูุจูุงูุงุช ูุณุจููุง

ุจุงููุณุจุฉ ููู ูู ุงูููุฐุฌุฉ ุงููุบููุฉ ุงูุชููุงุฆูุฉ ูุงูููุฐุฌุฉ ุงููุบููุฉ ุงููููุนุฉุ ุชุชูุซู ุฎุทูุฉ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ุงูุดุงุฆุนุฉ ูู ุฏูุฌ ุฌููุน ุงูุฃูุซูุฉ ุซู ุชูุณูู ุงููุฌููุนุฉ ุจุฃููููุง ุฅูู ูุทุน ุฐุงุช ุญุฌู ูุชุณุงูู. ูุฎุชูู ูุฐุง ุงุฎุชูุงููุง ูุจูุฑูุง ุนู ุงูููุฌ ุงููุนุชุงุฏุ ุญูุซ ูููู ุจุจุณุงุทุฉ ุจุชูุญูุฏ ุฃูุซูุฉ ูุฑุฏูุฉ. ููุงุฐุง ูููู ุจุฏูุฌ ูู ุดูุก ูุนูุงุ ุงูุณุจุจ ูู ุฃู ุงูุฃูุซูุฉ ุงููุฑุฏูุฉ ูุฏ ูุชู ุงูุชุทุงุนูุง ุฅุฐุง ูุงูุช ุทูููุฉ ุฌุฏูุงุ ููุฏ ูุคุฏู ุฐูู ุฅูู ููุฏุงู ูุนูููุงุช ูุฏ ุชููู ูููุฏุฉ ููููุฉ ููุฐุฌุฉ ุงููุบุฉ!

ูุฐุงุ ููุจุฏุกุ ุณูููู ุฃููุงู ุจุชูุญูุฏ ูุตูุง ูุงููุนุชุงุฏุ ูููู _ุจุฏูู_ ุชุนููู ุงูุฎูุงุฑ `truncation=True` ูู ููุญุฏูุง. ุณูููู ุฃูุถูุง ุจุงูุชูุงุท ูุนุฑููุงุช ุงููููุงุช ุฅุฐุง ูุงูุช ูุชููุฑุฉ (ูุงูุชู ุณุชููู ุฅุฐุง ููุง ูุณุชุฎุฏู ููุญุฏูุง ุณุฑูุนูุงุ ููุง ูู ููุถุญ ูู [ุงููุตู 6](/course/chapter6/3))ุ ุญูุซ ุณูุญุชุงุฌูุง ูุงุญููุง ููููุงู ุจุงูุงููุงุน ุนูู ูุณุชูู ุงููููุฉ ุจุงููุงูู. ุณูููู ุจุชุบููู ูุฐุง ูู ุฏุงูุฉ ุจุณูุทุฉุ ูุจูููุง ููุนู ุฐููุ ุณูููู ุจุฅุฒุงูุฉ ุฃุนูุฏุฉ "ุงููุต" ู"ุงูุชุณููุฉ" ูุฃููุง ูู ูุนุฏ ุจุญุงุฌุฉ ุฅูููุง:

```python
def tokenize_function(examples):
    result = tokenizer(examples["text"])
    if tokenizer.is_fast:
        result["word_ids"] = [result.word_ids(i) for i in range(len(result["input_ids"]))]
    return result

# Use batched=True to activate fast multithreading!
tokenized_datasets = imdb_dataset.map(
    tokenize_function, batched=True, remove_columns=["text", "label"]
)
tokenized_datasets
```

```python out
DatasetDict({
train: Dataset({
features: ['attention_mask', 'input_ids', 'word_ids'],
num_rows: 25000
})
test: Dataset({
features: ['attention_mask', 'input_ids', 'word_ids'],
num_rows: 25000
})
unsupervised: Dataset({
features: ['attention_mask', 'input_ids', 'word_ids'],
num_rows: 50000
})
})
```

ูุธุฑูุง ูุฃู DistilBERT ูู ูููุฐุฌ ูุดุงุจู ูู BERTุ ูููููุง ุฃู ูุฑู ุฃู ุงููุตูุต ุงููุดูุฑุฉ ุชุชููู ูู `input_ids` ู`attention_mask` ุงูุชู ุฑุฃููุงูุง ูู ูุตูู ุฃุฎุฑูุ ุจุงูุฅุถุงูุฉ ุฅูู `word_ids` ุงูุชู ุฃุถููุงูุง.

ุงูุขู ุจุนุฏ ุฃู ูููุง ุจุชูุญูุฏ ูุฑุงุฌุนุงุช ุงูุฃููุงู ุงูุฎุงุตุฉ ุจูุงุ ุชุชูุซู ุงูุฎุทูุฉ ุงูุชุงููุฉ ูู ุชุฌููุนูุง ุฌููุนูุง ูุนูุง ูุชูุณูู ุงููุชูุฌุฉ ุฅูู ูุทุน. ูููู ูุง ูู ุญุฌู ูุฐู ุงููุทุนุ ุณูุชุญุฏุฏ ุฐูู ูู ุงูููุงูุฉ ุจูููุฉ ุฐุงูุฑุฉ GPU ุงููุชููุฑุฉ ูุฏููุ ูููู ููุทุฉ ุงูุจุฏุงูุฉ ุงูุฌูุฏุฉ ูู ูุนุฑูุฉ ูุง ูู ุญุฌู ุงูุณูุงู ุงูุฃูุตู ูููููุฐุฌ. ูููู ุงุณุชูุชุงุฌ ุฐูู ุนู ุทุฑูู ูุญุต ุณูุฉ `model_max_length` ููููุญุฏ:

```python
tokenizer.model_max_length
```

```python out
512
```

ุชูุดุชู ูุฐู ุงููููุฉ ูู ููู *tokenizer_config.json* ุงููุฑุชุจุท ุจููุทุฉ ุชูุชูุดุ ูู ูุฐู ุงูุญุงูุฉุ ูููููุง ุฃู ูุฑู ุฃู ุญุฌู ุงูุณูุงู ูู 512 ุฑูุฒูุงุ ุชูุงููุง ูุซู BERT.

โ๏ธ **ุฌุฑุจู!** ุจุนุถ ููุงุฐุฌ ุงููุญููุ ูุซู [BigBird](https://huggingface.co/google/bigbird-roberta-base) ู [Longformer](hf.co/allenai/longformer-base-4096)ุ ููุง ุทูู ุณูุงู ุฃุทูู ุจูุซูุฑ ูู BERT ูููุงุฐุฌ ุงููุญูู ุงููุจูุฑุฉ ุงูุฃุฎุฑู. ูู ุจุชูููุฐ ููุญุฏ ูููุทุฉ ุชูุชูุด ูุงุญุฏุฉ ูู ูุฐู ุงูููุงุฐุฌ ูุชุญูู ููุง ุฅุฐุง ูุงูุช `model_max_length` ุชุชูุงูู ูุน ูุง ูู ูุฐููุฑ ูู ุจุทุงูุฉ ูููุฐุฌูุง.

ูุฐููุ ูุชุดุบูู ุชุฌุงุฑุจูุง ุนูู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุช ูุซู ุชูู ุงูููุฌูุฏุฉ ูู Google Colabุ ุณูุฎุชุงุฑ ุดูุฆูุง ุฃุตุบุฑ ููููุงู ููููู ุงูุงูุถูุงู ุฅูู ุงูุฐุงูุฑุฉ:

```python
chunk_size = 128
```

โ ููุงุญุธุฉ ุฃู ุงุณุชุฎุฏุงู ุญุฌู ูุทุนุฉ ุตุบูุฑ ูููู ุฃู ูููู ุถุงุฑูุง ูู ุงูุณููุงุฑูููุงุช ุงููุงูุนูุฉุ ูุฐุง ูุฌุจ ุนููู ุงุณุชุฎุฏุงู ุญุฌู ูุชูุงูู ูุน ุญุงูุฉ ุงูุงุณุชุฎุฏุงู ุงูุชู ุณุชุทุจู ูููุฐุฌู ุนูููุง.

ุงูุขู ูุฃุชู ุงูุฌุฒุก ุงูููุชุน. ูุชูุถูุญ ููููุฉ ุนูู ุงูุฏูุฌุ ุฏุนูุง ูุฃุฎุฐ ุจุนุถ ุงููุฑุงุฌุนุงุช ูู ูุฌููุนุฉ ุงูุชุฏุฑูุจ ุงูููุญุฏุฉ ุงูุฎุงุตุฉ ุจูุง ูุทุจุงุนุฉ ุนุฏุฏ ุงูุฑููุฒ ููู ูุฑุงุฌุนุฉ:

```python
# Slicing produces a list of lists for each feature
tokenized_samples = tokenized_datasets["train"][:3]

for idx, sample in enumerate(tokenized_samples["input_ids"]):
    print(f"'>>> Review {idx} length: {len(sample)}'")
```

```python out
'>>> Review 0 length: 200'
'>>> Review 1 length: 559'
'>>> Review 2 length: 192'
```

ุจุนุฏ ุฐููุ ูููููุง ุฏูุฌ ุฌููุน ูุฐู ุงูุฃูุซูุฉ ุจุงุณุชุฎุฏุงู ุชุนุจูุฑ ูุงููุณ ุจุณูุทุ ููุง ููู:

```python
concatenated_examples = {
k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()
}
total_length = len(concatenated_examples["input_ids"])
print(f"'>>> Concatenated reviews length: {total_length}'")
```

```python out
'>>> Concatenated reviews length: 951'
```

ุฑุงุฆุนุ ูุชุญูู ุงูุทูู ุงูุฅุฌูุงูู - ูุฐุง ุฏุนูุง ุงูุขู ููุณู ุงููุฑุงุฌุนุงุช ุงููุฏูุฌุฉ ุฅูู ูุทุน ุจุญุฌู `chunk_size`. ููููุงู ุจุฐููุ ูููู ุจุงูุชุนููู ุนูู ุงูููุฒุงุช ูู `concatenated_examples` ููุณุชุฎุฏู ุชุนุจูุฑ ูุงุฆูุฉ ูุฅูุดุงุก ุดุฑุงุฆุญ ููู ููุฒุฉ. ุงููุชูุฌุฉ ูู ูุงููุณ ูู ุงููุทุน ููู ููุฒุฉ:

```python
chunks = {
k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
for k, t in concatenated_examples.items()
}

for chunk in chunks["input_ids"]:
print(f"'>>> Chunk length: {len(chunk)}'")
```

```python out
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 55'
```

ููุง ููููู ุฃู ุชุฑู ูู ูุฐุง ุงููุซุงูุ ุณุชููู ุงููุทุนุฉ ุงูุฃุฎูุฑุฉ ุจุดูู ุนุงู ุฃุตุบุฑ ูู ุญุฌู ุงููุทุนุฉ ุงูุฃูุตู. ููุงู ุงุณุชุฑุงุชูุฌูุชุงู ุฑุฆูุณูุชุงู ููุชุนุงูู ูุน ูุฐุง:

1. ุฅุณูุงุท ุงููุทุนุฉ ุงูุฃุฎูุฑุฉ ุฅุฐุง ูุงูุช ุฃุตุบุฑ ูู `chunk_size`.
2. ูู ุจุชุจุทูู ุงููุทุนุฉ ุงูุฃุฎูุฑุฉ ุญุชู ูุตุจุญ ุทูููุง `chunk_size`.

ุณูุฃุฎุฐ ุงูููุฌ ุงูุฃูู ููุงุ ูุฐุง ุฏุนูุง ูุบูู ูู ุงูููุทู ุฃุนูุงู ูู ุฏุงูุฉ ูุงุญุฏุฉ ูููููุง ุชุทุจูููุง ุนูู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูููุญุฏุฉ ูุฏููุง:

```python
def group_texts(examples):
# Concatenate all texts
concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
# Compute length of concatenated texts
total_length = len(concatenated_examples[list(examples.keys())[0]])
# We drop the last chunk if it's smaller than chunk_size
total_length = (total_length // chunk_size) * chunk_size
# Split by chunks of max_len
result = {
k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
for k, t in concatenated_examples.items()
}
# Create a new labels column
result["labels"] = result["input_ids"].copy()
return result
```

ูุงุญุธ ุฃูู ูู ุงูุฎุทูุฉ ุงูุฃุฎูุฑุฉ ูู `group_texts()`ุ ูููู ุจุฅูุดุงุก ุนููุฏ "ุชุณููุงุช" ุฌุฏูุฏ ููู ูุณุฎุฉ ูู ุนููุฏ "input_ids". ููุง ุณูุฑู ูุฑูุจูุงุ ูุฑุฌุน ุฐูู ุฅูู ุฃู ุงููุฏู ูู ููุฐุฌุฉ ุงููุบุฉ ุงููููุนุฉ ูู ุงูุชูุจุค ุจุงูุฑููุฒ ุงููููุนุฉ ุจุดูู ุนุดูุงุฆู ูู ุฏูุนุฉ ุงูุฅุฏุฎุงูุ ููู ุฎูุงู ุฅูุดุงุก ุนููุฏ "ุชุณููุงุช"ุ ูููุฑ ุงูุญูููุฉ ุงูุฃุฑุถูุฉ ููููุฐุฌ ุงููุบุฉ ุงูุฎุงุต ุจูุง ููุชุนูู ูููุง.

ุฏุนูุง ุงูุขู ูุทุจู `group_texts()` ุนูู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูููุญุฏุฉ ุจุงุณุชุฎุฏุงู ูุธููุฉ `Dataset.map()` ุงูููุซููุฉ ูุฏููุง:

```python
lm_datasets = tokenized_datasets.map(group_texts, batched=True)
lm_datasets
```

```python out
DatasetDict({
train: Dataset({
features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
num_rows: 61289
})
test: Dataset({
features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
num_rows: 59905
})
unsupervised: Dataset({
features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
num_rows: 122963
})
})
```

ููููู ุฃู ุชุฑู ุฃู ุชุฌููุน ุงููุตูุต ุซู ุชูุณูููุง ุฅูู ูุทุน ูุฏ ุฃูุชุฌ ุฃูุซูุฉ ุฃูุซุฑ ุจูุซูุฑ ูู 25000 ุงูุฃุตููุฉ ูุชูุณููุงุช "ุงูุชุฏุฑูุจ" ู"ุงูุงุฎุชุจุงุฑ". ููุฑุฌุน ุฐูู ุฅูู ุฃููุง ูุฏููุง ุงูุขู ุฃูุซูุฉ ุชุชุถูู _ุฑููุฒูุง ูุชุฌุงูุฑุฉ_ ุชูุชุฏ ุนุจุฑ ุฃูุซูุฉ ูุชุนุฏุฏุฉ ูู ุงููุฌููุนุฉ ุงูุฃุตููุฉ. ููููู ุฃู ุชุฑู ุฐูู ุตุฑุงุญุฉู ุนู ุทุฑูู ุงูุจุญุซ ุนู ุงูุฑููุฒ ุงูุฎุงุตุฉ `[SEP]` ู`[CLS]` ูู ุฅุญุฏู ุงููุทุน:

```python
tokenizer.decode(lm_datasets["train"][1]["input_ids"])
```

```python out
".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"
```

ูู ูุฐุง ุงููุซุงูุ ููููู ุฃู ุชุฑู ูุฑุงุฌุนุชู ุฃููุงู ูุชุฏุงุฎูุชููุ ุฅุญุฏุงููุง ุนู ูููู ูุฏุฑุณู ูุงูุฃุฎุฑู ุนู ุงูุชุดุฑุฏ. ุฏุนูุง ูููู ูุธุฑุฉ ุฃูุถูุง ุนูู ุงูุดูู ุงูุฐู ุชุจุฏู ุนููู ุงูุชุณููุงุช ูููุฐุฌุฉ ุงููุบุฉ ุงููููุนุฉ:

```python out
tokenizer.decode(lm_datasets["train"][1]["labels"])
```

```python out
".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"
```

ููุง ูู ูุชููุน ูู ุฏุงูุชูุง `group_texts()` ุฃุนูุงูุ ูุจุฏู ูุฐุง ูุชุทุงุจููุง ูุน ุฑููุฒ "input_ids" ุงููุดูุฑุฉ - ูููู ููู ูููู ููููุฐุฌูุง ุฃู ูุชุนูู ุฃู ุดูุก ุนูู ุงูุฅุทูุงูุ ูุญู ููุชูุฏ ุฎุทูุฉ ุฑุฆูุณูุฉ: ุฅุฏุฑุงุฌ ุฑููุฒ `[MASK]` ูู ููุงุถุน ุนุดูุงุฆูุฉ ูู ุงููุฏุฎูุงุช! ุฏุนููุง ูุฑู ููู ูููููุง ุงูููุงู ุจุฐูู ุฃุซูุงุก ุงูุถุจุท ุงูุฏููู ุจุงุณุชุฎุฏุงู ุฌุงูุน ุจูุงูุงุช ุฎุงุต.
## ุถุจุท ุฏููู ููููุฐุฌ DistilBERT ุจุงุณุชุฎุฏุงู ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช (API) Trainer

ุฅู ุถุจุท ุฏููู ููููุฐุฌ ุงููุบุฉ ุงููููุนุฉ ูุดุจู ุฅูู ุญุฏ ูุจูุฑ ุถุจุท ุฏููู ููููุฐุฌ ุชุตููู ุงูุชุณูุณูุ ููุง ูุนููุง ูู [ุงููุตู 3](/course/chapter3). ุงููุฑู ุงููุญูุฏ ูู ุฃููุง ูุญุชุงุฌ ุฅูู ุฌุงูุน ุจูุงูุงุช ุฎุงุต ููููู ุฅุฎูุงุก ุจุนุถ ุงูุฑููุฒ ุจุดูู ุนุดูุงุฆู ูู ูู ุฏูุนุฉ ูู ุงููุตูุต. ูุญุณู ุงูุญุธุ ูุฃุชู ๐ค Transformers ูุฒูุฏูุง ุจู `DataCollatorForLanguageModeling` ูุฎุตุต ููุฐู ุงููููุฉ ุจุงูุฐุงุช. ูู ูุง ุนูููุง ูุนูู ูู ุชูุฑูุฑ ุงููุญูู ุงููุบูู ูุญุฌุฉ `mlm_probability` ุงูุชู ุชุญุฏุฏ ุงููุณุจุฉ ุงููุฆููุฉ ููุฑููุฒ ุงูุชู ุณูุชู ุฅุฎูุงุคูุง. ุณูุฎุชุงุฑ 15ูชุ ููู ุงูููุฏุงุฑ ุงููุณุชุฎุฏู ูู BERT ูุฎูุงุฑ ุดุงุฆุน ูู ุงูุฃุฏุจูุงุช:

```python
from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```

ููุฑุคูุฉ ููููุฉ ุนูู ุงูุฅุฎูุงุก ุงูุนุดูุงุฆูุ ุฏุนููุง ูุทุนู ุจุนุถ ุงูุฃูุซูุฉ ูุฌุงูุน ุงูุจูุงูุงุช. ูุธุฑูุง ูุฃูู ูุชููุน ูุงุฆูุฉ ูู ุงูููุงููุณุ ุญูุซ ููุซู ูู ูุงููุณ ุฌุฒุกูุง ูุงุญุฏูุง ูู ุงููุต ุงููุชูุงุตูุ ูุฅููุง ูููู ุฃููุงู ุจุงูุชุนููู ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ูุจู ุชุบุฐูุฉ ุงูุฏูุนุฉ ุฅูู ุฌุงูุน ุงูุจูุงูุงุช. ูููู ุจุฅุฒุงูุฉ ููุชุงุญ "word_ids" ููุฐุง ุฌุงูุน ุงูุจูุงูุงุช ูุฃูู ูุง ูุชููุน ุฐูู:

```python
samples = [lm_datasets["train"][i] for i in range(2)]
for sample in samples:
_ = sample.pop("word_ids")

for chunk in data_collator(samples)["input_ids"]:
print(f"\n'>>> {tokenizer.decode(chunk)}'")
```

```python output
'>>> [CLS] bromwell [MASK] is a cartoon comedy. it ran at the same [MASK] as some other [MASK] about school life, [MASK] as " teachers ". [MASK] [MASK] [MASK] in the teaching [MASK] lead [MASK] to believe that bromwell high\'[MASK] satire is much closer to reality than is " teachers ". the scramble [MASK] [MASK] financially, the [MASK]ful students whogn [MASK] right through [MASK] pathetic teachers\'pomp, the pettiness of the whole situation, distinction remind me of the schools i knew and their students. when i saw [MASK] episode in [MASK] a student repeatedly tried to burn down the school, [MASK] immediately recalled. [MASK]...'

'>>> .... at.. [MASK]... [MASK]... high. a classic line plucked inspector : i\'[MASK] here to [MASK] one of your [MASK]. student : welcome to bromwell [MASK]. i expect that many adults of my age think that [MASK]mwell [MASK] is [MASK] fetched. what a pity that it isn\'t! [SEP] [CLS] [MASK]ness ( or [MASK]lessness as george ๅฎin stated )ๅฌ been an issue for years but never [MASK] plan to help those on the street that were once considered human [MASK] did everything from going to school, [MASK], [MASK] vote for the matter. most people think [MASK] the homeless'
```

ุฑุงุฆุนุ ููุฏ ูุฌุญ ุงูุฃูุฑ! ูููููุง ุฃู ูุฑู ุฃู ุงูุฑูุฒ `[MASK]` ุชู ุฅุฏุฎุงูู ุจุดูู ุนุดูุงุฆู ูู ููุงูุน ูุฎุชููุฉ ูู ูุตูุง. ุณุชููู ูุฐู ูู ุงูุฑููุฒ ุงูุชู ุณูุชุนูู ุนูู ูููุฐุฌูุง ุงูุชูุจุค ุจูุง ุฃุซูุงุก ุงูุชุฏุฑูุจ - ูุฌูุงู ุฌุงูุน ุงูุจูุงูุงุช ูู ุฃูู ุณูููู ุจุชุนุดูุฉ ุฅุฏุฎุงู ุงูุฑูุฒ `[MASK]` ูุน ูู ุฏูุนุฉ!

<Tip>

โ๏ธ **ุฌุฑุจู!** ูู ุจุชุดุบูู ููุชุทู ุงูููุฏ ุฃุนูุงู ุนุฏุฉ ูุฑุงุช ููุดุงูุฏุฉ ุงูุฅุฎูุงุก ุงูุนุดูุงุฆู ูุญุฏุซ ุฃูุงู ุนูููู ูุจุงุดุฑุฉ! ุฃูุถูุงุ ุงุณุชุจุฏู ุทุฑููุฉ `tokenizer.decode()` ุจู `tokenizer.convert_ids_to_tokens()` ููุดุงูุฏุฉ ุฃูู ูุชู ุฅุฎูุงุก ุฑูุฒ ูุงุญุฏ ูู ูููุฉ ูุนููุฉ ูู ุจุนุถ ุงูุฃุญูุงูุ ูููุณ ุงูุขุฎุฑูู.

</Tip>

ุนูุฏูุง ูุชุนูู ุงูุฃูุฑ ุจุชุฏุฑูุจ ููุงุฐุฌ ุนูู ููุฐุฌุฉ ุงููุบุฉ ุงููููุนุฉุ ูููู ุงุณุชุฎุฏุงู ุชูููุฉ ูุงุญุฏุฉ ุชุชูุซู ูู ุฅุฎูุงุก ูููุงุช ูุงููุฉ ูุนูุงุ ูููุณ ููุท ุงูุฑููุฒ ุงููุฑุฏูุฉ. ููุทูู ุนูู ูุฐุง ุงูููุฌ ุงุณู _ุฅุฎูุงุก ุงููููุงุช ุงููุงููุฉ_. ุฅุฐุง ุฃุฑุฏูุง ุงุณุชุฎุฏุงู ุฅุฎูุงุก ุงููููุงุช ุงููุงููุฉุ ูุณูู ูุญุชุงุฌ ุฅูู ุจูุงุก ุฌุงูุน ุจูุงูุงุช ุจุฃููุณูุง. ุฌุงูุน ุงูุจูุงูุงุช ูู ูุฌุฑุฏ ุฏุงูุฉ ุชุฃุฎุฐ ูุงุฆูุฉ ูู ุงูุนููุงุช ูุชุญููููุง ุฅูู ุฏูุนุฉุ ูุฐุง ุฏุนููุง ููุนู ุฐูู ุงูุขู! ุณูุณุชุฎุฏู ูุนุฑููุงุช ุงููููุงุช ุงููุญุณูุจุฉ ุณุงุจููุง ูุฅูุดุงุก ุฎุฑูุทุฉ ุจูู ูุคุดุฑุงุช ุงููููุงุช ูุงูุฑููุฒ ุงูููุงุจูุฉุ ุซู ููุฑุฑ ุจุดูู ุนุดูุงุฆู ุฃู ุงููููุงุช ุณูุชู ุฅุฎูุงุคูุง ูุชุทุจูู ุงูููุงุน ุนูู ุงูุฅุฏุฎุงูุงุช. ูุงุญุธ ุฃู ุงูุชุตูููุงุช ุฌููุนูุง -100 ุจุงุณุชุซูุงุก ุชูู ุงูุชู ุชูุงุจู ูููุงุช ุงูููุงุน.

```py
import collections
import numpy as np

from transformers import default_data_collator

wwm_probability = 0.2


def whole_word_masking_data_collator(features):
for feature in features:
word_ids = feature.pop("word_ids")

# ูู ุจุฅูุดุงุก ุฎุฑูุทุฉ ุจูู ุงููููุงุช ููุคุดุฑุงุช ุงูุฑููุฒ ุงูููุงุจูุฉ
mapping = collections.defaultdict(list)
current_word_index = -1
current_word = None
for idxุ word_id ูู enumerate(word_ids):
if word_id is not None:
if word_id != current_word:
current_word = word_id
current_word_index += 1
mapping[current_word_index].append(idx)

# ุฅุฎูุงุก ุงููููุงุช ุจุดูู ุนุดูุงุฆู
mask = np.random.binomial(1, wwm_probability, (len(mapping),))
input_ids = feature["input_ids"]
labels = feature["labels"]
new_labels = [-100] * len(labels)
for word_id in np.where(mask)[0]:
word_id = word_id.item()
for idx in mapping[word_id]:
new_labels[idx] = labels[idx]
input_ids[idx] = tokenizer.mask_token_id
feature["labels"] = new_labels

return default_data_collator(features)
```

ุจุนุฏ ุฐููุ ูููููุง ุชุฌุฑุจุชู ุนูู ููุณ ุงูุนููุงุช ููุง ูู ุงูุญุงู ูุจู:

```py
samples = [lm_datasets["train"][i] for i in range(2)]
batch = whole_word_masking_data_collator(samples)

for chunk in batch["input_ids"]:
print(f"\n'>>> {tokenizer.decode(chunk)}'")
```

```python out
'>>> [CLS] bromwell high is a cartoon comedy [MASK] it ran at the same time as some other programs about school life, such as " teachers ". my 35 years in the teaching profession lead me to believe that bromwell high\'s satire is much closer to reality than is " teachers ". the scramble to survive financially, the insightful students who can see right through their pathetic teachers\'pomp, the pettiness of the whole situation, all remind me of the schools i knew and their students. when i saw the episode in which a student repeatedly tried to burn down the school, i immediately recalled.....'

'>>> .... [MASK] [MASK] [MASK] [MASK]....... high. a classic line : inspector : i\'m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn\'t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless'
```

<Tip>

โ๏ธ **ุฌุฑุจู!** ูู ุจุชุดุบูู ููุชุทู ุงูููุฏ ุฃุนูุงู ุนุฏุฉ ูุฑุงุช ููุดุงูุฏุฉ ุงูุฅุฎูุงุก ุงูุนุดูุงุฆู ูุญุฏุซ ุฃูุงู ุนูููู ูุจุงุดุฑุฉ! ุฃูุถูุงุ ุงุณุชุจุฏู ุทุฑููุฉ `tokenizer.decode()` ุจู `tokenizer.convert_ids_to_tokens()` ููุดุงูุฏุฉ ุฑููุฒ ุงููููุฉ ุงููุนุทุงุฉ ูุชู ุฅุฎูุงุคูุง ุฏุงุฆููุง ูุนูุง.

</Tip>

ุงูุขู ุจุนุฏ ุฃู ุฃุตุจุญ ูุฏููุง ุฌุงูุน ุจูุงูุงุชุ ูุฅู ุจููุฉ ุฎุทูุงุช ุงูุถุจุท ุงูุฏููู ููุงุณูุฉ. ูููู ุฃู ูุณุชุบุฑู ุงูุชุฏุฑูุจ ุจุนุถ ุงูููุช ุนูู Google Colab ุฅุฐุง ูู ุชูู ูุญุธูุธูุง ุจูุง ูููู ูุชุณุฌูู ููุงุท P100 GPU ๐ญุ ูุฐุง ูุณูููู ุฃููุงู ุจุชุฎููุถ ุญุฌู ูุฌููุนุฉ ุงูุชุฏุฑูุจ ุฅูู ุจุถุนุฉ ุขูุงู ูู ุงูุฃูุซูุฉ. ูุง ุชูููุ ูุง ูุฒุงู ุจุฅููุงููุง ุงูุญุตูู ุนูู ูููุฐุฌ ูุบุฉ ูุงุฆู ุฌุฏูุง! ุฅุญุฏู ุงูุทุฑู ุงูุณุฑูุนุฉ ูุชุฎููุถ ุนููุฉ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ูู ๐ค Datasets ูู ุนุจุฑ ุฏุงูุฉ `Dataset.train_test_split()` ุงูุชู ุฑุฃููุงูุง ูู [ุงููุตู 5](/course/chapter5):

```python
train_size = 10_000
test_size = int(0.1 * train_size)

downsampled_dataset = lm_datasets["train"].train_test_split(
train_size=train_size, test_size=test_size, seed=42
)
downsampled_dataset
```

```python out
DatasetDict({
train: Dataset({
features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
num_rows: 10000
})
test: Dataset({
features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
num_rows: 1000
})
})
```

ููุฏ ูุงู ูุฐุง ุชููุงุฆููุง ุจุฅูุดุงุก ุชูุณููุงุช "train" ู"test" ุฌุฏูุฏุฉุ ูุน ุชุนููู ุญุฌู ูุฌููุนุฉ ุงูุชุฏุฑูุจ ุฅูู 10000 ูุซุงู ูุญุฌู ุงูุชุญูู ูู ุงูุตุญุฉ ุฅูู 10ูช ูู ุฐูู - ูุง ุชุชุฑุฏุฏ ูู ุฒูุงุฏุฉ ุฐูู ุฅุฐุง ูุงู ูุฏูู ูุญุฏุฉ ูุนุงูุฌุฉ ุฑุณููุงุช (GPU) ูููุฉ! ุงูุดูุก ุงูุชุงูู ุงูุฐู ูุญุชุงุฌ ุฅูู ุงูููุงู ุจู ูู ุชุณุฌูู ุงูุฏุฎูู ุฅูู Hub Hugging Face. ุฅุฐุง ููุช ุชุดุบู ูุฐุง ุงูููุฏ ูู ุฏูุชุฑ ููุงุญุธุงุชุ ูููููู ุงูููุงู ุจุฐูู ุจุงุณุชุฎุฏุงู ุฏุงูุฉ ุงููุณุงุนุฏุฉ ูุฐู:

```python
from huggingface_hub import notebook_login

notebook_login()
```

ูุงูุฐู ุณูุธูุฑ ูุฑุจุน ุญูุงุฑ ููููู ูู ุฅุฏุฎุงู ุจูุงูุงุช ุงูุงุนุชูุงุฏ ุงูุฎุงุตุฉ ุจู. ุจุฏูุงู ูู ุฐููุ ููููู ุชุดุบูู:

```
huggingface-cli login
```

ูู ุงููุญุทุฉ ุงูุทุฑููุฉ ุงูููุถูุฉ ูุฏูู ูุชุณุฌูู ุงูุฏุฎูู ููุงู.

ุจูุฌุฑุฏ ุชุณุฌูู ุงูุฏุฎููุ ูููููุง ุฅูุดุงุก ูุฌููุนุงุช ุจูุงูุงุช TensorFlow ุงูุฎุงุตุฉ ุจูุง. ููููุงู ุจุฐููุ ุณูุณุชุฎุฏู ุทุฑููุฉ `prepare_tf_dataset()`ุ ูุงูุชู ุชุณุชุฎุฏู ูููุฐุฌูุง ููุชูุจุค ุชููุงุฆููุง ุจุงูุฃุนูุฏุฉ ุงูุชู ูุฌุจ ุฅุฏุฎุงููุง ูู ูุฌููุนุฉ ุงูุจูุงูุงุช. ุฅุฐุง ููุช ุชุฑูุฏ ุงูุชุญูู ุจุฏูุฉ ูู ุงูุฃุนูุฏุฉ ุงูุชู ุณูุชู ุงุณุชุฎุฏุงููุงุ ูููููู ุงุณุชุฎุฏุงู ุทุฑููุฉ `Dataset.to_tf_dataset()` ุจุฏูุงู ูู ุฐูู. ููุญูุงุธ ุนูู ุงูุฃููุฑ ุจุณูุทุฉุ ุณูุณุชุฎุฏู ุฌุงูุน ุงูุจูุงูุงุช ุงูููุงุณู ููุงุ ูููู ููููู ุฃูุถูุง ุชุฌุฑุจุฉ ุฌุงูุน ุจูุงูุงุช ุฅุฎูุงุก ุงููููุงุช ุงููุงููุฉ ูููุงุฑูุฉ ุงููุชุงุฆุฌ ูููุงุฑุณุฉ:

```python
tf_train_dataset = model.prepare_tf_dataset(
downsampled_dataset["train"],
collate_fn=data_collator,
shuffle=True,
batch_size=32,
)

tf_eval_dataset = model.prepare_tf_dataset(
downsampled_dataset["test"],
collate_fn=data_collator,
shuffle=False,
batch_size=32,
)
```

ุจุนุฏ ุฐููุ ูููู ุจุถุจุท ูุฑุท ูุนููุงุช ุงูุชุฏุฑูุจ ุงูุฎุงุตุฉ ุจูุง ูุชุฌููุน ูููุฐุฌูุง. ูุณุชุฎุฏู ุฏุงูุฉ `create_optimizer()` ูู ููุชุจุฉ ๐ค Transformersุ ูุงูุชู ุชุนุทููุง ูุญุณู AdamW ุจูุนุฏู ุชุนูู ุฎุทู. ูุณุชุฎุฏู ุฃูุถูุง ุงูุฎุณุงุฑุฉ ุงููุฏูุฌุฉ ูููููุฐุฌุ ูุงูุชู ุชููู ุงูุงูุชุฑุงุถูุฉ ุนูุฏ ุนุฏู ุชุญุฏูุฏ ุฎุณุงุฑุฉ ูุญุฌุฉ ูู `compile()`ุ ููุญุฏุฏ ุฏูุฉ ุงูุชุฏุฑูุจ ุฅูู `"mixed_float16"`. ูุงุญุธ ุฃูู ุฅุฐุง ููุช ุชุณุชุฎุฏู ูุญุฏุฉ ูุนุงูุฌุฉ ุฑุณููุงุช (GPU) ูู Colab ุฃู ูุญุฏุฉ ูุนุงูุฌุฉ ุฑุณููุงุช (GPU) ุฃุฎุฑู ูุง ุชุฏุนู float16 ุงููุนุฌูุ ููุฌุจ ุนููู ุนูู ุงูุฃุฑุฌุญ ุงูุชุนููู ุนูู ูุฐุง ุงูุณุทุฑ.

ุจุงูุฅุถุงูุฉ ุฅูู ุฐููุ ูููู ุจุฅุนุฏุงุฏ `PushToHubCallback` ุงูุฐู ุณูุญูุธ ุงููููุฐุฌ ุฅูู Hub ุจุนุฏ ูู ุญูุจุฉ. ููููู ุชุญุฏูุฏ ุงุณู ุงููุณุชูุฏุน ุงูุฐู ุชุฑูุฏ ุฏูุนู ุฅูู ุญุฌุฉ `hub_model_id` (ุนูู ูุฌู ุงูุฎุตูุตุ ุณูุชุนูู ุนููู ุงุณุชุฎุฏุงู ูุฐู ุงูุญุฌุฉ ููุฏูุน ุฅูู ููุธูุฉ). ุนูู ุณุจูู ุงููุซุงูุ ูุฏูุน ุงููููุฐุฌ ุฅูู ููุธูุฉ [`huggingface-course`](https://huggingface.co/huggingface-course)ุ ุฃุถููุง `hub_model_id="huggingface-course/distilbert-finetuned-imdb"`. ุจุดูู ุงูุชุฑุงุถูุ ุณูุชู ุงุณุชุฎุฏุงู ุงููุณุชูุฏุน ุงูููุฌูุฏ ูู ูุณุงุญุฉ ุงูุงุณู ุงูุฎุงุตุฉ ุจู ููุชู ุชุณููุชู ุจุงุณู ุฏููู ุงูุฅุฎุฑุงุฌ ุงูุฐู ููุช ุจุชุนููููุ ูุฐุง ูู ุญุงูุชูุง ุณูููู `"lewtun/distilbert-finetuned-imdb"`.

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
init_lr=2e-5,
num_warmup_steps=1_000,
num_train_steps=num_train_steps,
weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# ุชุฏุฑูุจ ูู ุฏูุฉ ุงูููุทุฉ ุงูุนุงุฆูุฉ ุงููุฎุชูุทุฉ 16
tf.keras.mixed_precision.set_global_policy("mixed_float16")

model_name = model_checkpoint.split("/")[-1]
callback = PushToHubCallback(
output_dir=f"{model_name}-finetuned-imdb"ุ tokenizer=tokenizer
)
```

ูุญู ุงูุขู ุนูู ุงุณุชุนุฏุงุฏ ูุชุดุบูู `model.fit()` - ูููู ูุจู ุงูููุงู ุจุฐููุ ุฏุนููุง ูููู ูุธุฑุฉ ุณุฑูุนุฉ ุนูู _ุงูุญูุฑุฉ_ุ ูุงูุชู ุชุนุฏ ูููุงุณูุง ุดุงุฆุนูุง ูุชูููู ุฃุฏุงุก ููุงุฐุฌ ุงููุบุฉ.

{:else}

ุจูุฌุฑุฏ ุชุณุฌูู ุงูุฏุฎููุ ูููููุง ุชุญุฏูุฏ ุงูุญุฌุฌ ูู `Trainer`:

```python
from transformers import TrainingArguments

batch_size = 64
# ุนุฑุถ ุฎุณุงุฑุฉ ุงูุชุฏุฑูุจ ูุน ูู ุญูุจุฉ
logging_steps = len(downsampled_dataset["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

training_args = TrainingArguments(
output_dir=f"{model_name}-finetuned-imdb"ุ
overwrite_output_dir=Trueุ
ุงุณุชุฑุงุชูุฌูุฉ ุงูุชูููู = "epoch"ุ
ูุนุฏู ุงูุชุนูู = 2e-5ุ
weight_decay = 0.01ุ
per_device_train_batch_size=batch_sizeุ
per_device_eval_batch_size=batch_sizeุ
push_to_hub=Trueุ
fp16=Trueุ
logging_steps=logging_stepsุ
)
```

ููุฏ ูููุง ุจุชุนุฏูู ุจุนุถ ุงูุฎูุงุฑุงุช ุงูุงูุชุฑุงุถูุฉ ููุงุ ุจูุง ูู ุฐูู `logging_steps` ูุถูุงู ุชุชุจุน ุฎุณุงุฑุฉ ุงูุชุฏุฑูุจ ูุน ูู ุญูุจุฉ. ููุฏ ุงุณุชุฎุฏููุง ุฃูุถูุง `fp16=True` ูุชูููู ุงูุชุฏุฑูุจ ุงูุฏูููุ ูุงูุฐู ูุนุทููุง ุฏูุนุฉ ุฃุฎุฑู ูู ุงูุณุฑุนุฉ. ุจุดูู ุงูุชุฑุงุถูุ ุณูููู `Trainer` ุจุฅุฒุงูุฉ ุฃู ุฃุนูุฏุฉ ููุณุช ุฌุฒุกูุง ูู ุทุฑููุฉ `forward()` ูููููุฐุฌ. ููุฐุง ูุนูู ุฃูู ุฅุฐุง ููุช ุชุณุชุฎุฏู ุฌุงูุน ุจูุงูุงุช ุฅุฎูุงุก ุงููููุงุช ุงููุงููุฉุ ูุณูู ุชุญุชุงุฌ ุฃูุถูุง ุฅูู ุชุนููู `remove_unused_columns=False` ูุถูุงู ุนุฏู ููุฏุงู ุนููุฏ "word_ids" ุฃุซูุงุก ุงูุชุฏุฑูุจ.

ูุงุญุธ ุฃูู ููููู ุชุญุฏูุฏ ุงุณู ุงููุณุชูุฏุน ุงูุฐู ุชุฑูุฏ ุฏูุนู ุฅูู ุญุฌุฉ `hub_model_id` (ุนูู ูุฌู ุงูุฎุตูุตุ ุณูุชุนูู ุนููู ุงุณุชุฎุฏุงู ูุฐู ุงูุญุฌุฉ ููุฏูุน ุฅูู ููุธูุฉ). ุนูู ุณุจูู ุงููุซุงูุ ุนูุฏูุง ูููุง ุจุฏูุน ุงููููุฐุฌ ุฅูู ููุธูุฉ [`huggingface-course`](https://huggingface.co/huggingface-course)ุ ุฃุถููุง `hub_model_id="huggingface-course/distilbert-finetuned-imdb"` ุฅูู `TrainingArguments`. ุจุดูู ุงูุชุฑุงุถูุ ุณูุชู ุงุณุชุฎุฏุงู ุงููุณุชูุฏุน ุงูููุฌูุฏ ูู ูุณุงุญุฉ ุงูุงุณู ุงูุฎุงุตุฉ ุจู ููุชู ุชุณููุชู ุจุงุณู ุฏููู ุงูุฅุฎุฑุงุฌ ุงูุฐู ููุช ุจุชุนููููุ ูุฐุง ูู ุญุงูุชูุง ุณูููู `"lewtun/distilbert-finetuned-imdb"`.

ุงูุขู ูุฏููุง ูู ุงูููููุงุช ุงููุงุฒูุฉ ูุชููุฆุฉ `Trainer`. ููุง ูุณุชุฎุฏู ุฌุงูุน ุงูุจูุงูุงุช ุงูููุงุณู ููุทุ ูููู ููููู ุชุฌุฑุจุฉ ุฌุงูุน ุจูุงูุงุช ุฅุฎูุงุก ุงููููุงุช ุงููุงููุฉ ูููุงุฑูุฉ ุงููุชุงุฆุฌ ูููุงุฑุณุฉ:

```python
from transformers import Trainer

trainer = Trainer(
model=modelุ
args=training_argsุ
train_dataset=downsampled_dataset["train"]ุ
eval_dataset=downsampled_dataset["test"]ุ
data_collator=data_collatorุ
tokenizer=tokenizerุ
)
```

ูุญู ุงูุขู ุนูู ุงุณุชุนุฏุงุฏ ูุชุดุบูู `trainer.train()` - ูููู ูุจู ุงููู
## ุญูุฑุฉ ุงููุบุฉ ูููุงุฐุฌ ุงููุบุฉ

ุนูู ุนูุณ ุงูููุงู ุงูุฃุฎุฑู ูุซู ุชุตููู ุงููุต ุฃู ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉุ ุญูุซ ูุชู ุฅุนุทุงุคูุง ูุฌููุนุฉ ุจูุงูุงุช ููุณููุฉ ููุชุฏุฑูุจุ ูู ููุฐุฌุฉ ุงููุบุฉ ูุง ุชูุฌุฏ ุฃู ุนูุงูุงุช ูุงุถุญุฉ. ุฅุฐูุ ููู ูุญุฏุฏ ูุง ุงูุฐู ูุฌุนู ูููุฐุฌ ุงููุบุฉ ุฌูุฏูุงุ ุชูุงููุง ูุซู ููุฒุฉ ุงูุชุตุญูุญ ุงูุชููุงุฆู ูู ูุงุชููุ ูุฅู ูููุฐุฌ ุงููุบุฉ ุงูุฌูุฏ ูู ุงูุฐู ูุนุทู ุงุญุชูุงูุงุช ุนุงููุฉ ููุฌูู ุงูุชู ุชููู ููุงุนุฏ ุงููุบุฉ ูููุง ุตุญูุญุฉุ ูุงุญุชูุงูุงุช ููุฎูุถุฉ ููุฌูู ุบูุฑ ุงููููููุฉ. ููุฅุนุทุงุฆู ููุฑุฉ ุฃูุถู ุนูุง ูุจุฏู ุนููู ุฐููุ ููููู ุงูุนุซูุฑ ุนูู ูุฌููุนุงุช ูุงููุฉ ูู "ูุดู ุงูุชุตุญูุญ ุงูุชููุงุฆู" ุนุจุฑ ุงูุฅูุชุฑูุชุ ุญูุซ ุฃูุชุฌ ุงููููุฐุฌ ูู ูุงุชู ุงูุดุฎุต ุจุนุถ ุงูุฅููุงู ุงููุถุญู (ูุบูุฑ ุงูููุงุณุจ ูู ูุซูุฑ ูู ุงูุฃุญูุงู)!

ุจุงูุชุฑุงุถ ุฃู ูุฌููุนุฉ ุงุฎุชุจุงุฑูุง ุชุชููู ูู ูุนุธููุง ูู ุฌูู ุตุญูุญุฉ ูุญูููุงุ ูุฅู ุฅุญุฏู ุทุฑู ููุงุณ ุฌูุฏุฉ ูููุฐุฌ ุงููุบุฉ ุงูุฎุงุต ุจูุง ูู ุญุณุงุจ ุงูุงุญุชูุงูุงุช ุงูุชู ูุนุทููุง ูููููุฉ ุงูุชุงููุฉ ูู ุฌููุน ุฌูู ูุฌููุนุฉ ุงูุงุฎุชุจุงุฑ. ุชุดูุฑ ุงูุงุญุชูุงูุงุช ุงูุนุงููุฉ ุฅูู ุฃู ุงููููุฐุฌ "ุบูุฑ ููุฏูุด" ุฃู "ูุญุชุงุฑ" ูู ุงูุฃูุซูุฉ ุบูุฑ ุงููุฑุฆูุฉุ ูุชุดูุฑ ุฅูู ุฃูู ุชุนูู ุงูุฃููุงุท ุงูุฃุณุงุณูุฉ ูููุงุนุฏ ุงููุบุฉ ูู ุงููุบุฉ. ููุงู ุงูุนุฏูุฏ ูู ุงูุชุนุฑููุงุช ุงูุฑูุงุถูุฉ ููุญูุฑุฉุ ูููู ุงูุชุนุฑูู ุงูุฐู ุณูุณุชุฎุฏูู ูุนุฑููุง ุนูู ุฃููุง ุงูุฃุณูุฉ ูููุฏุงู ุงูุงูุชุฑูุจูุง ุงููุชูุงุทุนุฉ. ูุจุงูุชุงููุ ูููููุง ุญุณุงุจ ุญูุฑุฉ ูููุฐุฌูุง ุงูููุฏุฑุจ ูุณุจููุง ุจุงุณุชุฎุฏุงู ูุธููุฉ `Trainer.evaluate()` ูุญุณุงุจ ููุฏุงู ุงูุงูุชุฑูุจูุง ุงููุชูุงุทุน ุนูู ูุฌููุนุฉ ุงูุงุฎุชุจุงุฑ ุซู ุฃุฎุฐ ุงูุฃุณูุฉ ูููุชูุฌุฉ:

```python
import math

eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```

ุจุงูุชุฑุงุถ ุฃู ูุฌููุนุฉ ุงุฎุชุจุงุฑูุง ุชุชููู ูู ูุนุธููุง ูู ุฌูู ุตุญูุญุฉ ูุญูููุงุ ูุฅู ุฅุญุฏู ุทุฑู ููุงุณ ุฌูุฏุฉ ูููุฐุฌ ุงููุบุฉ ุงูุฎุงุต ุจูุง ูู ุญุณุงุจ ุงูุงุญุชูุงูุงุช ุงูุชู ูุนุทููุง ูููููุฉ ุงูุชุงููุฉ ูู ุฌููุน ุฌูู ูุฌููุนุฉ ุงูุงุฎุชุจุงุฑ. ุชุดูุฑ ุงูุงุญุชูุงูุงุช ุงูุนุงููุฉ ุฅูู ุฃู ุงููููุฐุฌ ูุดูุฑ ุฅูู ุฃู ุงููููุฐุฌ "ุบูุฑ ููุฏูุด" ุฃู "ูุญุชุงุฑ" ูู ุงูุฃูุซูุฉ ุบูุฑ ุงููุฑุฆูุฉุ ูุชุดูุฑ ุฅูู ุฃูู ุชุนูู ุงูุฃููุงุท ุงูุฃุณุงุณูุฉ ูููุงุนุฏ ุงููุบุฉ ูู ุงููุบุฉ. ููุงู ุงูุนุฏูุฏ ูู ุงูุชุนุฑููุงุช ุงูุฑูุงุถูุฉ ููุญูุฑุฉุ ูููู ุงูุชุนุฑูู ุงูุฐู ุณูุณุชุฎุฏูู ูุนุฑููุง ุนูู ุฃููุง ุงูุฃุณูุฉ ูููุฏุงู ุงูุงูุชุฑูุจูุง ุงููุชูุงุทุนุฉ. ูุจุงูุชุงููุ ูููููุง ุญุณุงุจ ุญูุฑุฉ ูููุฐุฌูุง ุงูููุฏุฑุจ ูุณุจููุง ุจุงุณุชุฎุฏุงู ุทุฑููุฉ `model.evaluate()` ูุญุณุงุจ ููุฏุงู ุงูุงูุชุฑูุจูุง ุงููุชูุงุทุน ุนูู ูุฌููุนุฉ ุงูุงุฎุชุจุงุฑ ุซู ุฃุฎุฐ ุงูุฃุณูุฉ ูููุชูุฌุฉ:

```python
import math

eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexity: {math.exp(eval_loss):.2f}")
```

```python out
>>> Perplexity: 21.75
```

ูุดูุฑ ุงูุฎูุงุถ ุฏุฑุฌุฉ ุงูุญูุฑุฉ ุฅูู ุชุญุณู ูููุฐุฌ ุงููุบุฉุ ููููููุง ุฃู ูุฑู ููุง ุฃู ูููุฐุฌูุง ุงูุฃููู ูู ูููุฉ ูุจูุฑุฉ ุฅูู ุญุฏ ูุง. ุฏุนููุง ูุฑู ุฅุฐุง ูุงู ุจุฅููุงููุง ุฎูุถู ุนู ุทุฑูู ุงูุถุจุท ุงูุฏููู! ููููุงู ุจุฐููุ ูููู ุฃููุงู ุจุชุดุบูู ุญููุฉ ุงูุชุฏุฑูุจ:

```python
trainer.train()
```

```python
model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```

ุซู ูุญุณุจ ุงูุญูุฑุฉ ุงููุงุชุฌุฉ ุนูู ูุฌููุนุฉ ุงูุงุฎุชุจุงุฑ ููุง ูู ููุถุญ ุณุงุจููุง:

```python
eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```

```python
eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexity: {math.exp(eval_loss):.2f}")
```

```python out
>>> Perplexity: 11.32
```

ุฑุงุฆุน - ูุฐุง ุงูุฎูุงุถ ูุจูุฑ ูู ุงูุญูุฑุฉุ ููุง ูุฎุจุฑูุง ุฃู ุงููููุฐุฌ ูุฏ ุชุนูู ุดูุฆูุง ุนู ูุฌุงู ูุฑุงุฌุนุงุช ุงูุฃููุงู!

ุจูุฌุฑุฏ ุงูุงูุชูุงุก ูู ุงูุชุฏุฑูุจุ ูููููุง ุฏูุน ุจุทุงูุฉ ุงููููุฐุฌ ุจูุนูููุงุช ุงูุชุฏุฑูุจ ุฅูู ุงููุญูุฑ (ูุชู ุญูุธ ููุงุท ุงูุชูุชูุด ุฃุซูุงุก ุงูุชุฏุฑูุจ ููุณู):

```python
trainer.push_to_hub()
```

โ๏ธ **ุฌุฑุจ ุจููุณู!** ูู ุจุชุดุบูู ุงูุชุฏุฑูุจ ุฃุนูุงู ุจุนุฏ ุชุบููุฑ ุฌุงูุน ุงูุจูุงูุงุช ุฅูู ุฌุงูุน ุงูุชูููู ุจูููุฉ ูุงููุฉ. ูู ุชุญุตู ุนูู ูุชุงุฆุฌ ุฃูุถูุ

ูู ุญุงูุชูุง ุงูุงุณุชุฎุฏุงููุฉุ ูู ููู ุจุญุงุฌุฉ ุฅูู ุงูููุงู ุจุฃู ุดูุก ุฎุงุต ูุน ุญููุฉ ุงูุชุฏุฑูุจุ ูููู ูู ุจุนุถ ุงูุญุงูุงุช ูุฏ ุชุญุชุงุฌ ุฅูู ุชูููุฐ ููุทู ูุฎุตุต. ููุฐู ุงูุชุทุจููุงุชุ ููููู ุงุณุชุฎุฏุงู ๐ค Accelerate - ุฏุนููุง ูููู ูุธุฑุฉ!

## ุถุจุท ุฏููู ูู DistilBERT ุจุงุณุชุฎุฏุงู ๐ค Accelerate

ููุง ุฑุฃููุง ูุน `Trainer`ุ ูุฅู ุงูุถุจุท ุงูุฏููู ููููุฐุฌ ุงููุบุฉ ุงููููุน ูุดุจู ุฅูู ุญุฏ ูุจูุฑ ูุซุงู ุชุตููู ุงููุต ูู ุงููุตู 3. ูู ุงููุงูุนุ ูุฅู ุงูุฏูุฉ ุงููุญูุฏุฉ ูู ุงุณุชุฎุฏุงู ุฌุงูุน ุจูุงูุงุช ุฎุงุตุ ููุฏ ุบุทููุง ุฐูู ุจุงููุนู ูู ูุฐุง ุงููุณู!

ููุน ุฐููุ ููุฏ ุฑุฃููุง ุฃู `DataCollatorForLanguageModeling` ูุทุจู ุฃูุถูุง ุงูุชูููู ุงูุนุดูุงุฆู ูุน ูู ุชููููุ ูุฐุง ูุณูุฑู ุจุนุถ ุงูุชููุจุงุช ูู ุฏุฑุฌุงุช ุงูุญูุฑุฉ ูุน ูู ุชุดุบูู ุชุฏุฑูุจ. ุฅุญุฏู ุทุฑู ุงููุถุงุก ุนูู ูุฐุง ุงููุตุฏุฑ ูู ุงูุนุดูุงุฆูุฉ ูู ุชุทุจูู ุงูุชูููู ูุฑุฉ ูุงุญุฏุฉ ุนูู ูุฌููุนุฉ ุงูุงุฎุชุจุงุฑ ุจุฃููููุงุ ุซู ุงุณุชุฎุฏุงู ุฌุงูุน ุงูุจูุงูุงุช ุงูุงูุชุฑุงุถู ูู ๐ค Transformers ูุฌูุน ุงูุฏูุนุงุช ุฃุซูุงุก ุงูุชูููู. ููุฑุคูุฉ ููููุฉ ุนูู ุฐููุ ุฏุนููุง ูููุฐ ุฏุงูุฉ ุจุณูุทุฉ ุชุทุจู ุงูุชูููู ุนูู ุฏูุนุฉุ ูุดุงุจูุฉ ูุชุฌุฑุจุชูุง ุงูุฃููู ูุน `DataCollatorForLanguageModeling`:

```python
def insert_random_mask(batch):
features = [dict(zip(batch, t)) for t in zip(*batch.values())]
masked_inputs = data_collator(features)
# Create a new "masked" column for each column in the dataset
return {"masked_" + k: v.numpy() for k, v in masked_inputs.items()}
```

ุจุนุฏ ุฐููุ ุณูุทุจู ูุฐู ุงูุฏุงูุฉ ุนูู ูุฌููุนุฉ ุงูุงุฎุชุจุงุฑ ุงูุฎุงุตุฉ ุจูุง ููุญุฐู ุงูุฃุนูุฏุฉ ุบูุฑ ุงููููุนุฉ ุญุชู ูุชููู ูู ุงุณุชุจุฏุงููุง ุจุงูุฃุนูุฏุฉ ุงููููุนุฉ. ููููู ุงุณุชุฎุฏุงู ุงูุชูููู ุจูููุฉ ูุงููุฉ ุนู ุทุฑูู ุงุณุชุจุฏุงู `data_collator` ุฃุนูุงู ุจุงูููุงุณุจุ ููู ูุฐู ุงูุญุงูุฉ ูุฌุจ ุนููู ุฅุฒุงูุฉ ุงูุณุทุฑ ุงูุฃูู ููุง:

```py
downsampled_dataset = downsampled_dataset.remove_columns(["word_ids"])
eval_dataset = downsampled_dataset["test"].map(
insert_random_mask,
batched=True,
remove_columns=downsampled_dataset["test"].column_names,
)
eval_dataset = eval_dataset.rename_columns(
{
"masked_input_ids": "input_ids",
"masked_attention_mask": "attention_mask",
"masked_labels": "labels",
}
)
```

ูููููุง ุจุนุฏ ุฐูู ุฅุนุฏุงุฏ ูุญููุงุช ุงูุจูุงูุงุช ูุงููุนุชุงุฏุ ูููููุง ุณูุณุชุฎุฏู `default_data_collator` ูู ๐ค Transformers ููุฌููุนุฉ ุงูุชูููู:

```python
from torch.utils.data import DataLoader
from transformers import default_data_collator

batch_size = 64
train_dataloader = DataLoader(
downsampled_dataset["train"],
shuffle=True,
batch_size=batch_size,
collate_fn=data_collator,
)
eval_dataloader = DataLoader(
eval_dataset, batch_size=batch_size, collate_fn=default_data_collator
)
```

ูู ููุงุ ูุชุจุน ุงูุฎุทูุงุช ุงูููุงุณูุฉ ูุน ๐ค Accelerate. ุฃูู ุฃูุฑ ูู ุงูุนูู ูู ุชุญููู ุฅุตุฏุงุฑ ุฌุฏูุฏ ูู ุงููููุฐุฌ ุงูููุฏุฑุจ ูุณุจููุง:

```
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)
```

ุจุนุฏ ุฐููุ ูุญุชุงุฌ ุฅูู ุชุญุฏูุฏ ุงููุญุณูุ ุณูุณุชุฎุฏู `AdamW` ุงูููุงุณู:

```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

ูุน ูุฐู ุงูุฃุดูุงุกุ ูููููุง ุงูุขู ุฅุนุฏุงุฏ ูู ุดูุก ููุชุฏุฑูุจ ุจุงุณุชุฎุฏุงู ูุงุฆู `Accelerator`:

```python
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
model, optimizer, train_dataloader, eval_dataloader
)
```

ุงูุขู ุจุนุฏ ุฃู ุชู ุชูููู ูููุฐุฌูุง ููุญุณููุง ููุญููุงุช ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุงุ ูููููุง ุชุญุฏูุฏ ุฌุฏูู ุงูุชุนูู ููุง ููู:

```python
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
"linear",
optimizer=optimizer,
num_warmup_steps=0,
num_training_steps=num_training_steps,
)
```

ููุงู ุดูุก ูุงุญุฏ ููุท ูุฌุจ ูุนูู ูุจู ุงูุชุฏุฑูุจ: ุฅูุดุงุก ูุณุชูุฏุน ูููุฐุฌ ุนูู Hub Hugging Face! ูููููุง ุงุณุชุฎุฏุงู ููุชุจุฉ ๐ค Hub ูุฅูุดุงุก ุงูุงุณู ุงููุงูู ููุณุชูุฏุนูุง ุฃููุงู:

```python
from huggingface_hub import get_full_repo_name

model_name = "distilbert-base-uncased-finetuned-imdb-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'lewtun/distilbert-base-uncased-finetuned-imdb-accelerate'
```

ุซู ูู ุจุฅูุดุงุก ูููุณุฎ ุงููุณุชูุฏุน ุจุงุณุชุฎุฏุงู ูุฆุฉ `Repository` ูู ๐ค Hub:

```python
from huggingface_hub import Repository

output_dir = model_name
repo = Repository(output_dir, clone_from=repo_name)
```

ุจุนุฏ ุงูููุงู ุจุฐููุ ูู ุงูุณูู ูุชุงุจุฉ ุญููุฉ ุงูุชุฏุฑูุจ ูุงูุชูููู ุงููุงููุฉ:

```python
from tqdm.auto import tqdm
import torch
import math

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
# Training
model.train()
for batch in train_dataloader:
outputs = model(**batch)
loss = outputs.loss
accelerator.backward(loss)

optimizer.step()
lr_scheduler.step()
optimizer.zero_grad()
progress_bar.update(1)

# Evaluation
model.eval()
losses = []
for step, batch in enumerate(eval_dataloader):
with torch.no_grad():
outputs = model(**batch)

loss = outputs.loss
losses.append(accelerator.gather(loss.repeat(batch_size)))

losses = torch.cat(losses)
losses = losses[: len(eval_dataset)]
try:
perplexity = math.exp(torch.mean(losses))
except OverflowError:
perplexity = float("inf")

print(f">>> Epoch {epoch}: Perplexity: {perplexity}")

# Save and upload
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
if accelerator.is_main_process:
tokenizer.save_pretrained(output_dir)
repo.push_to_hub(
commit_message=f"Training in progress epoch {epoch}", blocking=False
)
```

```python out
>>> Epoch 0: Perplexity: 11.397545307900472
>>> Epoch 1: Perplexity: 10.904909330983092
>>> Epoch 2: Perplexity: 10.729503505340409
```

ุฑุงุฆุนุ ููุฏ ุชูููุง ูู ุชูููู ุงูุญูุฑุฉ ูุน ูู ุญูุจุฉ ูุถูุงู ุฃู ุชููู ุนูููุงุช ุงูุชุฏุฑูุจ ุงููุชุนุฏุฏุฉ ูุงุจูุฉ ููุชูุฑุงุฑ!

## ุงุณุชุฎุฏุงู ูููุฐุฌูุง ุงููุถุจูุท ุงูุฏููู

ููููู ุงูุชูุงุนู ูุน ูููุฐุฌู ุงููุถุจูุท ุงูุฏููู ุฅูุง ุนู ุทุฑูู ุงุณุชุฎุฏุงู ุฃุฏุงุชู ุนูู ุงููุญูุฑ ุฃู ูุญูููุง ุจุงุณุชุฎุฏุงู `pipeline` ูู ๐ค Transformers. ุฏุนููุง ูุณุชุฎุฏู ุงูุฃุฎูุฑ ูุชูุฒูู ูููุฐุฌูุง ุจุงุณุชุฎุฏุงู ุฎุท ุฃูุงุจูุจ `fill-mask`:

```python
from transformers import pipeline

mask_filler = pipeline(
"fill-mask", model="huggingface-course/distilbert-base-uncased-finetuned-imdb"
)
```

ุจุนุฏ ุฐููุ ูููููุง ุฅุทุนุงู ุฎุท ุงูุฃูุงุจูุจ ูุต ุงูุนููุฉ ุงูุฎุงุต ุจูุง "ูุฐุง ูููู [MASK]" ูุฑุคูุฉ ุฃูุถู 5 ุชูุจุคุงุช:

```python
preds = mask_filler(text)

for pred in preds:
print(f">>> {pred['sequence']}")
```

```python out
'>>> this is a great movie.'
'>>> this is a great film.'
'>>> this is a great story.'
'>>> this is a great movies.'
'>>> this is a great character.'
```

ุฑุงุฆุน - ููุฏ ูุงู ูููุฐุฌูุง ุจูุถูุญ ุจุชูููู ุฃูุฒุงูู ููุชูุจุค ุจุงููููุงุช ุงููุฑุชุจุทุฉ ุจุดูู ุฃูุจุฑ ุจุงูุฃููุงู!

ูุฐุง ูุฎุชุชู ุชุฌุฑุจุชูุง ุงูุฃููู ูุน ุชุฏุฑูุจ ูููุฐุฌ ุงููุบุฉ. ูู ุงููุณู 6ุ ุณุชุชุนูู ููููุฉ ุชุฏุฑูุจ ูููุฐุฌ ุชูููุฏู ูุซู GPT-2 ูู ุงูุตูุฑุ ุชูุฌู ุฅูู ููุงู ุฅุฐุง ููุช ุชุฑุบุจ ูู ูุนุฑูุฉ ููููุฉ ุชุฏุฑูุจ ูููุฐุฌ ุงููุญูู ุงูุฎุงุต ุจู!

โ๏ธ **ุฌุฑุจูุง!** ูููุงุณ ููุงุฆุฏ ุชูููู ุงููุฌุงูุ ูู ุจุถุจุท ุฏููู ููุตูู ุนูู ููุตูุงุช IMDb ููู ูู ููุงุท ุงูุชุญูู DistilBERT ุงูููุฏุฑุจุฉ ูุณุจููุง ูุงููุถุจูุทุฉ ุงูุฏููู. ุฅุฐุง ููุช ุจุญุงุฌุฉ ุฅูู ุชุฐููุฑ ุจุชุตููู ุงููุตุ ูุฑุงุฌุน ุงููุตู 3.