بالتأكيد، سأبدأ الترجمة من التعليق التالي: 

## جمع البيانات 
يتوفر كود Python بوفرة من مستودعات الكود مثل GitHub، والتي يمكننا استخدامها لإنشاء مجموعة بيانات عن طريق البحث في كل مستودع Python. كان هذا هو النهج المتبع في كتاب "Transformers" لتعليم نموذج GPT-2 كبير. باستخدام تفريغ GitHub بحجم حوالي 180 جيجابايت يحتوي على حوالي 20 مليون ملف Python يسمى "codeparrot"، قام المؤلفون ببناء مجموعة بيانات قاموا بعد ذلك بمشاركتها على Hugging Face Hub. 

ومع ذلك، فإن التدريب على المجموعة الكاملة يستغرق وقتًا طويلاً ويستهلك الكثير من موارد الحوسبة، ولا نحتاج سوى إلى جزء من المجموعة المعنية بمجموعة Python للعلوم. لذلك، دعنا نبدأ بتصفية مجموعة بيانات "codeparrot" للحصول على جميع الملفات التي تتضمن أيًا من المكتبات الموجودة في هذه المجموعة. نظرًا لحجم مجموعة البيانات، نريد تجنب تنزيلها؛ بدلاً من ذلك، سنستخدم ميزة البث لتصفية البيانات أثناء التنقل. لمساعدتنا في تصفية عينات الكود باستخدام المكتبات التي ذكرناها سابقًا، سنستخدم الدالة التالية: 

يعد هذا مثالًا رائعًا على كيفية الاستفادة من المحتوى المتاح بحرية على الويب لإنشاء مجموعات بيانات مخصصة لتدريب نماذج اللغة.
## إعداد مجموعة البيانات 

ستكون الخطوة الأولى هي توحيد بياناتنا، بحيث يمكننا استخدامها للتدريب. نظرًا لأن هدفنا هو استكمال استدعاءات الدوال القصيرة بشكل أساسي، يمكننا الحفاظ على حجم السياق صغيرًا نسبيًا. تتمثل ميزة ذلك في أنه يمكننا تدريب النموذج بشكل أسرع بكثير، كما أنه يتطلب ذاكرة أقل بكثير. إذا كان من المهم لتطبيقك أن يكون له سياق أكبر (على سبيل المثال، إذا كنت تريد أن يقوم النموذج بكتابة اختبارات الوحدة بناءً على ملف بتعريف الدالة)، فتأكد من زيادة هذا الرقم، ولكن ضع في اعتبارك أيضًا أن هذا يأتي مع بصمة ذاكرة GPU أكبر. الآن، دعونا نقوم بتثبيت حجم السياق عند 128 رمزًا، على عكس 1024 أو 2048 المستخدمة في GPT-2 أو GPT-3، على التوالي.

تحتوي معظم المستندات على أكثر من 128 رمزًا، لذا فإن اقتصاص المدخلات ببساطة إلى الطول الأقصى من شأنه أن يقضي على جزء كبير من مجموعة بياناتنا. بدلاً من ذلك، سنستخدم خيار `return_overflowing_tokens` لتوحيد الإدخال بالكامل وتقسيمه إلى عدة أجزاء، كما فعلنا في [الفصل 6](/course/chapter6/4). سنستخدم أيضًا خيار `return_length` لإرجاع طول كل جزء تم إنشاؤه تلقائيًا. غالبًا ما يكون الجزء الأخير أصغر من حجم السياق، وسنتخلص من هذه القطع لتجنب مشكلات الحشو؛ نحن لا نحتاجها حقًا لأن لدينا الكثير من البيانات على أي حال.

دعونا نرى بالضبط كيف يعمل هذا من خلال النظر في أول مثالين:

```py
from transformers import AutoTokenizer

context_length = 128
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
raw_datasets["train"][:2]["content"],
truncation=True,
max_length=context_length,
return_overflowing_tokens=True,
return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")
```

```python out
Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

يمكننا أن نرى أننا نحصل على 34 قطعة في المجموع من هذين المثالين. بالنظر إلى أطوال الأجزاء، يمكننا أن نرى أن الأجزاء الموجودة في نهايات كلا المستندين تحتوي على أقل من 128 رمزًا (117 و41، على التوالي). تمثل هذه نسبة صغيرة جدًا من إجمالي الأجزاء التي لدينا، لذا يمكننا التخلص منها بأمان. باستخدام حقل `overflow_to_sample_mapping`، يمكننا أيضًا إعادة بناء الأجزاء التي تنتمي إلى عينات الإدخال.

مع هذه العملية، نستخدم ميزة مفيدة لوظيفة `Dataset.map()` في مكتبة 🤗 Datasets، والتي تتمثل في أنها لا تتطلب خرائط واحد لواحد؛ كما رأينا في [القسم 3](/course/chapter7/3)، يمكننا إنشاء دفعات تحتوي على عدد أكبر أو أقل من العناصر من دفعة الإدخال. هذا مفيد عند إجراء عمليات مثل زيادة البيانات أو تصفية البيانات التي تغير عدد العناصر. في حالتنا، عند توحيد كل عنصر إلى أجزاء بحجم السياق المحدد، نقوم بإنشاء العديد من العينات من كل مستند. نحن بحاجة فقط إلى التأكد من حذف الأعمدة الموجودة، لأنها ذات حجم متعارض. إذا أردنا الاحتفاظ بها، فيمكننا تكرارها بشكل مناسب وإعادتها داخل مكالمة `Dataset.map()`

```py
def tokenize(element):
outputs = tokenizer(
element["content"],
truncation=True,
max_length=context_length,
return_overflowing_tokens=True,
return_length=True,
)
input_batch = []
for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
if length == context_length:
input_batch.append(input_ids)
return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets
```

```python out
DatasetDict({
train: Dataset({
features: ['input_ids'],
num_rows: 16702061
})
valid: Dataset({
features: ['input_ids'],
num_rows: 93164
})
})
```

الآن لدينا 16.7 مليون مثال يحتوي كل منها على 128 رمزًا، وهو ما يتوافق مع حوالي 2.1 مليار رمز في المجموع. للمقارنة، تم تدريب نماذج GPT-3 وCodex من OpenAI على 300 و100 مليار رمز على التوالي، حيث تم تهيئة نماذج Codex من نقاط تفتيش GPT-3. هدفنا في هذا القسم ليس منافسة هذه النماذج، والتي يمكنها إنشاء نصوص طويلة ومتماسكة، ولكن لإنشاء إصدار مصغر يوفر وظيفة استكمال تلقائي سريعة لعلماء البيانات.

الآن بعد أن أصبحت مجموعة البيانات جاهزة، دعونا نقوم بإعداد النموذج!

✏️ **جربه!** لم يكن التخلص من جميع الأجزاء التي تكون أصغر من حجم السياق مشكلة كبيرة هنا لأننا نستخدم نوافذ سياق صغيرة. مع زيادة حجم السياق (أو إذا كان لديك مجموعة من المستندات القصيرة)، فإن نسبة الأجزاء التي يتم التخلص منها ستزداد أيضًا. هناك طريقة أكثر كفاءة لإعداد البيانات تتمثل في ضم جميع العينات الموحدة في دفعة باستخدام رمز `eos_token_id` بينها، ثم إجراء التقسيم على التسلسلات المدمجة. كممارسة، عدِّل وظيفة `tokenize()` لاستخدام هذا النهج. لاحظ أنك تريد تعيين `truncation=False` وإزالة الحجج الأخرى من المحول البرمجي للحصول على التسلسل الكامل لمعرفات الرموز.
## تهيئة نموذج جديد

تتمثل خطوتنا الأولى في تهيئة نموذج GPT-2 جديد تمامًا. سنستخدم نفس التهيئة لنموذجنا كما في النموذج الصغير لـ GPT-2، لذا سنقوم بتحميل التهيئة المسبقة، والتأكد من أن حجم الرموز مماثل لحجم مفردات النموذج، وإرسال معرفات الرموز "bos" و"eos" (بداية ونهاية التسلسل):

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
"gpt2",
vocab_size=len(tokenizer),
n_ctx=context_length,
bos_token_id=tokenizer.bos_token_id,
eos_token_id=tokenizer.eos_token_id,
)
```

بهذه التهيئة، يمكننا تحميل نموذج جديد. لاحظ أن هذه هي المرة الأولى التي لا نستخدم فيها الدالة `from_pretrained()`، لأننا نقوم بتهيئة نموذج بأنفسنا:

```py
model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")
```

```python out
GPT-2 size: 124.2M parameters
```

{:else}

```py
from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
"gpt2",
vocab_size=len(tokenizer),
n_ctx=context_length,
bos_token_id=tokenizer.bos_token_id,
eos_token_id=tokenizer.eos_token_id,
)
```

بهذه التهيئة، يمكننا تحميل نموذج جديد. لاحظ أن هذه هي المرة الأولى التي لا نستخدم فيها الدالة `from_pretrained()`، لأننا نقوم بتهيئة نموذج بأنفسنا:

```py
model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  # Builds the model
model.summary()
```

```python out
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
transformer (TFGPT2MainLayer multiple                  124242432
=================================================================
Total params: 124,242,432
Trainable params: 124,242,432
Non-trainable params: 0
_________________________________________________________________
```

{/if}

يحتوي نموذجنا على 124 مليون معلمة يجب ضبطها. قبل أن نتمكن من بدء التدريب، نحتاج إلى إعداد جامع بيانات يقوم بإنشاء الدفعات. يمكننا استخدام جامع البيانات `DataCollatorForLanguageModeling`، المصمم خصيصًا لوضع نماذج اللغة (كما يوحي الاسم بشكل خفي). بالإضافة إلى تكديس الدفعات ووساداتها، فإنه يقوم أيضًا بإنشاء تسميات نموذج اللغة - في وضع اللغة السببي، تعمل الإدخالات كعلامات أيضًا (متحولة بمقدار عنصر واحد)، وينشئ جامع البيانات هذا أثناء التدريب حتى لا نحتاج إلى تكرار `input_ids`.

لاحظ أن `DataCollatorForLanguageModeling` يدعم كل من وضع نمذجة اللغة المقنعة (MLM) ووضع نمذجة اللغة السببية (CLM). بشكل افتراضي، فإنه يعد البيانات لـ MLM، ولكن يمكننا التبديل إلى CLM عن طريق تعيين وسيط `mlm=False`:

{#if fw === 'pt'}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
```

{:else}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")
```

{/if}

دعونا نلقي نظرة على مثال:

```py
out = data_collator([tokenized_datasets["train"][i] for i in range(5)])
for key in out:
print(f"{key} shape: {out[key].shape}")
```

{#if fw === 'pt'}

```python out
input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])
```

{:else}

```python out
input_ids shape: (5, 128)
attention_mask shape: (5, 128)
labels shape: (5, 128)
```

{/if}

يمكننا أن نرى أن الأمثلة قد تم تكديسها وأن جميع المصفوفات لها نفس الشكل.

{#if fw === 'tf'}

الآن يمكننا استخدام طريقة `prepare_tf_dataset()` لتحويل مجموعات البيانات الخاصة بنا إلى مجموعات بيانات TensorFlow باستخدام جامع البيانات الذي أنشأناه أعلاه:

```python
tf_train_dataset = model.prepare_tf_dataset(
tokenized_datasets["train"],
collate_fn=data_collator,
shuffle=True,
batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
tokenized_datasets["valid"],
collate_fn=data_collator,
shuffle=False,
batch_size=32,
)
```

{/if}

<Tip warning={true}>

⚠️ يحدث تحويل الإدخالات والتسميات لمواءمتها داخل النموذج، لذا فإن جامع البيانات يقوم فقط بنسخ الإدخالات لإنشاء التسميات.

</Tip>

الآن لدينا كل شيء في مكانه لتدريب نموذجنا بالفعل - لم يكن الأمر بهذا القدر من العمل بعد كل شيء! قبل أن نبدأ التدريب، يجب أن نقوم بتسجيل الدخول إلى Hugging Face. إذا كنت تعمل في دفتر ملاحظات، فيمكنك القيام بذلك باستخدام دالة المساعدة هذه:

```python
from huggingface_hub import notebook_login

notebook_login()
```

سيؤدي هذا إلى عرض مربع حوار يمكنك من خلاله إدخال بيانات اعتماد تسجيل الدخول إلى Hugging Face.

إذا كنت لا تعمل في دفتر الملاحظات، فما عليك سوى كتابة السطر التالي في المحطة الطرفية الخاصة بك:

```bash
huggingface-cli login
```

{#if fw === 'pt'}

كل ما تبقى هو تكوين الحجج التدريب وتشغيل `Trainer`. سنستخدم جدول تعلم ذاتي بمعدل تعلم دوري مع بعض الاحماء وحجم دفعة فعال يبلغ 256 (`per_device_train_batch_size` * `gradient_accumulation_steps`). يستخدم تراكم التدرجات عندما لا تتناسب دفعة واحدة مع الذاكرة، ويبني تدريجيًا التدرج من خلال عدة تمريرات للأمام والخلف. سنرى هذا في العمل عند إنشاء حلقة التدريب باستخدام 🤗 Accelerate.

```py
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
output_dir="codeparrot-ds",
per_device_train_batch_size=32,
per_device_eval_batch_size=32,
evaluation_strategy="steps",
eval_steps=5_000,
logging_steps=5_000,
gradient_accumulation_steps=8,
num_train_epochs=1,
weight_decay=0.1,
warmup_steps=1_000,
lr_scheduler_type="cosine",
learning_rate=5e-4,
save_steps=5_000,
fp16=True,
push_to_hub=True,
)

trainer = Trainer(
model=model,
tokenizer=tokenizer,
args=args,
data_collator=data_collator,
train_dataset=tokenized_datasets["train"],
eval_dataset=tokenized_datasets["valid"],
)
```

الآن يمكننا ببساطة بدء تشغيل `Trainer` وانتظر حتى ينتهي التدريب. اعتمادًا على ما إذا كنت تشغله على مجموعة التدريب الكاملة أو جزء منها، سيستغرق هذا 20 أو ساعتين على التوالي، لذا احصل على بعض القهوة وكتاب جيد للقراءة!

```py
trainer.train()
```

بعد اكتمال التدريب، يمكننا دفع النموذج والمحلل إلى المركز:

```py
trainer.push_to_hub()
```

{:else}

كل ما تبقى هو تكوين فرط المعلمات التدريب واستدعاء `compile()` و`fit()`. سنستخدم جدول تعلم ذاتي بمعدل تعلم دوري مع بعض الاحماء لتحسين استقرار التدريب:

```py
from transformers import create_optimizer
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
init_lr=5e-5,
num_warmup_steps=1_000,
num_train_steps=num_train_steps,
weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

الآن يمكننا ببساطة استدعاء `model.fit()` وانتظر حتى ينتهي التدريب. اعتمادًا على ما إذا كنت تشغله على مجموعة التدريب الكاملة أو جزء منها، سيستغرق هذا 20 أو ساعتين على التوالي، لذا احصل على بعض القهوة وكتاب جيد للقراءة! بعد اكتمال التدريب، يمكننا دفع النموذج والمحلل إلى المركز:

```py
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="codeparrot-ds", tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```

{/if}

<Tip>

✏️ **جربه!** استغرق الأمر منا حوالي 30 سطرًا من التعليمات البرمجية بالإضافة إلى `TrainingArguments` للانتقال من النصوص الخام إلى تدريب GPT-2. جربه باستخدام مجموعة البيانات الخاصة بك وشاهد ما إذا كان بإمكانك الحصول على نتائج جيدة!

</Tip>

<Tip>

{#if fw === 'pt'}

💡 إذا كان لديك إمكانية الوصول إلى جهاز به وحدات معالجة رسومات متعددة، فحاول تشغيل التعليمات البرمجية هناك. يقوم `Trainer` بإدارة أجهزة متعددة تلقائيًا، ويمكن أن يسرع التدريب بشكل كبير.

{:else}

💡 إذا كان لديك إمكانية الوصول إلى جهاز به وحدات معالجة رسومات متعددة، فيمكنك تجربة استخدام سياق `MirroredStrategy` للتسريع بشكل كبير من التدريب. ستحتاج إلى إنشاء كائن `tf.distribute.MirroredStrategy`، والتأكد من تشغيل أي طرق `to_tf_dataset()` أو `prepare_tf_dataset()` بالإضافة إلى إنشاء النموذج واستدعاء `fit()` جميعًا في سياق `scope()` الخاص به. يمكنك الاطلاع على الوثائق المتعلقة بهذا الأمر [هنا](https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit).

{/if}

</Tip>

## توليد التعليمات البرمجية مع خط أنابيب

الآن هي لحظة الحقيقة: دعونا نرى مدى نجاح النموذج المدرب بالفعل! يمكننا أن نرى في السجلات أن الخسارة انخفضت بثبات، ولكن لوضع النموذج على المحك، دعونا نرى مدى نجاحه في بعض المطالبات. للقيام بذلك، سنقوم بتغليف النموذج في خط أنابيب توليد النص، وسنضعه على وحدة معالجة الرسومات للحصول على أجيال سريعة إذا كان هناك وحدة معالجة رسومات متاحة:

{#if fw === 'pt'}

```py
import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
"text-generation", model="huggingface-course/codeparrot-ds", device=device
)
```

{:else}

```py
from transformers import pipeline

course_model = TFGPT2LMHeadModel.from_pretrained("huggingface-course/codeparrot-ds")
course_tokenizer = AutoTokenizer.from_pretrained("huggingface-course/codeparrot-ds")
pipe = pipeline(
"text-generation", model=course_model, tokenizer=course_tokenizer, device=0
)
```

{/if}

دعونا نبدأ بالمهمة البسيطة المتمثلة في إنشاء مخطط تشتت:

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create scatter plot with x, y
plt.scatter(x, y)

# create scatter
```

تبدو النتيجة صحيحة. هل يعمل أيضًا لعملية `pandas`؟ دعونا نرى ما إذا كان بإمكاننا إنشاء `DataFrame` من صفيفين:

```py
txt = """\
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# create some data
x = np.random.randn(100)
y = np.random.randn(100)

# create dataframe from x and y
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for
```

رائع، هذا هو الجواب الصحيح - على الرغم من أنه بعد ذلك يقوم بإدراج عمود "x" مرة أخرى. نظرًا لأن عدد الرموز المولدة محدود، يتم قطع حلقة "for" التالية. دعونا نرى ما إذا كان بإمكاننا القيام بشيء أكثر تعقيدًا وجعل النموذج يساعدنا في استخدام عملية `groupby`:

```py
txt = """\
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# dataframe with profession, income and name
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# calculate the mean income per profession
profession = df.groupby(['profession']).mean()

# compute the
```

ليس سيئًا؛ هذه هي الطريقة الصحيحة للقيام بذلك. أخيرًا، دعونا نرى ما إذا كان بإمكاننا أيضًا استخدامه لـ `scikit-learn` وإعداد نموذج Random Forest:

```py
txt = """
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# import random forest regressor from scikit-learn
from sklearn.ensemble import RandomForestRegressor

# fit random forest model with 300 estimators on X, y:
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf
```

{#if fw === 'tf'}

من خلال النظر في هذه الأمثلة القليلة، يبدو أن النموذج قد تعلم بعضًا من بناء جملة مجموعة أدوات علم البيانات في Python. بالطبع، سيتعين علينا تقييم النموذج بشكل أكثر شمولاً قبل نشره في العالم الحقيقي، ولكن هذا نموذج أولي مثير للإعجاب.

{:else}

من خلال النظر في هذه الأمثلة القليلة، يبدو أن النموذج قد تعلم بعضًا من بناء جملة مجموعة أدوات علم البيانات في Python (بالطبع، س
## التدريب باستخدام 🤗 Accelerate

لقد رأينا كيف نقوم بتدريب نموذج باستخدام `Trainer`، والذي يسمح ببعض التخصيص. ومع ذلك، في بعض الأحيان نريد التحكم الكامل في حلقة التدريب، أو نريد إجراء بعض التغييرات الغريبة. في هذه الحالة، يعد 🤗 Accelerate خيارًا رائعًا، وفي هذا القسم، سنمر عبر الخطوات اللازمة لاستخدامه لتدريب نموذجنا. لجعل الأمور أكثر إثارة للاهتمام، سنضيف أيضًا لفة إلى حلقة التدريب.

<Youtube id="Hm8_PgVTFuc"/>

نظرًا لأننا مهتمون بشكل أساسي بالاكتمال التلقائي المعقول لمكتبات علوم البيانات، فمن المنطقي إعطاء وزن أكبر لعينات التدريب التي تستخدم هذه المكتبات بشكل أكبر. يمكننا التعرف بسهولة على هذه الأمثلة من خلال استخدام كلمات رئيسية مثل `plt` و`pd` و`sk` و`fit` و`predict`، والتي تعد أكثر أسماء الاستيراد شيوعًا لـ `matplotlib.pyplot` و`pandas` و`sklearn`، بالإضافة إلى نمط fit/predict الأخير. إذا تم تمثيل كل منها كرموز مميزة واحدة، فيمكننا التحقق بسهولة مما إذا كانت تحدث في تسلسل الإدخال. يمكن أن يكون للرموز المميزة بادئة مسافة بيضاء، لذا فسنتحقق أيضًا من هذه الإصدارات في قاموس مفردات المحلل اللغوي. للتحقق من أنها تعمل، سنضيف رمزًا مميزًا واحدًا للاختبار يجب تقسيمه إلى عدة رموز مميزة:

```py
keytoken_ids = []
for keyword in [
"plt",
"pd",
"sk",
"fit",
"predict",
" plt",
" pd",
" sk",
" fit",
" predict",
"testtest",
]:
ids = tokenizer([keyword]).input_ids[0]
if len(ids) == 1:
keytoken_ids.append(ids[0])
else:
print(f"Keyword has not single token: {keyword}")
```

```python out
'Keyword has not single token: testtest'
```

رائع، يبدو أن هذا يعمل بشكل جيد! الآن يمكننا كتابة دالة خسارة مخصصة تأخذ تسلسل الإدخال، والاحتمالات، والرموز المميزة الرئيسية التي حددناها كمدخلات. أولاً، نحتاج إلى محاذاة الاحتمالات والمدخلات: يشكل تسلسل الإدخال المنزاح إلى اليمين بمقدار واحد التسميات، نظرًا لأن الرمز المميز التالي هو التسمية للرمز المميز الحالي. يمكننا تحقيق ذلك عن طريق بدء التسميات من الرمز المميز الثاني لتسلسل الإدخال، نظرًا لأن النموذج لا يقوم بتوقع للرمز المميز الأول على أي حال. ثم نقطع الاحتمال الأخير، حيث لا توجد تسمية للرمز المميز الذي يتبع تسلسل الإدخال الكامل. بهذه الطريقة، يمكننا حساب الخسارة لكل عينة وحساب تكرار جميع الكلمات الرئيسية في كل عينة. أخيرًا، نحسب المتوسط المرجح لجميع العينات باستخدام التكرارات كأوزان. نظرًا لأننا لا نريد التخلص من جميع العينات التي لا تحتوي على كلمات رئيسية، فنضيف 1 إلى الأوزان:

```py
from torch.nn import CrossEntropyLoss
import torch


def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
# Shift so that tokens < n predict n
shift_labels = inputs[..., 1:].contiguous()
shift_logits = logits[..., :-1, :].contiguous()
# Calculate per-token loss
loss_fct = CrossEntropyLoss(reduce=False)
loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
# Resize and average loss per sample
loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)
# Calculate and scale weighting
weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(
axis=[0, 2]
)
weights = alpha * (1.0 + weights)
# Calculate weighted average
weighted_loss = (loss_per_sample * weights).mean()
return weighted_loss
```

قبل أن نتمكن من بدء التدريب باستخدام دالة الخسارة الجديدة الرائعة هذه، يلزم إعداد بعض الأمور:

- نحتاج إلى محملات البيانات لتحميل البيانات في دفعات.
- نحتاج إلى إعداد معلمات انحلال الوزن.
- نريد التقييم من وقت لآخر، لذا فمن المنطقي لف وظيفة التقييم في دالة.

لنبدأ بمحملات البيانات. كل ما نحتاجه هو تعيين تنسيق مجموعة البيانات إلى `"torch"`، ثم يمكننا تمريره إلى PyTorch `DataLoader` بحجم الدفعة المناسب:

```py
from torch.utils.data.dataloader import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(tokenized_datasets["train"], batch_size=32, shuffle=True)
eval_dataloader = DataLoader(tokenized_datasets["valid"], batch_size=32)
```

بعد ذلك، نقوم بتجمع المعلمات بحيث يعرف المحسن أيها سيحصل على انحلال وزن إضافي. عادة، يتم إعفاء جميع مصطلحات الانحياز وLayerNorm weights من هذا؛ إليك كيفية القيام بذلك:

```py
weight_decay = 0.1


def get_grouped_params(model, no_decay=["bias", "LayerNorm.weight"]):
params_with_wd, params_without_wd = [], []
for n, p in model.named_parameters():
if any(nd in n for nd in no_decay):
params_without_wd.append(p)
else:
params_with_wd.append(p)
return [
{"params": params_with_wd, "weight_decay": weight_decay},
{"params": params_without_wd, "weight_decay": 0.0},
]
```

نظرًا لأننا نريد تقييم النموذج بانتظام على مجموعة التحقق أثناء التدريب، دعونا نكتب دالة لذلك أيضًا. فهو يقوم ببساطة بتشغيل محمل بيانات التقييم وجمع جميع الخسائر عبر العمليات:

```py
def evaluate():
model.eval()
losses = []
for step, batch in enumerate(eval_dataloader):
with torch.no_grad():
outputs = model(batch["input_ids"], labels=batch["input_ids"])

losses.append(accelerator.gather(outputs.loss))
loss = torch.mean(torch.cat(losses))
try:
perplexity = torch.exp(loss)
except OverflowError:
perplexity = float("inf")
return loss.item(), perplexity.item()
```

مع دالة `evaluate()`، يمكننا الإبلاغ عن الخسارة و [perplexity](/course/chapter7/3) في فترات منتظمة. بعد ذلك، نعيد تعريف نموذجنا للتأكد من أننا نتدرب من الصفر مرة أخرى:

```py
model = GPT2LMHeadModel(config)
```

بعد ذلك، يمكننا تعريف محسننا، باستخدام الدالة من قبل لتقسيم المعلمات لانحلال الوزن:

```py
from torch.optim import AdamW

optimizer = AdamW(get_grouped_params(model), lr=5e-4)
```

الآن دعونا نعد النموذج والمحسن ومحملات البيانات حتى نتمكن من بدء التدريب:

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>
🚨 إذا كنت تتدرب على TPU، فستحتاج إلى نقل كل التعليمات البرمجية بدءًا من الخلية أعلاه إلى دالة تدريب مخصصة. راجع [الفصل 3](/course/chapter3) لمزيد من التفاصيل.
</Tip>

الآن بعد أن أرسلنا `train_dataloader` إلى `accelerator.prepare()`، يمكننا استخدام طوله لحساب عدد خطوات التدريب. تذكر أنه يجب علينا دائمًا القيام بذلك بعد إعداد محمل البيانات، حيث ستغير هذه الطريقة طوله. نستخدم جدولًا خطيًا كلاسيكيًا من معدل التعلم إلى 0:

```py
from transformers import get_scheduler

num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
name="linear",
optimizer=optimizer,
num_warmup_steps=1_000,
num_training_steps=num_training_steps,
)
```

أخيرًا، لدفع نموذجنا إلى Hub، سيتعين علينا إنشاء كائن `Repository` في مجلد عمل. قم بتسجيل الدخول أولاً إلى Hub Hugging Face، إذا لم تكن قد سجلت الدخول بالفعل. سنحدد اسم المستودع من معرف النموذج الذي نريد منحه لنموذجنا (لا تتردد في استبدال `repo_name` بخيارك الخاص؛ كل ما يحتاجه هو احتواء اسم المستخدم الخاص بك، وهو ما تفعله وظيفة `get_full_repo_name()`):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/codeparrot-ds-accelerate'
```

بعد ذلك، يمكننا استنساخ هذا المستودع في مجلد محلي. إذا كان موجودًا بالفعل، فيجب أن يكون هذا المجلد المحلي مستنسخًا موجودًا للمستودع الذي نعمل معه:

```py
output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

الآن يمكننا تحميل أي شيء نقوم بحفظه في `output_dir` عن طريق استدعاء طريقة `repo.push_to_hub()`. سيساعدنا هذا في تحميل النماذج المتوسطة في نهاية كل فترة.

قبل التدريب، دعونا نجري اختبارًا سريعًا لمعرفة ما إذا كانت دالة التقييم تعمل بشكل صحيح:

```py
evaluate()
```

```python out
(10.934126853942871, 56057.14453125)
```

تلك قيم عالية جدًا للخسارة والارتباك، ولكن ليس من المستغرب أننا لم نقم بتدريب النموذج بعد. بهذا، أصبح كل شيء جاهزًا لكتابة الجزء الأساسي من نص البرنامج النصي للتدريب: حلقة التدريب. في حلقة التدريب، نقوم بالتعيين على محمل البيانات ونمرر الدفعات إلى النموذج. باستخدام الاحتمالات، يمكننا بعد ذلك تقييم دالة الخسارة المخصصة لدينا. نقوم بضبط الخسارة عن طريق عدد خطوات تجميع التدرجات حتى لا نقوم بإنشاء خسائر أكبر عند تجميع المزيد من الخطوات. قبل التحسين، نقوم أيضًا بقص التدرجات للتقارب الأفضل. وأخيرًا، نقوم بتقييم النموذج على مجموعة التقييم باستخدام دالة `evaluate()` الجديدة كل بضع خطوات:

```py
from tqdm.notebook import tqdm

gradient_accumulation_steps = 8
eval_steps = 5_000

model.train()
completed_steps = 0
for epoch in range(num_train_epochs):
for step, batch in tqdm(
enumerate(train_dataloader, start=1), total=num_training_steps
):
logits = model(batch["input_ids"]).logits
loss = keytoken_weighted_loss(batch["input_ids"], logits, keytoken_ids)
if step % 100 == 0:
accelerator.print(
{
"samples": step * samples_per_step,
"steps": completed_steps,
"loss/train": loss.item() * gradient_accumulation_steps,
}
)
loss = loss / gradient_accumulation_steps
accelerator.backward(loss)
if step % gradient_accumulation_steps == 0:
accelerator.clip_grad_norm_(model.parameters(), 1.0)
optimizer.step()
lr_scheduler.step()
optimizer.zero_grad()
completed_steps += 1
if (step % (eval_steps * gradient_accumulation_steps)) == 0:
eval_loss, perplexity = evaluate()
accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
model.train()
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
if accelerator.is_main_process:
tokenizer.save_pretrained(output_dir)
repo.push_to_hub(
commit_message=f"Training in progress step {step}", blocking=False
)
```

وهذا كل شيء - لديك الآن حلقة تدريب مخصصة خاصة بك للنماذج اللغوية السببية مثل GPT-2 والتي يمكنك تخصيصها أكثر لتلبية احتياجاتك.

<Tip>
✏️ **جربه!** إما إنشاء دالة خسارة مخصصة خاصة بك مصممة لحالتك الاستخدام، أو إضافة خطوة مخصصة أخرى إلى حلقة التدريب.
</Tip>

<Tip>
✏️ **جربه!** عند تشغيل تجارب التدريب الطويلة، من الجيد تسجيل المقاييس المهمة باستخدام أدوات مثل TensorBoard أو Weights & Biases. أضف تسجيل الدخول المناسب إلى حلقة التدريب حتى تتمكن دائمًا من التحقق من كيفية سير التدريب.
</Tip>

{/if}