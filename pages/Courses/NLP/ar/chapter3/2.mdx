ูู ูุชู ุชุฑุฌูุฉ ุงููุต ุงูุจุฑูุฌู ูุงูุฑูุงุจุท ููููุง ูุชุนูููุงุชู.

# ูุนุงูุฌุฉ ุงูุจูุงูุงุช

ุงุณุชูุฑุงุฑูุง ูุน ุงููุซุงู ูู [ุงููุตู ุงูุณุงุจู](/course/chapter2)ุ ูููุง ููู ููููุฉ ุชุฏุฑูุจ ูุตูู ุชุณูุณู ุนูู ุฏูุนุฉ ูุงุญุฏุฉ ูู PyTorch:

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# ููุง ูู ุงูุญุงู ูู ูุจู
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
"ููุช ุฃูุชุธุฑ ุฏูุฑุฉ ูุบููุฌููุณ ุทูุงู ุญูุงุชู."
"ูุฐู ุงูุฏูุฑุฉ ุฑุงุฆุนุฉ!"
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# ูุฐุง ุฌุฏูุฏ
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```

ุงุณุชูุฑุงุฑูุง ูุน ุงููุซุงู ูู [ุงููุตู ุงูุณุงุจู](/course/chapter2)ุ ูููุง ููู ููููุฉ ุชุฏุฑูุจ ูุตูู ุชุณูุณู ุนูู ุฏูุนุฉ ูุงุญุฏุฉ ูู TensorFlow:

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# ููุง ูู ุงูุญุงู ูู ูุจู
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
"ููุช ุฃูุชุธุฑ ุฏูุฑุฉ ูุบููุฌููุณ ุทูุงู ุญูุงุชู."
"ูุฐู ุงูุฏูุฑุฉ ุฑุงุฆุนุฉ!"
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# ูุฐุง ุฌุฏูุฏ
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```

ุจุงูุทุจุนุ ูุฅู ุชุฏุฑูุจ ุงููููุฐุฌ ุนูู ุฌููุชูู ููุท ูู ูุคุฏู ุฅูู ูุชุงุฆุฌ ุฌูุฏุฉ ุฌุฏูุง. ููุญุตูู ุนูู ูุชุงุฆุฌ ุฃูุถูุ ุณุชุญุชุงุฌ ุฅูู ุฅุนุฏุงุฏ ูุฌููุนุฉ ุจูุงูุงุช ุฃูุจุฑ.

ูู ูุฐุง ุงููุณูุ ุณูุณุชุฎุฏู ููุซุงู ูุฌููุนุฉ ุจูุงูุงุช MRPC (Microsoft Research Paraphrase Corpus)ุ ุงูุชู ุชู ุชูุฏูููุง ูู [ูุฑูุฉ](https://www.aclweb.org/anthology/I05-5002.pdf) ุจูุงุณุทุฉ ููููุงู ุจ. ุฏููุงู ููุฑูุณ ุจุฑูููุช. ุชุชููู ูุฌููุนุฉ ุงูุจูุงูุงุช ูู 5,801 ุฒูุฌ ูู ุงูุฌููุ ูุน ุชุณููุฉ ุชุดูุฑ ุฅูู ูุง ุฅุฐุง ูุงูุช paraphrases ุฃู ูุง (ุฃูุ ุฅุฐุง ูุงู ููุง ุงูุฌููุชูู ุชุนููุงู ููุณ ุงูุดูุก). ููุฏ ุงุฎุชุฑูุงูุง ููุฐุง ุงููุตู ูุฃููุง ูุฌููุนุฉ ุจูุงูุงุช ุตุบูุฑุฉุ ูุฐุง ููู ุงูุณูู ุฅุฌุฑุงุก ุชุฌุงุฑุจ ุนูููุง.

### ุชุญููู ูุฌููุนุฉ ุจูุงูุงุช ูู Hub

ูุญุชูู Hub ุนูู ุฃูุซุฑ ูู ุงูููุงุฐุฌุ ููุง ุฃู ูุฏูู ูุฌููุนุงุช ุจูุงูุงุช ูุชุนุฏุฏุฉ ุจุงูุนุฏูุฏ ูู ุงููุบุงุช ุงููุฎุชููุฉ. ููููู ุชุตูุญ ูุฌููุนุงุช ุงูุจูุงูุงุช [ููุง](https://huggingface.co/datasets)ุ ูููุตู ุจุชุฌุฑุจุฉ ุชุญููู ููุนุงูุฌุฉ ูุฌููุนุฉ ุจูุงูุงุช ุฌุฏูุฏุฉ ุจูุฌุฑุฏ ุงูุงูุชูุงุก ูู ูุฐุง ุงููุณู (ุฑุงุฌุน ุงููุซุงุฆู ุงูุนุงูุฉ [ููุง](https://huggingface.co/docs/datasets/loading)). ูููู ุงูุขูุ ุฏุนููุง ูุฑูุฒ ุนูู ูุฌููุนุฉ ุจูุงูุงุช MRPC! ูุฐู ูู ูุงุญุฏุฉ ูู 10 ูุฌููุนุงุช ุจูุงูุงุช ุชุชููู ูู [GLUE benchmark](https://gluebenchmark.com/)ุ ูุงูุชู ูู ูุนูุงุฑ ุฃูุงุฏููู ูุณุชุฎุฏู ูููุงุณ ุฃุฏุงุก ููุงุฐุฌ ML ุนุจุฑ 10 ููุงู ุชุตููู ุงููุต ุงููุฎุชููุฉ.

ุชููุฑ ููุชุจุฉ Datasets ๐ค ุฃูุฑูุง ุจุณูุทูุง ุฌุฏูุง ูุชูุฒูู ูุฌููุนุฉ ุจูุงูุงุช ูุชุฎุฒูููุง ูุคูุชูุง ุนูู Hub. ูููููุง ุชูุฒูู ูุฌููุนุฉ ุจูุงูุงุช MRPC ุนูู ุงููุญู ุงูุชุงูู:

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python out
DatasetDict({
train: Dataset({
features: ['sentence1', 'sentence2', 'label', 'idx'],
num_rows: 3668
})
validation: Dataset({
features: ['sentence1', 'sentence2', 'label', 'idx'],
num_rows: 408
})
test: Dataset({
features: ['sentence1', 'sentence2', 'label', 'idx'],
num_rows: 1725
})
})
```

ููุง ุชุฑููุ ูุญุตู ุนูู ูุงุฆู `DatasetDict` ูุญุชูู ุนูู ูุฌููุนุฉ ุงูุชุฏุฑูุจ ูุงูุชุญูู ูุงูุงุฎุชุจุงุฑ. ูุญุชูู ูู ูููุง ุนูู ุนุฏุฉ ุฃุนูุฏุฉ (`sentence1`ุ `sentence2`ุ `label`ุ ู`idx`) ูุนุฏุฏ ูุชุบูุฑ ูู ุงูุตูููุ ููู ุนุฏุฏ ุงูุนูุงุตุฑ ูู ูู ูุฌููุนุฉ (ูุฐููุ ููุงู 3,668 ุฒูุฌูุง ูู ุงูุฌูู ูู ูุฌููุนุฉ ุงูุชุฏุฑูุจุ ู408 ูู ูุฌููุนุฉ ุงูุชุญููุ ู1,725 ูู ูุฌููุนุฉ ุงูุงุฎุชุจุงุฑ).

ูููู ูุฐุง ุงูุฃูุฑ ุจุชูุฒูู ูุฌููุนุฉ ุงูุจูุงูุงุช ูุชุฎุฒูููุง ูุคูุชูุงุ ุงูุชุฑุงุถููุง ูู *~/.cache/huggingface/datasets*. ุชุฐูุฑ ูู ุงููุตู 2 ุฃูู ููููู ุชุฎุตูุต ูุฌูุฏ ุงูุชุฎุฒูู ุงููุคูุช ุงูุฎุงุต ุจู ุนู ุทุฑูู ุชุนููู ูุชุบูุฑ ุงูุจูุฆุฉ `HF_HOME`.

ูููููุง ุงููุตูู ุฅูู ูู ุฒูุฌ ูู ุงูุฌูู ูู ูุงุฆู `raw_datasets` ุงูุฎุงุต ุจูุง ุนู ุทุฑูู ุงูููุฑุณุฉุ ูุซู ุงููุงููุณ:

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python out
{'idx': 0ุ
'label': 1ุ
'sentence1': 'ุงุชูู ุนูุฑูุฒู ุดููููุ ุงูุฐู ุฃุทูู ุนููู ุงุณู "ุงูุดุงูุฏ"ุ ุจุชุญุฑูู ุฃุฏูุชู ุนู ุนูุฏ.'
'sentence2': 'ูุจุงูุฅุดุงุฑุฉ ุฅููู ุจุงุณู "ุงูุดุงูุฏ" ููุทุ ุงุชูู ุนูุฑูุฒู ุดูููู ุจุชุญุฑูู ุฃุฏูุชู ุนู ุนูุฏ.'}
```

ูููููุง ุฃู ูุฑู ุฃู ุงูุชุณููุงุช ูู ุจุงููุนู ุฃุนุฏุงุฏ ุตุญูุญุฉุ ูุฐูู ูู ูุถุทุฑ ุฅูู ุฅุฌุฑุงุก ุฃู ูุนุงูุฌุฉ ูุณุจูุฉ ููุงู. ููุนุฑูุฉ ุงูุฑูู ุงูุตุญูุญ ุงูููุงุจู ููู ุชุณููุฉุ ูููููุง ูุญุต `ุงูููุฒุงุช` ูู `raw_train_dataset`. ุณูุนุฑููุง ูุฐุง ุจููุน ูู ุนููุฏ:

```py
raw_train_dataset.features
```

```python out
{'sentence1': Value(dtype='string', id=None)ุ
'sentence2': Value(dtype='string', id=None)ุ
'label': ClassLabel(num_classes=2ุ names=['not_equivalent'ุ 'equivalent']ุ names_file=Noneุ id=None)ุ
'idx': Value(dtype='int32'ุ id=None)}
```

ูู ุงูุฎูููุฉุ `label` ูู ูู ููุน `ClassLabel`ุ ููุชู ุชุฎุฒูู ุฎุฑูุทุฉ ุงูุฃุนุฏุงุฏ ุงูุตุญูุญุฉ ุฅูู ุงุณู ุงูุชุณููุฉ ูู ูุฌูุฏ *ุงูุฃุณูุงุก*. ููุงุจู `0` `not_equivalent`ุ ู`1` ููุงุจู `equivalent`.

โ๏ธ **ุฌุฑุจูุง!** ุงูุธุฑ ุฅูู ุงูุนูุตุฑ 15 ูู ูุฌููุนุฉ ุงูุชุฏุฑูุจ ูุงูุนูุตุฑ 87 ูู ูุฌููุนุฉ ุงูุชุญูู. ูุง ูู ุชุณููุงุชูุงุ
### ูุนุงูุฌุฉ ูุฌููุนุฉ ุงูุจูุงูุงุช ูุจู ุงูุชุฏุฑูุจ

ููุนุงูุฌุฉ ูุฌููุนุฉ ุงูุจูุงูุงุชุ ูุญุชุงุฌ ุฅูู ุชุญููู ุงููุต ุฅูู ุฃุฑูุงู ูููู ูููููุฐุฌ ููููุง. ููุง ุฑุฃูุช ูู [ุงููุตู ุงูุณุงุจู](/course/chapter2)ุ ูุชู ุฐูู ุจุงุณุชุฎุฏุงู ุฃุฏุงุฉ "ูุนุงูุฌุฉ ุงูุฑููุฒ". ูููููุง ุฅุฏุฎุงู ุฌููุฉ ูุงุญุฏุฉ ุฃู ูุงุฆูุฉ ูู ุงูุฌูู ูู ุฃุฏุงุฉ "ูุนุงูุฌุฉ ุงูุฑููุฒ"ุ ูุฐูู ูููููุง ูุจุงุดุฑุฉ ูุนุงูุฌุฉ ุฌููุน ุงูุฌูู ุงูุฃููู ูุฌููุน ุงูุฌูู ุงูุซุงููุฉ ูู ูู ุฒูุฌ ููุง ููู:

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

ููุน ุฐููุ ูุง ูููููุง ุจุจุณุงุทุฉ ุชูุฑูุฑ ุชุณูุณููู ุฅูู ุงููููุฐุฌ ูุงูุญุตูู ุนูู ุชูุจุค ุจูุง ุฅุฐุง ูุงูุช ุงูุฌููุชูู ุนุจุงุฑุฉ ุนู ุฅุนุงุฏุฉ ุตูุงุบุฉ ุฃู ูุง. ูุญู ุจุญุงุฌุฉ ุฅูู ุงูุชุนุงูู ูุน ุงูุชุณูุณููู ูุฒูุฌุ ูุชุทุจูู ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ุงูููุงุณุจุฉ. ูุญุณู ุงูุญุธุ ูููู ูุฃุฏุงุฉ "ูุนุงูุฌุฉ ุงูุฑููุฒ" ุฃูุถูุง ุงูุชุนุงูู ูุน ุฒูุฌ ูู ุงูุชุณูุณูุงุช ูุฅุนุฏุงุฏููุง ุจุงูุทุฑููุฉ ุงูุชู ูุชููุนูุง ูููุฐุฌ BERT:

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python out
{
'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

ููุฏ ูุงูุดูุง ููุงุชูุญ `input_ids` ู`attention_mask` ูู [ุงููุตู 2](/course/chapter2)ุ ูููููุง ุฃุฌููุง ุงูุญุฏูุซ ุนู `token_type_ids`. ูู ูุฐุง ุงููุซุงูุ ูุฐุง ูุง ูุฎุจุฑ ุงููููุฐุฌ ุจุงูุฌุฒุก ุงูุฐู ููุชูู ุฅูู ุงูุฌููุฉ ุงูุฃููู ูุงูุฌุฒุก ุงูุฐู ููุชูู ุฅูู ุงูุฌููุฉ ุงูุซุงููุฉ.

<Tip>

โ๏ธ **ุฌุฑุจูุง!** ุฎุฐ ุงูุนูุตุฑ 15 ูู ูุฌููุนุฉ ุงูุชุฏุฑูุจ ููู ุจูุนุงูุฌุฉ ุงูุฌููุชูู ุจุดูู ูููุตู ููุฒูุฌ. ูุง ูู ุงููุฑู ุจูู ุงููุชูุฌุชููุ

</Tip>

ุฅุฐุง ูููุง ุจูู ุชุฑููุฒ ุงููุนุฑูุงุช ุฏุงุฎู `input_ids` ูุฑุฉ ุฃุฎุฑู ุฅูู ูููุงุช:

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

ุณูุญุตู ุนูู:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

ูุฐูู ูุฑู ุฃู ุงููููุฐุฌ ูุชููุน ุฃู ุชููู ุงููุฏุฎูุงุช ุนูู ุงูุดูู `[CLS] sentence1 [SEP] sentence2 [SEP]` ุนูุฏูุง ูููู ููุงู ุฌููุชูู. ูุจููุงุกูุฉ ุฐูู ูุน `token_type_ids`ุ ูุญุตู ุนูู:

```python out
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

ููุง ุชุฑูุ ูุฅู ุฃุฌุฒุงุก ุงููุฏุฎูุงุช ุงูููุงุจูุฉ ูู `[CLS] sentence1 [SEP]` ูุฏููุง ุฌููุนูุง ูุนุฑู ููุน ุงูุฑูุฒ "0"ุ ูู ุญูู ุฃู ุงูุฃุฌุฒุงุก ุงูุฃุฎุฑูุ ุงูููุงุจูุฉ ูู `sentence2 [SEP]`ุ ูุฏููุง ุฌููุนูุง ูุนุฑู ููุน ุงูุฑูุฒ "1".

ููุงุญุธุฉ: ุฅุฐุง ููุช ุจุงุฎุชูุงุฑ ููุทุฉ ุชูุชูุด ูุฎุชููุฉุ ููู ุชุญุตู ุจุงูุถุฑูุฑุฉ ุนูู `token_type_ids` ูู ูุฏุฎูุงุชู ุงููุนุงูุฌุฉ (ุนูู ุณุจูู ุงููุซุงูุ ูุง ูุชู ุฅุฑุฌุงุนูุง ุฅุฐุง ููุช ุชุณุชุฎุฏู ูููุฐุฌ DistilBERT). ูุชู ุฅุฑุฌุงุนูุง ููุท ุนูุฏูุง ูุนุฑู ุงููููุฐุฌ ูุงุฐุง ููุนู ุจูุงุ ูุฃูู ุดุงูุฏูุง ุฃุซูุงุก ุงูุชุฏุฑูุจ ุงููุณุจู.

ููุงุ ุชู ุชุฏุฑูุจ BERT ุจุงุณุชุฎุฏุงู ูุนุฑูุงุช ููุน ุงูุฑูุฒุ ุจุงูุฅุถุงูุฉ ุฅูู ูุฏู ููุฐุฌุฉ ุงููุบุฉ ุงููููุนุฉ ุงูุฐู ุชุญุฏุซูุง ุนูู ูู [ุงููุตู 1](/course/chapter1)ุ ูุฏูู ูุฏู ุฅุถุงูู ูุณูู _ุงูุชูุจุค ุจุงูุฌููุฉ ุงูุชุงููุฉ_. ุงููุฏู ูู ูุฐู ุงููููุฉ ูู ููุฐุฌุฉ ุงูุนูุงูุฉ ุจูู ุฃุฒูุงุฌ ุงูุฌูู.

ูุน ุงูุชูุจุค ุจุงูุฌููุฉ ุงูุชุงููุฉุ ูุชู ุชุฒููุฏ ุงููููุฐุฌ ุจุฃุฒูุงุฌ ูู ุงูุฌูู (ูุน ุฑููุฒ ูููุนุฉ ุจุดูู ุนุดูุงุฆู) ููุทูุจ ููู ุงูุชูุจุค ุจูุง ุฅุฐุง ูุงูุช ุงูุฌููุฉ ุงูุซุงููุฉ ุชุชุจุน ุงูุฃููู. ูุฌุนู ุงููููุฉ ุบูุฑ ุจุณูุทุฉุ ูุตู ุงูููุช ุชุชุจุน ุงูุฌูู ุจุนุถูุง ุงูุจุนุถ ูู ุงููุณุชูุฏ ุงูุฃุตูู ุงูุฐู ุชู ุงุณุชุฎุฑุงุฌูุง ูููุ ูุงููุตู ุงูุขุฎุฑ ูู ุงูููุช ุชุฃุชู ุงูุฌููุชุงู ูู ูุณุชูุฏูู ูุฎุชูููู.

ุจุดูู ุนุงูุ ูุง ุชุญุชุงุฌ ุฅูู ุงูููู ุจุดุฃู ูุง ุฅุฐุง ูุงูุช ููุงู `token_type_ids` ูู ูุฏุฎูุงุชู ุงููุนุงูุฌุฉ ุฃู ูุง: ุทุงููุง ุฃูู ุชุณุชุฎุฏู ููุณ ููุทุฉ ุงูุชูุชูุด ูุฃุฏุงุฉ "ูุนุงูุฌุฉ ุงูุฑููุฒ" ูุงููููุฐุฌุ ุณูููู ูู ุดูุก ุนูู ูุง ูุฑุงู ูุฃู ุฃุฏุงุฉ "ูุนุงูุฌุฉ ุงูุฑููุฒ" ุชุนุฑู ูุง ูุฌุจ ุชูุฏููู ููููุฐุฌูุง.

ุงูุขู ุจุนุฏ ุฃู ุฑุฃููุง ููู ูููู ูุฃุฏุงุฉ "ูุนุงูุฌุฉ ุงูุฑููุฒ" ุงูุชุนุงูู ูุน ุฒูุฌ ูุงุญุฏ ูู ุงูุฌููุ ูููููุง ุงุณุชุฎุฏุงููุง ููุนุงูุฌุฉ ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง: ูุซููุง ูุนููุง ูู [ุงููุตู ุงูุณุงุจู](/course/chapter2)ุ ูููููุง ุฅุฏุฎุงู ูุงุฆูุฉ ูู ุฃุฒูุงุฌ ุงูุฌูู ูู ุฃุฏุงุฉ "ูุนุงูุฌุฉ ุงูุฑููุฒ" ูู ุฎูุงู ุฅุนุทุงุฆูุง ูุงุฆูุฉ ุงูุฌูู ุงูุฃูููุ ุซู ูุงุฆูุฉ ุงูุฌูู ุงูุซุงููุฉ. ูุฐุง ูุชูุงูู ุฃูุถูุง ูุน ุฎูุงุฑุงุช ุงูุญุดู ูุงูุชูููู ุงูุชู ุฑุฃููุงูุง ูู [ุงููุตู 2](/course/chapter2). ูุฐุงุ ุฅุญุฏู ุทุฑู ูุนุงูุฌุฉ ูุฌููุนุฉ ุจูุงูุงุช ุงูุชุฏุฑูุจ ูู:

```py
tokenized_dataset = tokenizer(
raw_datasets["train"]["sentence1"],
raw_datasets["train"]["sentence2"],
padding=True,
truncation=True,
)
```

ูุฐุง ูุนูู ุจุดูู ุฌูุฏุ ููููู ูู ุนูุจ ุฅุฑุฌุงุน ูุงููุณ (ูุน ููุงุชูุญูุงุ `input_ids`ุ `attention_mask`ุ ู`token_type_ids`ุ ูุงูููู ุงูุชู ูู ููุงุฆู ูู ุงูููุงุฆู). ููุง ุฃูู ูู ูุนูู ุฅูุง ุฅุฐุง ูุงู ูุฏูู ุฐุงูุฑุฉ ูุตูู ุนุดูุงุฆู (RAM) ูุงููุฉ ูุชุฎุฒูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง ุฃุซูุงุก ุงููุนุงูุฌุฉ (ูู ุญูู ุฃู ูุฌููุนุงุช ุงูุจูุงูุงุช ูู ููุชุจุฉ ูุฌููุนุงุช ุงูุจูุงูุงุช ๐ค ูู ูููุงุช [Apache Arrow](https://arrow.apache.org/) ุงููุฎุฒูุฉ ุนูู ุงููุฑุตุ ูุฐุง ูุฃูุช ุชุญุชูุธ ููุท ุจุงูุนููุงุช ุงูุชู ุชุทูุจูุง ูุญููุฉ ูู ุงูุฐุงูุฑุฉ).

ููุญูุงุธ ุนูู ุงูุจูุงูุงุช ููุฌููุนุฉ ุจูุงูุงุชุ ุณูุณุชุฎุฏู ุทุฑููุฉ [`Dataset.map()`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.map). ูุณูุญ ููุง ุฐูู ุฃูุถูุง ุจุจุนุถ ุงููุฑููุฉ ุงูุฅุถุงููุฉุ ุฅุฐุง ููุง ุจุญุงุฌุฉ ุฅูู ูุฒูุฏ ูู ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ุฃูุซุฑ ูู ูุฌุฑุฏ ุงููุนุงูุฌุฉ. ุชุนูู ุทุฑููุฉ `map()` ูู ุฎูุงู ุชุทุจูู ุฏุงูุฉ ุนูู ูู ุนูุตุฑ ูู ุนูุงุตุฑ ูุฌููุนุฉ ุงูุจูุงูุงุชุ ูุฐุง ุฏุนูุง ูููู ุจุชุนุฑูู ุฏุงูุฉ ุชููู ุจูุนุงูุฌุฉ ูุฏุฎูุงุชูุง:

```py
def tokenize_function(example):
return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

ุชููู ูุฐู ุงูุฏุงูุฉ ุจุฃุฎุฐ ูุงููุณ (ูุซู ุนูุงุตุฑ ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง) ูุฅุฑุฌุงุน ูุงููุณ ุฌุฏูุฏ ุจููุงุชูุญ `input_ids`ุ `attention_mask`ุ ู`token_type_ids`. ูุงุญุธ ุฃููุง ุชุนูู ุฃูุถูุง ุฅุฐุง ูุงู ุงููุงููุณ "example" ูุญุชูู ุนูู ุนุฏุฉ ุนููุงุช (ูู ููุชุงุญ ููุงุฆูุฉ ูู ุงูุฌูู) ูุธุฑูุง ูุฃู "tokenizer" ุชุนูู ุนูู ููุงุฆู ูู ุฃุฒูุงุฌ ุงูุฌููุ ููุง ุฑุฃููุง ุณุงุจููุง. ุณูุณูุญ ููุง ุฐูู ุจุงุณุชุฎุฏุงู ุงูุฎูุงุฑ `batched=True` ูู ููุงููุฉ `map()`ุ ูุงูุชู ุณุชุณุฑุน ุจุดูู ูุจูุฑ ูู ุงููุนุงูุฌุฉ. ูุชู ุฏุนู "tokenizer" ุจูุงุณุทุฉ "tokenizer" ููุชูุจ ุจูุบุฉ Rust ูู ููุชุจุฉ [๐ค Tokenizers](https://github.com/huggingface/tokenizers). ูููู ุฃู ุชููู ูุฐู ุงูุฃุฏุงุฉ ููุนุงูุฌุฉ ุงูุฑููุฒ ุณุฑูุนุฉ ููุบุงูุฉุ ูููู ููุท ุฅุฐุง ุฃุนุทููุงูุง ุงููุซูุฑ ูู ุงููุฏุฎูุงุช ูู ููุณ ุงูููุช.

ูุงุญุธ ุฃููุง ุชุฑููุง ุญุฌุฉ "padding" ุฎุงุฑุฌ ุฏุงูุฉ ุงููุนุงูุฌุฉ ุงูุฎุงุตุฉ ุจูุง ูู ุงูููุช ุงูุญุงูู. ููุฑุฌุน ุฐูู ุฅูู ุฃู ุญุดู ุฌููุน ุงูุนููุงุช ุฅูู ุงูุทูู ุงูุฃูุตู ุบูุฑ ูุนุงู: ููู ุงูุฃูุถู ุญุดู ุงูุนููุงุช ุนูุฏูุง ูููู ุจุจูุงุก ุฏูุนุฉุ ุญูุซ ุฃููุง ุจุญุงุฌุฉ ููุท ุฅูู ุญุดููุง ุฅูู ุงูุทูู ุงูุฃูุตู ูู ุชูู ุงูุฏูุนุฉุ ูููุณ ุงูุทูู ุงูุฃูุตู ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง. ูููู ุฃู ูููุฑ ุฐูู ุงููุซูุฑ ูู ุงูููุช ูููุฉ ุงููุนุงูุฌุฉ ุนูุฏูุง ุชููู ุงููุฏุฎูุงุช ุฐุงุช ุฃุทูุงู ูุชุบูุฑุฉ ููุบุงูุฉ!

ููุง ูููู ุจุชุทุจูู ุฏุงูุฉ ุงููุนุงูุฌุฉ ุนูู ุฌููุน ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง ูู ููุณ ุงูููุช. ูุญู ูุณุชุฎุฏู `batched=True` ูู ููุงููุฉ `map` ุจุญูุซ ูุชู ุชุทุจูู ุงูุฏุงูุฉ ุนูู ุนูุงุตุฑ ูุชุนุฏุฏุฉ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง ูู ููุณ ุงูููุชุ ูููุณ ุนูู ูู ุนูุตุฑ ุนูู ุญุฏุฉ. ูุณูุญ ุฐูู ุจูุนุงูุฌุฉ ูุณุจูุฉ ุฃุณุฑุน.

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

ุชุทุจู ููุชุจุฉ ูุฌููุนุงุช ุงูุจูุงูุงุช ๐ค ูุฐู ุงููุนุงูุฌุฉ ุนู ุทุฑูู ุฅุถุงูุฉ ุญููู ุฌุฏูุฏุฉ ุฅูู ูุฌููุนุงุช ุงูุจูุงูุงุชุ ูุงุญุฏ ููู ููุชุงุญ ูู ุงููุงููุณ ุงูุฐู ุชุนูุฏู ุฏุงูุฉ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ:

```python out
DatasetDict({
train: Dataset({
features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
num_rows: 3668
})
validation: Dataset({
features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
num_rows: 408
})
test: Dataset({
features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
num_rows: 1725
})
})
```

ููููู ุญุชู ุงุณุชุฎุฏุงู ุงููุนุงูุฌุฉ ุงููุชุนุฏุฏุฉ ุนูุฏ ุชุทุจูู ุฏุงูุฉ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ุงูุฎุงุตุฉ ุจู ุจุงุณุชุฎุฏุงู `map()` ุนู ุทุฑูู ุชูุฑูุฑ ุญุฌุฉ `num_proc`. ูู ููุนู ุฐูู ููุง ูุฃู ููุชุจุฉ ๐ค Tokenizers ุชุณุชุฎุฏู ุจุงููุนู ุนุฏุฉ ุฎููุท ููุนุงูุฌุฉ ุนููุงุชูุง ุจุดูู ุฃุณุฑุนุ ูููู ุฅุฐุง ููุช ูุง ุชุณุชุฎุฏู ุฃุฏุงุฉ "ูุนุงูุฌุฉ ุฑููุฒ" ุณุฑูุนุฉ ูุฏุนููุฉ ุจูุฐู ุงูููุชุจุฉุ ููุฏ ูุคุฏู ุฐูู ุฅูู ุชุณุฑูุน ูุนุงูุฌุชู ุงููุณุจูุฉ.

ุชูุฑุฌุน ุฏุงูุฉ `tokenize_function` ุงูุฎุงุตุฉ ุจูุง ูุงููุณูุง ุจููุงุชูุญ `input_ids`ุ `attention_mask`ุ ู`token_type_ids`ุ ูุฐุง ูุชู ุฅุถุงูุฉ ูุฐู ุงูุญููู ุงูุซูุงุซุฉ ุฅูู ุฌููุน ุชูุณููุงุช ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง. ูุงุญุธ ุฃูู ูุงู ุจุฅููุงููุง ุฃูุถูุง ุชุบููุฑ ุงูุญููู ุงูููุฌูุฏุฉ ุฅุฐุง ุฃุนุงุฏุช ุฏุงูุฉ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ุงูุฎุงุตุฉ ุจูุง ูููุฉ ูููุชุงุญ ููุฌูุฏ ุจุงููุนู ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุชู ุทุจููุง ุนูููุง `map()`.

ุฃุฎุฑ ุดูุก ุณูุญุชุงุฌ ุฅูู ูุนูู ูู ุญุดู ุฌููุน ุงูุฃูุซูุฉ ุฅูู ุทูู ุงูุนูุตุฑ ุงูุฃุทูู ุนูุฏ ุชุฌููุน ุงูุนูุงุตุฑ ูุนูุง - ููู ุชูููุฉ ูุดูุฑ ุฅูููุง ุจุงุณู *ุงูุญุดู ุงูุฏููุงูููู*.
### ุงูุญุดู ุงูุฏููุงูููู

<Youtube id="7q5NyFT8REg"/>

ุฅู ุงููุธููุฉ ุงููุณุคููุฉ ุนู ุชุฌููุน ุงูุนููุงุช ุฏุงุฎู ุฏูุนุฉ ุชุณูู ูุธููุฉ ุงูุฌูุน. ููู ุญุฌุฉ ูููู ุชูุฑูุฑูุง ุนูุฏ ุฅูุดุงุก `DataLoader`ุ ูุงูุงูุชุฑุงุถู ูู ูุธููุฉ ุณุชููู ููุท ุจุชุญููู ุนููุงุชู ุฅูู ุชูุณูุฑุงุช PyTorch ูุฏูุฌูุง (ุจุดูู ูุชูุฑุฑ ุฅุฐุง ูุงูุช ุนูุงุตุฑู ููุงุฆู ุฃู ูุฌููุนุงุช ุฃู ููุงููุณ). ูู ูููู ูุฐุง ูููููุง ูู ุญุงูุชูุง ูุธุฑูุง ูุฃู ุงููุฏุฎูุงุช ุงูุชู ูุฏููุง ูู ุชููู ุฌููุนูุง ุจููุณ ุงูุญุฌู. ููุฏ ุฃุฑุฌุฃูุง ุงูุญุดู ุนู ุนูุฏุ ูุชุทุจููู ููุท ุญุณุจ ุงูุญุงุฌุฉ ุนูู ูู ุฏูุนุฉ ูุชุฌูุจ ูุฌูุฏ ูุฏุฎูุงุช ุทูููุฉ ููุบุงูุฉ ูุน ุงููุซูุฑ ูู ุงูุญุดู. ุณูุคุฏู ูุฐุง ุฅูู ุชุณุฑูุน ุงูุชุฏุฑูุจ ุฅูู ุญุฏ ูุงุ ูููู ูุงุญุธ ุฃูู ุฅุฐุง ููุช ุชุชุฏุฑุจ ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ tensor  (TPU) ููุฏ ูุณุจุจ ูุดููุงุช - ุชูุถู ูุญุฏุงุช TPU ุงูุฃุดูุงู ุงูุซุงุจุชุฉุ ุญุชู ุนูุฏูุง ูุชุทูุจ ุฐูู ุญุดููุง ุฅุถุงูููุง.

ุฅู ุงููุธููุฉ ุงููุณุคููุฉ ุนู ุชุฌููุน ุงูุนููุงุช ุฏุงุฎู ุฏูุนุฉ ุชุณูู ูุธููุฉ ุงูุฌูุน. ูุงููุธููุฉ ุงูุงูุชุฑุงุถูุฉ ูู ูุธููุฉ ุณุชููู ููุท ุจุชุญููู ุนููุงุชู ุฅูู ุชูุณูุฑ TensorFlow ูุฏูุฌูุง (ุจุดูู ูุชูุฑุฑ ุฅุฐุง ูุงูุช ุนูุงุตุฑู ููุงุฆู ุฃู ูุฌููุนุงุช ุฃู ููุงููุณ). ูู ูููู ูุฐุง ูููููุง ูู ุญุงูุชูุง ูุธุฑูุง ูุฃู ุงููุฏุฎูุงุช ุงูุชู ูุฏููุง ูู ุชููู ุฌููุนูุง ุจููุณ ุงูุญุฌู. ููุฏ ุฃุฑุฌุฃูุง ุงูุญุดู ุนู ุนูุฏุ ูุชุทุจููู ููุท ุญุณุจ ุงูุญุงุฌุฉ ุนูู ูู ุฏูุนุฉ ูุชุฌูุจ ูุฌูุฏ ูุฏุฎูุงุช ุทูููุฉ ููุบุงูุฉ ูุน ุงููุซูุฑ ูู ุงูุญุดู. ุณูุคุฏู ูุฐุง ุฅูู ุชุณุฑูุน ุงูุชุฏุฑูุจ ุฅูู ุญุฏ ูุงุ ูููู ูุงุญุธ ุฃูู ุฅุฐุง ููุช ุชุชุฏุฑุจ ุนูู ูุญุฏุฉ TPU ููุฏ ูุณุจุจ ูุดููุงุช - ุชูุถู ูุญุฏุงุช TPU ุงูุฃุดูุงู ุงูุซุงุจุชุฉุ ุญุชู ุนูุฏูุง ูุชุทูุจ ุฐูู ุญุดููุง ุฅุถุงูููุง.

ููุนู ุฐูู ุนููููุงุ ูุชุนูู ุนูููุง ุชุญุฏูุฏ ูุธููุฉ ุงูุฌูุน ุงูุชู ุณุชุทุจู ุงููููุฉ ุงูุตุญูุญุฉ ูู ุงูุญุดู ุนูู ุนูุงุตุฑ ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุชู ูุฑูุฏ ุชุฌููุนูุง ูู ุฏูุนุฉ ูุงุญุฏุฉ. ูุญุณู ุงูุญุธุ ุชููุฑ ููุชุจุฉ Hugging Face Transformers ููุง ูุซู ูุฐู ุงููุธููุฉ ูู ุฎูุงู `DataCollatorWithPadding`. ุฅูู ูุฃุฎุฐ ูุญุฏุฏ ููุงูุน ุนูุฏ ุฅูุดุงุก ูุซูู ูู (ููุนุฑูุฉ ุฑูุฒ ุงูุญุดู ุงูุฐู ุณูุชู ุงุณุชุฎุฏุงููุ ููุง ุฅุฐุง ูุงู ุงููููุฐุฌ ูุชููุน ุงูุญุดู ุนูู ุงููุณุงุฑ ุฃู ุนูู ูููู ุงููุฏุฎูุงุช) ูุณููุนู ูู ูุง ุชุญุชุงุฌู:

```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```

ูุงุฎุชุจุงุฑ ูุฐู ุงููุนุจุฉ ุงูุฌุฏูุฏุฉุ ุฏุนูุง ูุฃุฎุฐ ุจุนุถ ุงูุนููุงุช ูู ูุฌููุนุฉ ุงูุชุฏุฑูุจ ุงูุชู ููุฏ ุชุฌููุนูุง ูู ุฏูุนุฉ ูุงุญุฏุฉ. ููุงุ ูููู ุจุฅุฒุงูุฉ ุฃุนูุฏุฉ `idx` ู`sentence1` ู`sentence2` ูุฃููุง ูู ุชููู ุถุฑูุฑูุฉ ูุชุญุชูู ุนูู ุณูุงุณู (ููุง ูููููุง ุฅูุดุงุก ุชูุณูุฑุงุช ุจุงุณุชุฎุฏุงู ุงูุณูุงุณู) ููููู ูุธุฑุฉ ุนูู ุฃุทูุงู ูู ุฅุฏุฎุงู ูู ุงูุฏูุนุฉ:

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python out
[50, 59, 47, 67, 59, 50, 62, 32]
```

ูุง ุนุฌุจุ ูุญุตู ุนูู ุนููุงุช ุฐุงุช ุฃุทูุงู ูุฎุชููุฉุ ูู 32 ุฅูู 67. ูุนูู ุงูุญุดู ุงูุฏููุงูููู ุฃูู ูุฌุจ ุญุดู ุงูุนููุงุช ูู ูุฐู ุงูุฏูุนุฉ ุฌููุนูุง ุฅูู ุทูู 67ุ ููู ุงูุทูู ุงูุฃูุตู ุฏุงุฎู ุงูุฏูุนุฉ. ุจุฏูู ุงูุญุดู ุงูุฏููุงููููุ ุณูุชุนูู ุญุดู ุฌููุน ุงูุนููุงุช ุฅูู ุงูุทูู ุงูุฃูุตู ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุงุ ุฃู ุงูุทูู ุงูุฃูุตู ุงูุฐู ูููู ุฃู ููุจูู ุงููููุฐุฌ. ุฏุนููุง ูุชุฃูุฏ ูุฑุฉ ุฃุฎุฑู ูู ุฃู `data_collator` ูุญุดู ุงูุฏูุนุฉ ุฏููุงูููููุง ุจุดูู ุตุญูุญ:

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

```python out
{'attention_mask': TensorShape([8, 67]),
'input_ids': TensorShape([8, 67]),
'token_type_ids': TensorShape([8, 67]),
'labels': TensorShape([8])}
```

```python out
{'attention_mask': torch.Size([8, 67]),
'input_ids': torch.Size([8, 67]),
'token_type_ids': torch.Size([8, 67]),
'labels': torch.Size([8])}
```

ููุชุงุฒ! ุงูุขู ุจุนุฏ ุฃู ุงูุชูููุง ูู ุงููุต ุงูุฎุงู ุฅูู ุงูุฏูุนุงุช ุงูุชู ูููู ุฃู ูุชุนุงูู ูุนูุง ูููุฐุฌูุงุ ููุญู ูุณุชุนุฏูู ูุถุจุท ุฏูุชู!

โ๏ธ **ุฌุฑุจู!** ูู ุจุชูุฑุงุฑ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ุนูู ูุฌููุนุฉ ุจูุงูุงุช GLUE SST-2. ุฅูู ูุฎุชูู ููููุงู ูุฃูู ูุชููู ูู ุฌูู ูุฑุฏูุฉ ุจุฏูุงู ูู ุฃุฒูุงุฌุ ูููู ูุฌุจ ุฃู ูุจุฏู ุจุงูู ูุง ูุนููุงู ููุณู. ููุชุญุฏู ุฃุตุนุจุ ุฌุฑุจ ูุชุงุจุฉ ุฏุงูุฉ ูุนุงูุฌุฉ ูุณุจูุฉ ุชุนูู ุนูู ุฃู ูู ููุงู GLUE.

ุงูุขู ุจุนุฏ ุฃู ุฃุตุจุญ ูุฏููุง ูุฌููุนุฉ ุงูุจูุงูุงุช ููุธููุฉ ุงูุฌูุนุ ูุชุนูู ุนูููุง ุงูุฌูุน ุจููููุง. ูููููุง ูุฏูููุง ุชุญููู ุงูุฏูุนุงุช ูุชุฌููุนูุงุ ูููู ูุฐุง ูุชุทูุจ ุงููุซูุฑ ูู ุงูุนููุ ููุฏ ูุง ูููู ูุนุงููุง ุฃูุถูุง. ุจุฏูุงู ูู ุฐููุ ููุงู ุทุฑููุฉ ุจุณูุทุฉ ุชููุฑ ุญูุงู ูุนุงููุง ููุฐู ุงููุดููุฉ: `to_tf_dataset()`. ุณูุชู ูู `tf.data.Dataset` ุญูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุ ูุน ุฏุงูุฉ ุชุฌููุน ุงุฎุชูุงุฑูุฉ. `tf.data.Dataset` ูู ุชูุณูู TensorFlow ุงูุฃุตูู ุงูุฐู ูููู ูู Keras ุงุณุชุฎุฏุงูู ูู `model.fit()`ุ ูุฐุง ูุฅู ูุฐู ุงูุทุฑููุฉ ุชุญูู ุนูู ุงูููุฑ ูุฌููุนุฉ ุจูุงูุงุช Hugging Face ุฅูู ุชูุณูู ุฌุงูุฒ ููุชุฏุฑูุจ. ุฏุนููุง ูุฑู ุฐูู ูู ุงูุนูู ูุน ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจูุง!

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
columns=["attention_mask", "input_ids", "token_type_ids"],
label_cols=["labels"],
shuffle=True,
collate_fn=data_collator,
batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
columns=["attention_mask", "input_ids", "token_type_ids"],
label_cols=["labels"],
shuffle=False,
collate_fn=data_collator,
batch_size=8,
)
```

ููุฐุง ูู ุดูุก! ูููููุง ุฃุฎุฐ ูุฐู ุงููุฌููุนุงุช ูู ุงูุจูุงูุงุช ุฅูู ุงููุญุงุถุฑุฉ ุงูุชุงููุฉุ ุญูุซ ุณูููู ุงูุชุฏุฑูุจ ููุชุนูุง ููุณุชููููุง ุจุนุฏ ูู ุงูุนูู ุงูุดุงู ูููุนุงูุฌุฉ ุงููุณุจูุฉ ููุจูุงูุงุช.