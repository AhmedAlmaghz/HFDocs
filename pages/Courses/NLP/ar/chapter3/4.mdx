# دورة تدريبية كاملة 

الآن، سنرى كيف يمكننا تحقيق نفس النتائج التي توصلنا إليها في القسم السابق دون استخدام فئة `Trainer`. مرة أخرى، نفترض أنك قمت بمعالجة البيانات في القسم 2. فيما يلي ملخص موجز يغطي كل ما ستحتاج إليه:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

### الاستعداد للتدريب

قبل كتابة حلقة التدريب الخاصة بنا بالفعل، سنحتاج إلى تحديد بعض الأشياء. أولها هي أدوات تحميل البيانات التي سنستخدمها للتنقل خلال الدفعات. ولكن قبل أن نتمكن من تحديد أدوات تحميل البيانات هذه، نحتاج إلى تطبيق بعض ما بعد المعالجة على `tokenized_datasets`، للاهتمام ببعض الأشياء التي قامت بها فئة `Trainer` تلقائيًا. على وجه التحديد، نحتاج إلى:

- إزالة الأعمدة المقابلة للقيم التي لا يتوقعها النموذج (مثل أعمدة "sentence1" و "sentence2").
- إعادة تسمية العمود "label" إلى "labels" (لأن النموذج يتوقع أن يكون اسم الحجة "labels").
- تعيين تنسيق مجموعات البيانات بحيث تعيد تنسيق PyTorch بدلاً من القوائم.

لدى `tokenized_datasets` طريقة لكل من هذه الخطوات:

```py
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```

بعد ذلك، يمكننا التحقق من أن النتيجة تحتوي فقط على أعمدة سيقبلها نموذجنا:

```python
["attention_mask", "input_ids", "labels", "token_type_ids"]
```

الآن وبعد أن انتهينا من ذلك، يمكننا بسهولة تحديد أدوات تحميل البيانات الخاصة بنا:

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

وللتحقق بسرعة من عدم وجود أي خطأ في معالجة البيانات، يمكننا فحص دفعة مثل هذه:

```py
for batch in train_dataloader:
break
{k: v.shape for k, v in batch.items()}
```

```python out
{'attention_mask': torch.Size([8, 65]),
'input_ids': torch.Size([8, 65]),
'labels': torch.Size([8]),
'token_type_ids': torch.Size([8, 65])}
```

لاحظ أن الأشكال الفعلية ستكون مختلفة قليلاً بالنسبة لك لأننا حددنا `shuffle=True` لأداة تحميل البيانات التدريبية ونقوم بالتقسيم إلى أقصى طول داخل الدفعة.

والآن، بعد أن انتهينا تمامًا من المعالجة المسبقة للبيانات (وهو هدف مرضٍ ولكنه بعيد المنال لأي ممارس لتعلم الآلة)، دعنا ننتقل إلى النموذج. نقوم بتهيئته تمامًا كما فعلنا في القسم السابق:

```py
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

وللتأكد من أن كل شيء سيسير بسلاسة أثناء التدريب، نمر دفعتنا إلى هذا النموذج:

```py
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```

```python out
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

ستعيد جميع نماذج 🤗 Transformers الخسارة عندما يتم توفير "labels"، كما نحصل على logits (اثنان لكل إدخال في دفعتنا، لذا فإن tensor بحجم 8 × 2).

نحن على وشك كتابة حلقة التدريب الخاصة بنا! نفتقد شيئين فقط: محسن ومخطط معدل التعلم. نظرًا لأننا نحاول تكرار ما كانت تفعله فئة `Trainer` يدويًا، فسنستخدم الافتراضيات نفسها. المحسن الذي يستخدمه `Trainer` هو `AdamW`، وهو نفسه Adam، ولكن مع تعديل لاضمحلال وزن منتظم (راجع ["Decoupled Weight Decay Regularization"](https://arxiv.org/abs/1711.05101) بقلم إيليا لوسشيلوف وفرانك هوتر):

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

أخيرًا، فإن مخطط معدل التعلم المستخدم بشكل افتراضي هو مجرد انخفاض خطي من القيمة القصوى (5e-5) إلى الصفر. ولتحديد ذلك بشكل صحيح، نحتاج إلى معرفة عدد خطوات التدريب التي سنقوم بها، والتي هي عدد حقبات التدريب التي نريد تشغيلها مضروبة في عدد دفعات التدريب (والتي هي طول أداة تحميل البيانات التدريبية الخاصة بنا). يستخدم `Trainer` ثلاث حقبات بشكل افتراضي، لذا فسنتبع ذلك:

```py
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
"linear",
optimizer=optimizer,
num_warmup_steps=0,
num_training_steps=num_training_steps,
)
print(num_training_steps)
```

```python out
1377
```

### حلقة التدريب

أمر واحد أخير: سنرغب في استخدام وحدة معالجة الرسومات (GPU) إذا كان بإمكاننا الوصول إلى واحدة (على وحدة المعالجة المركزية (CPU)، فقد يستغرق التدريب عدة ساعات بدلاً من بضع دقائق). للقيام بذلك، نحدد `device` سنضع نموذجنا ودفعاتنا عليه:

```py
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
device
```

```python out
device(type='cuda')
```

والآن نحن مستعدون للتدريب! للحصول على بعض الإحساس بالوقت الذي سيستغرقه التدريب، نضيف شريط تقدم فوق عدد خطوات التدريب الخاصة بنا، باستخدام مكتبة `tqdm`:

```py
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
for batch in train_dataloader:
batch = {k: v.to(device) for k, v in batch.items()}
outputs = model(**batch)
loss = outputs.loss
loss.backward()

optimizer.step()
lr_scheduler.step()
optimizer.zero_grad()
progress_bar.update(1)
```

يمكنك أن ترى أن جوهر حلقة التدريب يشبه إلى حد كبير تلك الموجودة في المقدمة. لم نطلب أي تقارير، لذا فإن حلقة التدريب هذه لن تخبرنا بأي شيء عن كيفية أداء النموذج. نحتاج إلى إضافة حلقة تقييم لذلك.

### حلقة التقييم

كما فعلنا سابقًا، سنستخدم مقياسًا يوفره مكتبة 🤗 Evaluate. لقد رأينا بالفعل طريقة `metric.compute()`، ولكن يمكن للمقاييس في الواقع أن تجمع الدفعات لنا أثناء التنقل في حلقة التنبؤ باستخدام طريقة `add_batch()`. وبمجرد أن نكون قد تراكمت جميع الدفعات، يمكننا الحصول على النتيجة النهائية مع `metric.compute()`. إليك كيفية تنفيذ كل هذا في حلقة تقييم:

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
batch = {k: v.to(device) for k, v in batch.items()}
with torch.no_grad():
outputs = model(**batch)

logits = outputs.logits
predictions = torch.argmax(logits, dim=-1)
metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

```python out
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

مرة أخرى، ستكون نتائجك مختلفة قليلاً بسبب العشوائية في تهيئة رأس النموذج وخلط البيانات، ولكن يجب أن تكون في نفس النطاق.

✏️ **جربها!** عدل حلقة التدريب السابقة لتدريب نموذجك على مجموعة بيانات SST-2.

### شحن حلقة التدريب الخاصة بك مع 🤗 Accelerate

ستعمل حلقة التدريب التي حددناها للتو بشكل جيد على وحدة معالجة مركزية (CPU) أو وحدة معالجة رسومات (GPU) واحدة. ولكن باستخدام مكتبة [🤗 Accelerate](https://github.com/huggingface/accelerate)، يمكننا، ببعض التعديلات البسيطة، تمكين التدريب الموزع على وحدات معالجة رسومات (GPU) أو وحدات معالجة الدقة (TPUs) المتعددة. بدءًا من إنشاء أدوات تحميل بيانات التدريب والتحقق، إليك ما تبدو عليه حلقة التدريب اليدوية الخاصة بنا:

```py
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
"linear",
optimizer=optimizer,
num_warmup_steps=0,
num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
for batch in train_dataloader:
batch = {k: v.to(device) for k, v in batch.items()}
outputs = model(**batch)
loss = outputs.loss
loss.backward()

optimizer.step()
lr_scheduler.step()
optimizer.zero_grad()
progress_bar.update(1)
```

وهنا التغييرات:

```diff
+ from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
"linear",
optimizer=optimizer,
num_warmup_steps=0,
num_training_steps=num_training_steps
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
outputs = model(**batch)
loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

optimizer.step()
lr_scheduler.step()
optimizer.zero_grad()
progress_bar.update(1)
```

الأسطر الأولى التي يجب إضافتها هي سطر الاستيراد. السطر الثاني يقوم بتهيئة كائن `Accelerator` الذي سينظر إلى البيئة ويقوم بتهيئة الإعداد الموزع المناسب. تتولى 🤗 Accelerate التعامل مع وضع الجهاز نيابة عنك، لذا يمكنك إزالة الأسطر التي تضع النموذج على الجهاز (أو، إذا كنت تفضل، قم بتغييرها لاستخدام `accelerator.device` بدلاً من `device`).

ثم يتم الجزء الأكبر من العمل في السطر الذي يرسل أدوات تحميل البيانات والنموذج والمحسن إلى `accelerator.prepare()`. سيقوم هذا بتغليف تلك الكائنات في الحاوية المناسبة للتأكد من أن التدريب الموزع الخاص بك يعمل كما هو مقصود. التغييرات المتبقية التي يجب إجراؤها هي إزالة السطر الذي يضع الدفعة على `device` (مرة أخرى، إذا كنت تريد الاحتفاظ بهذا، فيمكنك فقط تغييره لاستخدام `accelerator.device`) واستبدال `loss.backward()` بـ `accelerator.backward(loss)`.

⚠️ للحصول على أقصى استفادة من السرعة التي توفرها وحدات معالجة الدقة السحابية (TPUs)، نوصي بتقسيم عيناتك إلى طول ثابت باستخدام وسيطي `padding="max_length"` و `max_length` من المعالج.

إذا كنت ترغب في نسخه ولصقه للعب معه، فهذا ما تبدو عليه حلقة التدريب الكاملة مع 🤗 Accelerate:

```py
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
"linear",
optimizer=optimizer,
num_warmup_steps=0,
num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
for batch in train_dl:
outputs = model(**batch)
loss = outputs.loss
accelerator.backward(loss)

optimizer.step()
lr_scheduler.step()
optimizer.zero_grad()
progress_bar.update(1)
```

إن وضع هذا في نص برمجي `train.py` سيجعل هذا النص البرمجي قابلًا للتشغيل على أي نوع من الإعداد الموزع. لتجربته في إعدادك الموزع، قم بتشغيل الأمر:

```bash
accelerate config
```

والذي سيجعلك تجيب على بعض الأسئلة ويقوم بإلقاء إجاباتك في ملف تكوين يستخدمه هذا الأمر:

```
accelerate launch train.py
```

والذي سيبدأ التدريب الموزع.

إذا كنت تريد تجربته في دفتر ملاحظات (على سبيل المثال، لاختباره مع وحدات معالجة الرسومات (TPUs) على Colab)، فما عليك سوى لصق الكود في دالة `training_function()` وتشغيل الخلية الأخيرة باستخدام:

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

يمكنك العثور على المزيد من الأمثلة في مستودع [🤗 Accelerate](https://github.com/huggingface/accelerate/tree/main/examples).