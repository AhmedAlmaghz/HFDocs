# النماذج والمجموعات البيانات المُدربة مسبقًا لتصنيف الصوت
يضم مركز هاجينج فيس هاب أكثر من 500 نموذج مُدرب مسبقًا لتصنيف الصوت. في هذا القسم، سنستعرض بعض مهام تصنيف الصوت الشائعة ونقترح النماذج المُدربة مسبقًا المناسبة لكل منها. باستخدام فئة pipeline()، يكون التبديل بين النماذج والمهام مباشرًا - بمجرد معرفة كيفية استخدام pipeline() لنموذج واحد، ستتمكن من استخدامه لأي نموذج على المركز دون إجراء أي تغييرات على الكود! وهذا يجعل التجارب مع فئة pipeline() سريعة للغاية، مما يتيح لك اختيار أفضل نموذج مُدرب مسبقًا بسرعة لاحتياجاتك.

قبل أن نخوض في مشكلات تصنيف الصوت المختلفة، دعونا نلقي نظرة سريعة على البنى المعمارية للمحول التي يتم استخدامها عادة. البنية المعمارية القياسية لتصنيف الصوت مستوحاة من طبيعة المهمة؛ حيث نريد تحويل تسلسل من المدخلات الصوتية (أي مصفوفة الإدخال الصوتي) إلى تنبؤ بعلامة فئة واحدة. تقوم النماذج التي تستخدم الترميز فقط أولاً برسم خريطة لتسلسل الإدخال الصوتي إلى تسلسل من التمثيلات المخفية عن طريق تمرير المدخلات عبر كتلة محول. ثم يتم رسم خريطة لتسلسل التمثيلات المخفية إلى إخراج علامة الفئة عن طريق حساب المتوسط على المخفيين، وتمرير المتجه الناتج عبر طبقة تصنيف خطية. وبالتالي، هناك تفضيل للنماذج التي تستخدم الترميز فقط لتصنيف الصوت.

تضيف النماذج التي تستخدم فك الترميز فقط تعقيدًا غير ضروري إلى المهمة، لأنها تفترض أن الإخراج يمكن أن يكون أيضًا تسلسلًا من التنبؤات (بدلاً من تنبؤ بعلامة فئة واحدة)، وبالتالي فهي تولد عدة مخرجات. لذلك، فإنها أبطأ في الاستدلال وتميل إلى عدم استخدامها. يتم استبعاد نماذج الترميز وفك الترميز إلى حد كبير لنفس السبب. تتشابه هذه الخيارات المعمارية مع تلك الموجودة في معالجة اللغات الطبيعية، حيث يتم تفضيل النماذج التي تستخدم الترميز فقط مثل BERT لمهام تصنيف التسلسل، بينما يتم حجز نماذج فك الترميز فقط مثل GPT لمهام توليد التسلسل.

الآن بعد أن استعرضنا البنية المعمارية القياسية للمحول لتصنيف الصوت، دعونا ننتقل إلى المجموعات الفرعية المختلفة لتصنيف الصوت ونستعرض أكثر النماذج شيوعًا!

## تثبيت مكتبة Transformers
في وقت كتابة هذا المقال، تتوفر أحدث التحديثات المطلوبة لأنبوب تصنيف الصوت فقط في الإصدار الرئيسي من مستودع مكتبة Transformers، وليس في الإصدار الأخير من PyPi. للتأكد من حصولنا على هذه التحديثات محليًا، سنقوم بتثبيت مكتبة Transformers من الفرع الرئيسي باستخدام الأمر التالي:

## التعرف على الكلمات الرئيسية
التعرف على الكلمات الرئيسية (KWS) هي مهمة التعرف على كلمة رئيسية في عبارة منطوقة. تشكل مجموعة الكلمات الرئيسية المحتملة مجموعة علامات الفئات المتوقعة. وبالتالي، لاستخدام نموذج مُدرب مسبقًا للتعرف على الكلمات الرئيسية، يجب التأكد من مطابقة كلماتك الرئيسية لتلك التي تم تدريب النموذج عليها مسبقًا. فيما يلي، سنقدم مجموعتين من البيانات ونموذجين للتعرف على الكلمات الرئيسية.

### Minds-14
دعونا نستخدم مجموعة البيانات [MINDS-14](https://huggingface.co/datasets/PolyAI/minds14) نفسها التي استكشفناها في الوحدة السابقة. إذا كنت تتذكر، تحتوي MINDS-14 على تسجيلات لأشخاص يطرحون أسئلة على نظام مصرفي إلكتروني بعدة لغات ولهجات، ولديها فئة intent_class لكل تسجيل. يمكننا تصنيف التسجيلات حسب نية المكالمة.

```python
from datasets import load_dataset

minds = load_dataset("PolyAI/minds14", name="en-AU", split="train")
```

سنقوم بتحميل نقطة المراقبة ["anton-l/xtreme_s_xlsr_300m_minds14"](https://huggingface.co/anton-l/xtreme_s_xlsr_300m_minds14)، والتي هي نموذج XLS-R تم ضبطه بشكل دقيق على MINDS-14 لمدة 50 حقبة تقريبًا. يحقق دقة بنسبة 90% لجميع اللغات من MINDS-14 على مجموعة التقييم.

```python
from transformers import pipeline

classifier = pipeline(
"audio-classification",
model="anton-l/xtreme_s_xlsr_300m_minds14",
)
```

أخيرًا، يمكننا تمرير عينة إلى خط أنابيب التصنيف لإجراء التنبؤ:

```python
classifier(minds[0]["audio"])
```

**الإخراج:**

```
[
{"score": 0.9631525278091431, "label": "pay_bill"},
{"score": 0.02819698303937912, "label": "freeze"},
{"score": 0.0032787492964416742, "label": "card_issues"},
{"score": 0.0019414445850998163, "label": "abroad"},
{"score": 0.0008378693601116538, "label": "high_value_payment"},
]
```

رائع! لقد حددنا أن نية المكالمة كانت دفع فاتورة، باحتمالية 96%. يمكنك أن تتخيل استخدام نظام التعرف على الكلمات الرئيسية هذا كمرحلة أولى في مركز اتصال آلي، حيث نريد تصنيف مكالمات العملاء الواردة بناءً على استفساراتهم وتقديم الدعم السياقي لهم وفقًا لذلك.

### أوامر الكلام
أوامر الكلام هي مجموعة بيانات من الكلمات المنطوقة مصممة لتقييم نماذج تصنيف الصوت على كلمات الأوامر البسيطة. تتكون مجموعة البيانات من 15 فئة من الكلمات الرئيسية، وفئة للصمت، وفئة مجهولة لإدراج الإيجابيات الكاذبة. الكلمات الرئيسية الخمس عشرة هي كلمات مفردة يتم استخدامها عادةً في إعدادات الأجهزة للتحكم في المهام الأساسية أو إطلاق عمليات أخرى.

يعمل نموذج مشابه بشكل مستمر على هاتفك المحمول. هنا، بدلاً من وجود كلمات أوامر مفردة، لدينا "كلمات تنبيه" محددة لجهازك، مثل "مرحبًا جوجل" أو "مرحبًا سيري". عندما يكتشف نموذج تصنيف الصوت كلمات التنبيه هذه، فإنه يطلق هاتفك للاستماع إلى الميكروفون ونقل كلامك باستخدام نموذج التعرف على الكلام.

نموذج تصنيف الصوت أصغر وأخف بكثير من نموذج التعرف على الكلام، وغالبًا ما يحتوي على عدة ملايين من المعلمات مقارنة بمئات الملايين للتعرف على الكلام. وبالتالي، يمكن تشغيله باستمرار على جهازك دون استنزاف بطارية جهازك! فقط عندما يتم اكتشاف كلمة التنبيه يتم تشغيل نموذج التعرف على الكلام الأكبر، وبعد ذلك يتم إيقاف تشغيله مرة أخرى. سنغطي نماذج المحول للتعرف على الكلام في الوحدة التالية، لذا فبحلول نهاية الدورة التدريبية، يجب أن تكون لديك الأدوات التي تحتاجها لبناء مساعدك الصوتي الخاص!

كما هو الحال مع أي مجموعة بيانات على مركز هاجينج فيس هاب، يمكننا أن نحصل على فكرة عن نوع بيانات الصوت الموجودة بها دون تنزيلها أو الالتزام بها في الذاكرة. بعد الانتقال إلى [بطاقة مجموعة بيانات أوامر الكلام](https://huggingface.co/datasets/speech_commands) على المركز، يمكننا استخدام عارض مجموعة البيانات للتنقل خلال أول 100 عينة من مجموعة البيانات، والاستماع إلى ملفات الصوت والتحقق من أي معلومات أخرى خاصة بالبيانات الوصفية:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/speech_commands.png" alt="مخطط لمشاهد مجموعة البيانات.">
</div>

إن المعاينة الأولية هي طريقة رائعة لتجربة مجموعات بيانات الصوت قبل الالتزام باستخدامها. يمكنك اختيار أي مجموعة بيانات على المركز، والتنقل خلال العينات والاستماع إلى الصوت لمختلف المجموعات الفرعية والانقسامات، وتقييم ما إذا كانت مجموعة البيانات مناسبة لاحتياجاتك. بمجرد اختيار مجموعة البيانات، يكون من السهل تحميل البيانات حتى تتمكن من البدء في استخدامها.

دعونا نفعل ذلك بالضبط ونحمل عينة من مجموعة بيانات أوامر الكلام باستخدام وضع البث:

```python
speech_commands = load_dataset(
"speech_commands", "v0.02", split="validation", streaming=True
)
sample = next(iter(speech_commands))
```

سنقوم بتحميل نقطة مرجعية رسمية [Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer) تم ضبطها بدقة على مجموعة بيانات أوامر الكلام، تحت مساحة الاسم ["MIT/ast-finetuned-speech-commands-v2"](https://huggingface.co/MIT/ast-finetuned-speech-commands-v2):

```python
classifier = pipeline(
"audio-classification", model="MIT/ast-finetuned-speech-commands-v2"
)
classifier(sample["audio"].copy())
```

**الإخراج:**

```
[{'score': 0.9999892711639404, 'label': 'backward'},
{'score': 1.7504888774055871e-06, 'label': 'happy'},
{'score': 6.703040185129794e-07, 'label': 'follow'},
{'score': 5.805884484288981e-07, 'label': 'stop'},
{'score': 5.614546694232558e-07, 'label': 'up'}]
```

رائع! يبدو أن المثال يحتوي على كلمة "backward" باحتمالية عالية. يمكننا الاستماع إلى العينة والتأكد من صحة ذلك:

```
from IPython.display import Audio

Audio(sample["audio"]["array"], rate=sample["audio"]["sampling_rate"])
```

الآن، قد تتساءل عن كيفية اختيارنا لهذه النماذج المُدربة مسبقًا لإظهارها في أمثلة تصنيف الصوت هذه. في الواقع، من السهل جدًا العثور على نماذج مُدربة مسبقًا لمجموعة البيانات ومهمتك! أول شيء نحتاج إلى فعله هو الانتقال إلى مركز هاجينج فيس هاب والنقر على علامة تبويب "النماذج": https://huggingface.co/models

سيؤدي ذلك إلى عرض جميع النماذج على مركز هاجينج فيس هاب، مرتبة حسب مرات التنزيل في آخر 30 يومًا:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/all_models.png">
</div>

ستلاحظ على الجانب الأيسر أن لدينا مجموعة من علامات التبويب التي يمكننا اختيارها لتصفية النماذج حسب المهمة أو المكتبة أو مجموعة البيانات، وما إلى ذلك. قم بالتمرير لأسفل وحدد المهمة "تصنيف الصوت" من قائمة مهام الصوت:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/by_audio_classification.png">
</div>

الآن يتم تقديمنا بالمجموعة الفرعية من نماذج تصنيف الصوت التي تزيد عن 500 نموذج على المركز. لزيادة تنقيح هذا الاختيار، يمكننا تصفية النماذج حسب مجموعة البيانات. انقر فوق علامة التبويب "مجموعات البيانات"، وفي مربع البحث اكتب "speech_commands". أثناء الكتابة، سترى خيار `speech_commands` يظهر أسفل علامة التبويب "البحث". يمكنك النقر فوق هذا الزر لتصفية جميع نماذج تصنيف الصوت إلى تلك التي تم ضبطها بدقة على مجموعة بيانات أوامر الكلام:

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/by_speech_commands.png">
</div>

رائع! نرى أن لدينا 6 نماذج مُدربة مسبقًا متاحة لنا لهذه المجموعة من البيانات والمهمة المحددة. ستتعرف على النموذج الأول من هذه النماذج كنقطة مرجعية لنموذج مخطط طيف الصوت الذي استخدمناه في المثال السابق. هذه العملية لتصفية النماذج على المركز هي بالضبط كيف اخترنا نقطة المراقبة لإظهارها لك!

## التعرف على اللغة
التعرف على اللغة (LID) هي مهمة التعرف على اللغة المنطوقة في عينة صوتية من قائمة اللغات المرشحة. يمكن أن يشكل التعرف على اللغة جزءًا مهمًا في العديد من خطوط أنابيب الكلام. على سبيل المثال، بالنظر إلى عينة صوتية بلغة غير معروفة، يمكن استخدام نموذج التعرف على اللغة لتصنيف اللغة (اللغات) المنطوقة في عينة الصوت، ثم اختيار نموذج التعرف على الكلام المناسب المدرب على تلك اللغة لنسخ النص الصوتي للصوت.

### FLEURS
FLEURS (Few-shot Learning Evaluation of Universal Representations of Speech) هي مجموعة بيانات لتقييم أنظمة التعرف على الكلام في 102 لغة، بما في ذلك العديد من اللغات المصنفة على أنها "قليلة الموارد". الق نظرة على بطاقة مجموعة بيانات FLEURS على المركز واستكشف اللغات المختلفة الموجودة: [google/fleurs](https://huggingface.co/datasets/google/fleurs). هل يمكنك العثور على لغتك الأم هنا؟ إذا لم يكن كذلك، فما هي اللغة الأكثر ارتباطًا بها؟

دعونا نحمل عينة من الانقسام التحقق من صحة FLEURS باستخدام وضع البث:

```python
fleurs = load_dataset("google/fleurs", "all", split="validation", streaming=True)
sample = next(iter(fleurs))
```

رائع! الآن يمكننا تحميل نموذج تصنيف الصوت الخاص بنا. لهذا، سنستخدم إصدارًا من [Whisper](https://arxiv.org/pdf/2212.04356.pdf) تم ضبطه بدقة على مجموعة بيانات FLEURS، وهو حاليًا أكثر نماذج التعرف على اللغة أداءً على المركز:

```python
classifier = pipeline(
"audio-classification", model="sanchit-gandhi/whisper-medium-fleurs-lang-id"
)
```

بعد ذلك، يمكننا تمرير الصوت عبر المصنف الخاص بنا وإجراء التنبؤ:

```python
classifier(sample["audio"])
```

**الإخراج:**

```
[{'score': 0.9999330043792725, 'label': 'Afrikaans'},
{'score': 7.093023668858223e-06, 'label': 'Northern-Sotho'},
{'score': 4.269149485480739e-06, 'label': 'Icelandic'},
{'score': 3.2661141631251667e-06, 'label': 'Danish'},
{'score': 3.2580724109720904e-06, 'label': 'Cantonese Chinese'}]
```

يمكننا أن نرى أن النموذج تنبأ بأن الصوت كان باللغة الأفريكانية باحتمالية عالية للغاية (قريبة من 1). تحتوي
## التصنيف الصوتي الصفري 

في النموذج التقليدي لتصنيف الصوت، يتنبأ النموذج بعلامة تصنيف من مجموعة محددة مسبقًا من فئات التصنيف الممكنة. وهذا يفرض حاجزًا أمام استخدام النماذج المعاد تدريبها لتصنيف الصوت، حيث يجب أن تتطابق مجموعة علامات التصنيف الخاصة بالنموذج المعاد تدريبه مع مجموعة علامات التصنيف الخاصة بمهمة التصنيف النهائية. في المثال السابق لتحديد لغة الصوت، يجب أن يتنبأ النموذج بواحدة من 102 فئة لغة التي تم تدريبه عليها. إذا تطلبت مهمة التصنيف النهائية 110 لغات، فلن يتمكن النموذج من التنبؤ بـ 8 من أصل 110 لغات، وبالتالي سيتطلب الأمر إعادة التدريب لتحقيق التغطية الكاملة. وهذا يحد من فعالية التعلم التحويلي لمهمات تصنيف الصوت.

يعد التصنيف الصوتي الصفري طريقة لأخذ نموذج تصنيف صوتي معاد تدريبه ومدرب على مجموعة من الأمثلة الموسومة وتمكينه من تصنيف أمثلة جديدة من فئات غير مرئية سابقًا. دعونا نلقي نظرة على كيفية تحقيق ذلك!

حاليًا، تدعم مكتبة 🤗 Transformers نوعًا واحدًا من النماذج للتصنيف الصوتي الصفري: [نموذج CLAP](https://huggingface.co/docs/transformers/model_doc/clap).

CLAP هو نموذج قائم على المحول الذي يأخذ كل من الصوت والنص كمدخلات، ويحسب التشابه بين الاثنين. إذا أدخلنا نصًا يرتبط ارتباطًا وثيقًا بمدخلات صوتية، فسنحصل على درجة تشابه عالية. وعلى العكس من ذلك، فإن إدخال نص لا علاقة له بالمدخلات الصوتية سيعيد درجة تشابه منخفضة.

يمكننا استخدام تنبؤ التشابه هذا للتصنيف الصوتي الصفري عن طريق تمرير مدخلات صوتية واحدة إلى النموذج وعدة تسميات مرشحة. سيعيد النموذج درجة تشابه لكل تسمية مرشحة، ويمكننا اختيار التسمية التي تحتوي على أعلى درجة كتنبؤنا.

لنأخذ مثالًا نستخدم فيه مدخلات صوتية واحدة من مجموعة بيانات [Environmental Speech Challenge (ESC)](https://huggingface.co/datasets/ashraq/esc50):

```python
dataset = load_dataset("ashraq/esc50", split="train", streaming=True)
audio_sample = next(iter(dataset))["audio"]["array"]
```

بعد ذلك، نقوم بتعريف التسميات المرشحة، والتي تشكل مجموعة تسميات التصنيف الممكنة. سيعيد النموذج احتمال التصنيف لكل تسمية نقوم بتعريفها. وهذا يعني أننا نحتاج إلى معرفة مجموعة التسميات الممكنة مسبقًا في مشكلة التصنيف لدينا، بحيث تحتوي المجموعة على التسمية الصحيحة ويتم تعيين احتمال صالح لها. لاحظ أنه يمكننا إما تمرير مجموعة التسميات الكاملة إلى النموذج، أو مجموعة فرعية مختارة يدويًا نعتقد أنها تحتوي على التسمية الصحيحة. ستكون تمرير مجموعة التسميات الكاملة أكثر شمولاً، ولكن على حساب انخفاض دقة التصنيف لأن مساحة التصنيف أكبر (شريطة أن تكون التسمية الصحيحة هي مجموعة فرعية من التسميات التي اخترناها):

```python
candidate_labels = ["Sound of a dog", "Sound of vacuum cleaner"]
```

يمكننا تشغيل كل من المدخلات الصوتية والتسميات المرشحة من خلال النموذج للعثور على التسمية المرشحة الأكثر تشابهًا مع المدخلات الصوتية:

```python
classifier = pipeline(
task="zero-shot-audio-classification", model="laion/clap-htsat-unfused"
)
classifier(audio_sample, candidate_labels=candidate_labels)
```

**الناتج:**

```
[{'score': 0.9997242093086243, 'label': 'Sound of a dog'}, {'score': 0.0002758323971647769, 'label': 'Sound of vacuum cleaner'}]
```

رائع! يبدو أن النموذج واثق جدًا من أن لدينا صوت كلب - فهو يتنبأ به بنسبة 99.96%، لذا سنأخذ ذلك كتنبؤنا. دعونا نتأكد مما إذا كنا على صواب عن طريق الاستماع إلى العينة الصوتية (لا ترفع مستوى الصوت كثيرًا وإلا فقد تفاجأ بقفزة!):

```python
Audio(audio_sample, rate=1600Multiplier),
```

مثالي! لدينا صوت كلب ينبح 🐕، والذي يتطابق مع تنبؤ النموذج. جرب عينات صوتية وتسميات مرشحة مختلفة - هل يمكنك تحديد مجموعة من التسميات التي توفر تعميمًا جيدًا عبر مجموعة بيانات ESC؟ تلميح: فكر في المكان الذي يمكنك فيه العثور على معلومات حول الأصوات المحتملة في ESC وقم ببناء تسمياتك وفقًا لذلك!

قد تتساءل لماذا لا نستخدم خط أنابيب التصنيف الصوتي الصفري لجميع مهام التصنيف الصوتي؟

يبدو الأمر كما لو أننا نستطيع إجراء تنبؤات لأي مشكلة تصنيف صوتي عن طريق تحديد تسميات فئات مناسبة مسبقًا، وبالتالي تجاوز القيد الذي تنص عليه مهمة التصنيف لدينا بضرورة مطابقة التسميات التي تم تدريب النموذج عليها مسبقًا.

ويرجع ذلك إلى طبيعة نموذج CLAP المستخدم في خط أنابيب التصنيف الصفري: تم تدريب نموذج CLAP مسبقًا على بيانات تصنيف صوتي عامة، مشابهة لل أصوات البيئة في مجموعة بيانات ESC، بدلاً من بيانات الكلام المحددة، مثل التي كانت لدينا في مهمة تحديد لغة الصوت. إذا أعطيته كلامًا باللغة الإنجليزية وكلامًا باللغة الإسبانية، فسيتمكن نموذج CLAP من معرفة أن كلا المثالين كانا بيانات كلام 🗣️ لكنه لن يتمكن من التمييز بين اللغات بنفس الطريقة التي يمكن أن يقوم بها نموذج مخصص لتحديد لغة الصوت.

## ماذا بعد؟

لقد غطينا عددًا من مهام تصنيف الصوت المختلفة وقدمنا أكثر مجموعات البيانات والنماذج ملاءمة التي يمكنك تنزيلها من Hugging Face Hub واستخدامها في بضع سطور من التعليمات البرمجية باستخدام فئة `pipeline()`. تضمنت هذه المهام اكتشاف الكلمات الرئيسية وتحديد لغة الصوت والتصنيف الصوتي الصفري.

ولكن ماذا لو أردنا القيام بشيء **جديد**؟ لقد عملنا بشكل مكثف على مهام معالجة الكلام، ولكن هذا جانب واحد فقط من تصنيف الصوت. يعد **الموسيقى** مجالًا شعبيًا آخر لمعالجة الصوت. في حين أن الموسيقى لها ميزات مختلفة جوهريًا عن الكلام، يمكن تطبيق العديد من المبادئ نفسها التي تعلمناها بالفعل على الموسيقى.

في القسم التالي، سنمر عبر دليل خطوة بخطوة حول كيفية ضبط نموذج محول باستخدام مكتبة 🤗 Transformers لمهمة تصنيف الموسيقى. وبنهاية هذا القسم، سيكون لديك نقطة ضبط دقيقة يمكنك توصيلها بفئة `pipeline()`، مما يتيح لك تصنيف الأغاني بنفس الطريقة التي صنفنا بها الكلام هنا!