# الترجمة من الكلام إلى كلام
الترجمة من الكلام إلى كلام (STST أو S2ST) هي مهمة معالجة اللغة المنطوقة جديدة نسبيًا. وتنطوي على ترجمة الكلام من لغة إلى كلام بلغة أخرى:

يمكن اعتبار STST امتدادًا لمهمة الترجمة الآلية (MT) التقليدية: بدلاً من ترجمة النص من لغة إلى أخرى، نقوم بترجمة الكلام من لغة إلى أخرى. تتمتع STST بتطبيقات في مجال التواصل متعدد اللغات، مما يمكّن المتحدثين بلغات مختلفة من التواصل مع بعضهم البعض من خلال وسيط الكلام.

لنفترض أنك تريد التواصل مع شخص آخر عبر حاجز اللغة. بدلاً من كتابة المعلومات التي تريد نقلها ثم ترجمتها إلى نص باللغة المستهدفة، يمكنك التحدث بها مباشرةً وأن يقوم نظام STST بتحويل كلامك المنطوق إلى اللغة المستهدفة. يمكن للمستلم بعد ذلك الرد عن طريق التحدث مرة أخرى إلى نظام STST، ويمكنك الاستماع إلى رده. هذه طريقة أكثر طبيعية للتواصل مقارنة بالترجمة الآلية القائمة على النص.

في هذا الفصل، سنستكشف نهجًا متتاليًا لـ STST، حيث نجمع بين المعرفة التي اكتسبتها في الوحدة 5 و6 من الدورة. سنستخدم نظام ترجمة الكلام (ST) لنسخ الكلام المصدر إلى نص في اللغة المستهدفة، ثم الترجمة من نص إلى كلام (TTS) لتوليد الكلام في اللغة المستهدفة من النص المترجم:

يمكننا أيضًا استخدام نهج ثلاثي المراحل، حيث نستخدم أولاً نظام التعرف التلقائي على الكلام (ASR) لنسخ الكلام المصدر إلى نص بنفس اللغة، ثم الترجمة الآلية لترجمة النص المنسوخ إلى اللغة المستهدفة، وأخيرًا الترجمة من نص إلى كلام لتوليد الكلام في اللغة المستهدفة. ومع ذلك، فإن إضافة المزيد من المكونات إلى خط الأنابيب يؤدي إلى انتشار الأخطاء، حيث تتفاقم الأخطاء التي يتم تقديمها في نظام واحد أثناء مرورها عبر الأنظمة المتبقية، كما تزيد من زمن الوصول، حيث يجب إجراء الاستدلال لمزيد من النماذج.

على الرغم من أن هذا النهج المتتالي لـ STST بسيط للغاية، إلا أنه يؤدي إلى أنظمة STST فعالة للغاية. تم استخدام نظام ثلاثي المراحل متتالي من ASR + MT + TTS سابقًا لتشغيل العديد من المنتجات التجارية STST، بما في ذلك Google Translate. إنه أيضًا طريقة فعالة من حيث البيانات والحوسبة لتطوير نظام STST، حيث يمكن اقتران أنظمة التعرف على الكلام والترجمة من نص إلى كلام الموجودة لإنتاج نموذج STST جديد دون أي تدريب إضافي.

في الجزء المتبقي من هذه الوحدة، سنركز على إنشاء نظام STST يقوم بترجمة الكلام من أي لغة X إلى كلام باللغة الإنجليزية. يمكن توسيع نطاق الأساليب المشمولة لأنظمة STST التي تترجم من أي لغة X إلى أي لغة Y، ولكننا نترك ذلك كتمديد للقارئ ونقدم مؤشرات حيثما ينطبق ذلك. نقوم بتقسيم مهمة STST أكثر إلى مكونيها المكونين: ST وTTS. سننهي ذلك عن طريق تجميعها معًا لبناء عرض توضيحي لـ Gradio لعرض نظامنا.

## ترجمة الكلام
سنستخدم نموذج Whisper لنظام ترجمة الكلام الخاص بنا، لأنه قادر على الترجمة من أكثر من 96 لغة إلى الإنجليزية. على وجه التحديد، سنقوم بتحميل نقطة تفتيش Whisper Base، والتي يبلغ عدد معلماتها 74 مليون معلمة. إنه ليس بأي حال من الأحوال أكثر نماذج Whisper أداءً، حيث أن أكبر نقطة تفتيش Whisper أكبر من 20 ضعفًا، ولكن نظرًا لأننا نقوم بدمج نظامين تنبؤيين تلقائيًا (ST + TTS)، فإننا نريد التأكد من أن كل نموذج يمكنه التوليد بسرعة نسبية حتى نحصل على سرعة استدلال معقولة:

```python
import torch
from transformers import pipeline

device = "cuda:0" if torch.cuda.is_available() else "cpu"
pipe = pipeline(
"automatic-speech-recognition", model="openai/whisper-base", device=device
)
```

رائع! لاختبار نظام STST الخاص بنا، سنقوم بتحميل عينة صوتية بلغة غير الإنجليزية. دعنا نحمل المثال الأول من الشق الإيطالي (`it`) لمجموعة بيانات VoxPopuli:

```python
from datasets import load_dataset

dataset = load_dataset("facebook/voxpopuli", "it", split="validation", streaming=True)
sample = next(iter(dataset))
```

لسماع هذه العينة، يمكننا تشغيلها باستخدام عارض المجموعة على Hub: facebook/voxpopuli/viewer

أو تشغيل باستخدام ميزة الصوت ipynb:

```python
from IPython.display import Audio

Audio(sample["audio"]["array"], rate=sample["audio"]["sampling_rate"])
```

الآن دعنا نحدد دالة تأخذ هذا الإدخال الصوتي وتعيد النص المترجم. ستتذكر أنه يتعين علينا تمرير وسيط الإنشاء الخاص بالمهمة، وتعيينه على "الترجمة" لضمان قيام Whisper بأداء ترجمة الكلام وليس التعرف على الكلام:

```python
def translate(audio):
outputs = pipe(audio, max_new_tokens=256, generate_kwargs={"task": "translate"})
return outputs["text"]
```

يمكن أيضًا "خداع" Whisper لترجمة الكلام من أي لغة X إلى أي لغة Y. قم ببساطة بتعيين المهمة على "نسخ" واللغة إلى لغتك المستهدفة في وسيطات التوليد، على سبيل المثال، بالنسبة للإسبانية، يمكنك تعيين ما يلي:

`generate_kwargs={"task": "transcribe", "language": "es"}`

رائع! دعنا نتأكد بسرعة من أننا نحصل على نتيجة معقولة من النموذج:

```python
translate(sample["audio"].copy())
```

```
'نفسي والاجتماعي. أعتقد أن هذه خطوة مهمة جدًا في بناء مساحة قانونية للحرية وحركة وحماية الحقوق.'
```

حسنًا! إذا قارنا ذلك بالنص المصدر:

```python
sample["raw_text"]
```

```
'Penso che questo sia un passo in avanti importante nella costruzione di uno spazio giuridico di libertà di circolazione e di protezione dei diritti per le persone in Europa.'
```

سنرى أن الترجمة تتماشى تقريبًا (يمكنك التحقق المزدوج باستخدام Google Translate)، باستثناء بضع كلمات إضافية في بداية النسخة المكتوبة حيث كان المتحدث ينهي جملته السابقة.

بهذا نكون قد أكملنا النصف الأول من خط أنابيب STST المتتالي لدينا، ووضعنا في الممارسة المهارات التي اكتسبناها في الوحدة 5 عندما تعلمنا كيفية استخدام نموذج Whisper للتعرف على الكلام والترجمة. إذا كنت تريد مراجعة أي من الخطوات التي تمت تغطيتها، فاقرأ القسم الخاص بـ [نماذج مسبقة التدريب للتعرف على الكلام](../chapter5/asr_models) من الوحدة 5.

## الترجمة من نص إلى كلام
ينطوي النصف الثاني من نظام STST المتتالي الخاص بنا على رسم خريطة من النص الإنجليزي إلى الكلام الإنجليزي. لهذا، سنستخدم النموذج المسبق التدريب SpeechT5 TTS للترجمة من نص إلى كلام باللغة الإنجليزية. لا يحتوي 🤗 Transformers حاليًا على خط أنابيب TTS، لذا سيتعين علينا استخدام النموذج مباشرة بأنفسنا. هذا ليس بالأمر الكبير، فأنت جميعًا خبراء في استخدام النموذج للاستدلال بعد الوحدة 6!

أولاً، دعنا نقوم بتحميل معالج SpeechT5 والنموذج ومولد الصوت من نقطة التفتيش المسبقة التدريب:

```python
from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan

processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")

model = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts")
vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")
```

نستخدم هنا نقطة تفتيش SpeechT5 المدربة خصيصًا للترجمة من نص إلى كلام باللغة الإنجليزية. إذا كنت ترغب في الترجمة إلى لغة أخرى غير الإنجليزية، فقم إما بالتبديل إلى نقطة تفتيش SpeechT5 TTS المدربة على لغة اختيارك، أو استخدم نقطة تفتيش MMS TTS مسبقة التدريب بلغتك المستهدفة.

كما هو الحال مع نموذج Whisper، سنضع نموذج SpeechT5 ومولد الصوت على جهاز التسريع GPU الخاص بنا إذا كان لدينا واحد:

```python
model.to(device)
vocoder.to(device)
```

رائع! دعنا نحمل الآن تضمين المتحدث:

```python
embeddings_dataset = load_dataset("Matthijs/cmu-arctic-xvectors", split="validation")
speaker_embeddings = torch.tensor(embeddings_dataset[7306]["xvector"]).unsqueeze(0)
```

الآن يمكننا كتابة دالة تأخذ كإدخال موجه نصي، وتولد الكلام المقابل. سنقوم أولاً بمعالجة إدخال النص باستخدام معالج SpeechT5، وتقسيم النص للحصول على معرفات الإدخال الخاصة بنا. بعد ذلك، سنمرر معرفات الإدخال وتضمين المتحدث إلى نموذج SpeechT5، ووضع كل منهما على جهاز التسريع إذا كان متاحًا. أخيرًا، سنعيد الكلام المولد، ونعيده إلى وحدة المعالجة المركزية حتى نتمكن من تشغيله مرة أخرى في دفتر ملاحظات ipynb الخاص بنا:

```python
def synthesise(text):
inputs = processor(text=text, return_tensors="pt")
speech = model.generate_speech(
inputs["input_ids"].to(device), speaker_embeddings.to(device), vocoder=vocoder
)
return speech.cpu()
```

دعنا نتأكد من أنه يعمل بإدخال نص وهمي:

```python
speech = synthesise("Hey there! This is a test!")

Audio(speech, rate=16000)
```

يبدو جيدًا! الآن للجزء المثير - تجميع كل شيء معًا.

## إنشاء عرض توضيحي لـ STST
قبل إنشاء عرض توضيحي لـ [Gradio](https://gradio.app) لعرض نظام STST الخاص بنا، دعنا نقوم أولاً بفحص سريع للتأكد من أننا يمكننا دمج النموذجين، ووضع عينة صوتية والحصول على عينة صوتية. سنفعل ذلك عن طريق دمج الدالتين اللتين حددناهما في القسمين الفرعيين السابقين، بحيث نقوم بإدخال الصوت المصدر واسترداد النص المترجم، ثم نقوم بتركيب النص المترجم للحصول على الكلام المترجم. أخيرًا، سنقوم بتحويل الكلام المولَّد إلى مصفوفة `int16`، وهو تنسيق ملف الإخراج الصوتي الذي يتوقعه Gradio. للقيام بذلك، يجب علينا أولاً تطبيع مصفوفة الصوت بواسطة النطاق الديناميكي لنوع البيانات المستهدف (`int16`)، ثم تحويله من نوع بيانات NumPy الافتراضي (`float64`) إلى نوع البيانات المستهدف (`int16`):

```python
import numpy as np

target_dtype = np.int16
max_range = np.iinfo(target_dtype).max


def speech_to_speech_translation(audio):
translated_text = translate(audio)
synthesised_speech = synthesise(translated_text)
synthesised_speech = (synthesised_speech.numpy() * max_range).astype(np.int16)
return 16000, synthesised_speech
```

دعنا نتأكد من أن هذه الدالة المتصلة تعطي النتيجة المتوقعة:

```python
sampling_rate, synthesised_speech = speech_to_speech_translation(sample["audio"])

Audio(synthesised_speech, rate=sampling_rate)
```

مثالي! الآن سنقوم بتغليف هذا في عرض توضيحي لطيف لـ Gradio حتى نتمكن من تسجيل كلام المصدر الخاص بنا باستخدام إدخال الميكروفون أو إدخال الملف وتشغيل تنبؤ النظام:

```python
import gradio as gr

demo = gr.Blocks()

mic_translate = gr.Interface(
fn=speech_to_speech_translation,
inputs=gr.Audio(source="microphone", type="filepath"),
outputs=gr.Audio(label="Generated Speech", type="numpy"),
)

file_translate = gr.Interface(
fn=speech_to_speech_translation,
inputs=gr.Audio(source="upload", type="filepath"),
outputs=gr.Audio(label="Generated Speech", type="numpy"),
)

with demo:
gr.TabbedInterface([mic_translate, file_translate], ["Microphone", "Audio File"])

demo.launch(debug=True)
```

سيؤدي هذا إلى تشغيل عرض توضيحي لـ Gradio مشابه للعرض التوضيحي الذي يعمل على مساحة Hugging Face:

يمكنك [استنساخ](https://huggingface.co/spaces/course-demos/speech-to-speech-translation?duplicate=true) هذا العرض التوضيحي وتكييفه لاستخدام نقطة تفتيش Whisper مختلفة، أو نقطة تفتيش TTS مختلفة، أو تخفيف قيد إخراج الكلام باللغة الإنجليزية واتباع النصائح المقدمة لترجمة اللغة التي تختارها!
## المضي قدماً:

على الرغم من أن النظام المتسلسل طريقة فعالة من حيث الحساب والبيانات لبناء نظام تحويل الكلام إلى نص، إلا أنه يعاني من مشكلات انتشار الأخطاء والانحراف الإضافي المذكورة أعلاه. وقد استكشفت الأعمال الحديثة نهجًا "مباشرًا" لتحويل الكلام إلى نص، وهو نهج لا يتنبأ بنص إخراج وسيط، بل يقوم بدلاً من ذلك بالتعيين مباشرة من الكلام المصدر إلى الكلام الهدف. وتتمتع هذه الأنظمة أيضًا بالقدرة على الاحتفاظ بخصائص المتحدث المصدر في الكلام الهدف (مثل التنغيم والطبقة والنبرة). إذا كنت ترغب في معرفة المزيد عن هذه الأنظمة، تحقق من الموارد المدرجة في القسم الخاص بالقراءة التكميلية.