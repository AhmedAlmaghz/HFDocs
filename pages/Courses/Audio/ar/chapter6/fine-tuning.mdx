# ضبط نموذج SpeechT5 الدقيق

الآن بعد أن تعرفت على مهمة تحويل النص إلى كلام والأجزاء الداخلية لنموذج SpeechT5 الذي تم تدريبه مسبقًا على بيانات اللغة الإنجليزية، دعونا نرى كيف يمكننا ضبطه بدقة على لغة أخرى.

## الأعمال المنزلية

تأكد من أن لديك وحدة معالجة رسومية (GPU) إذا كنت تريد إعادة إنتاج هذا المثال. في دفتر ملاحظات، يمكنك التحقق من ذلك باستخدام الأمر التالي:

```bash
nvidia-smi
```

<Tip warning={true}>

في مثالنا، سنستخدم ما يقرب من 40 ساعة من بيانات التدريب. إذا كنت ترغب في المتابعة باستخدام GPU من الطبقة المجانية في Google Colab، فستحتاج إلى تقليل كمية بيانات التدريب إلى ما يقرب من 10-15 ساعة، وتقليل عدد خطوات التدريب.

</Tip>

ستحتاج أيضًا إلى بعض التبعيات الإضافية:

```bash
pip install transformers datasets soundfile speechbrain accelerate
```

أخيرًا، لا تنس تسجيل الدخول إلى حساب Hugging Face الخاص بك حتى تتمكن من تحميل نموذجك ومشاركته مع المجتمع:

```py
from huggingface_hub import notebook_login

notebook_login()
```

## مجموعة البيانات

بالنسبة لهذا المثال، سنأخذ الجزء الفرعي للغة الهولندية (`nl`) من مجموعة بيانات [VoxPopuli](https://huggingface.co/datasets/facebook/voxpopuli).

[VoxPopuli](https://huggingface.co/datasets/facebook/voxpopuli) هي مجموعة بيانات صوتية متعددة اللغات واسعة النطاق تتكون من بيانات مستخرجة من تسجيلات أحداث البرلمان الأوروبي من عام 2009 إلى عام 2020. يحتوي على بيانات صوتية-نصية موسومة لـ 15 لغة أوروبية. بينما سنستخدم الجزء الفرعي للغة الهولندية، فلا تتردد في اختيار جزء فرعي آخر.

هذه مجموعة بيانات التعرف التلقائي على الكلام (ASR)، لذلك، كما ذكرنا سابقًا، فهي ليست الخيار الأنسب لتدريب نماذج TTS. ومع ذلك، سيكون جيدًا بما يكفي لهذا التمرين.

دعونا نحمل البيانات:

```python
from datasets import load_dataset, Audio

dataset = load_dataset("facebook/voxpopuli", "nl", split="train")
len(dataset)
```

**الإخراج:**

```out
20968
```

20968 مثال يجب أن يكون كافيًا للضبط الدقيق. يتوقع SpeechT5 أن يكون لمجموعة البيانات معدل أخذ عينات يبلغ 16 كيلو هرتز، لذا تأكد من أن الأمثلة في مجموعة البيانات تلبي هذا الشرط:

```python
dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
```

## معالجة البيانات

دعونا نبدأ من خلال تحديد نقطة تفتيش النموذج لاستخدامها وتحميل المعالج المناسب الذي يحتوي على كل من الرموز، ومستخرج الميزات الذي سنحتاجه لإعداد البيانات للتدريب:

```py
from transformers import SpeechT5Processor

checkpoint = "microsoft/speecht5_tts"
processor = SpeechT5Processor.from_pretrained(checkpoint)
```

### تنظيف النص لرموز SpeechT5

أولاً، لإعداد النص، سنحتاج إلى الجزء الرمزي من المعالج، لذا دعونا نحصل عليه:

```py
tokenizer = processor.tokenizer
```

دعونا نلقي نظرة على مثال:

```python
dataset[0]
```

**الإخراج:**

```out
{'audio_id': '20100210-0900-PLENARY-3-nl_20100210-09:06:43_4',
'language': 9،
'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/02ec6a19d5b97c03e1379250378454dbf3fa2972943504a91c7da5045aa26a89/train_part_0/20100210-0900-PLENARY-3-nl_20100210-09:06:43_4.wav',
'array': array([ 4.27246094e-04,  1.31225586e-03,  1.03759766e-03, ...,
-9.15527344e-05,  7.62939453e-04, -2.44140625e-04]),
'sampling_rate': 16000},
'raw_text': 'Dat kan naar mijn gevoel alleen met een brede meerderheid die wij samen zoeken.',
'normalized_text': 'dat kan naar mijn gevoel alleen met een brede meerderheid die wij samen zoeken.',
'gender': 'female',
'speaker_id': '1122',
'is_gold_transcript': True,
'accent': 'None'}
```

ما قد تلاحظه هو أن أمثلة مجموعة البيانات تحتوي على ميزات `raw_text` و`normalized_text`. عند تحديد الميزة التي سيتم استخدامها كإدخال نصي، سيكون من المهم معرفة أن رموز SpeechT5 ليس لها أي رموز للأرقام. في `normalized_text`، يتم كتابة الأرقام كنص. وبالتالي، فهو مناسب بشكل أفضل، ويجب أن نستخدم `normalized_text` كنص إدخال.

بما أن SpeechT5 تم تدريبه على اللغة الإنجليزية، فقد لا يتعرف على أحرف معينة في مجموعة البيانات الهولندية. إذا تُركت كما هي، فسيتم تحويل هذه الأحرف إلى رموز `<unk>`. ومع ذلك، في اللغة الهولندية، يتم استخدام أحرف معينة مثل `à` للتشديد على المقاطع. للحفاظ على معنى النص، يمكننا استبدال هذا الحرف بـ `a` عادي.

لتحديد الرموز غير المدعومة، استخرج جميع الأحرف الفريدة في مجموعة البيانات باستخدام `SpeechT5Tokenizer` الذي يعمل مع الأحرف كرموز. للقيام بذلك، سنكتب دالة `extract_all_chars` التي تقوم بدمج النسخ النصية من جميع الأمثلة في سلسلة واحدة وتحويلها إلى مجموعة من الأحرف.

تأكد من تعيين `batched=True` و`batch_size=-1` في `dataset.map()` بحيث تكون جميع النسخ النصية متاحة مرة واحدة لدالة الخريطة.

```py
def extract_all_chars(batch):
all_text = " ".join(batch["normalized_text"])
vocab = list(set(all_text))
return {"vocab": [vocab], "all_text": [all_text]}


vocabs = dataset.map(
extract_all_chars,
batched=True,
batch_size=-1,
keep_in_memory=True,
remove_columns=dataset.column_names,
)

dataset_vocab = set(vocabs["vocab"][0])
tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}
```

الآن لديك مجموعتان من الأحرف: واحدة مع المفردات من مجموعة البيانات والأخرى مع المفردات من الرمز.

لتحديد أي أحرف غير مدعومة في مجموعة البيانات، يمكنك أخذ الفرق بين هاتين المجموعتين. ستتضمن المجموعة الناتجة الأحرف الموجودة في مجموعة البيانات ولكن ليس في الرمز.

```py
dataset_vocab - tokenizer_vocab
```

**الإخراج:**

```out
{' '، 'à'، 'ç'، 'è'، 'ë'، 'í'، 'ï'، 'ö'، 'ü'}
```

للتعامل مع الأحرف غير المدعومة التي تم تحديدها في الخطوة السابقة، يمكننا تحديد دالة تقوم بتعيين هذه الأحرف إلى رموز صالحة. لاحظ أن المسافات يتم استبدالها بالفعل بـ `▁` في الرمز ولا تحتاج إلى التعامل معها بشكل منفصل.

```py
replacements = [
("à"، "a")،
("ç"، "c")،
("è"، "e")،
("ë"، "e")،
("í"، "i")،
("ï"، "i")،
("ö"، "o")،
("ü"، "u")،
]


def cleanup_text(inputs):
for src، dst in replacements:
inputs["normalized_text"] = inputs["normalized_text"].replace(src، dst)
return inputs


dataset = dataset.map(cleanup_text)
```

الآن بعد أن تعاملنا مع الأحرف الخاصة في النص، حان الوقت للتركيز على بيانات الصوت.

### المتحدثون

تتضمن مجموعة بيانات VoxPopuli خطابات من متحدثين متعددين، ولكن كم عدد المتحدثين الممثلين في مجموعة البيانات؟ لتحديد ذلك، يمكننا حساب عدد المتحدثين الفريدين وعدد الأمثلة التي يساهم بها كل متحدث في مجموعة البيانات. بمعرفة أن هناك ما مجموعه 20968 مثالًا في مجموعة البيانات، ستمنحنا هذه المعلومات فهمًا أفضل لتوزيع المتحدثين والأمثلة في البيانات.

```py
from collections import defaultdict

speaker_counts = defaultdict(int)

for speaker_id in dataset["speaker_id"]:
speaker_counts[speaker_id] += 1
```

من خلال رسم مخطط توزيع تكراري، يمكنك الحصول على فكرة عن مقدار البيانات المتوفرة لكل متحدث.

```py
import matplotlib.pyplot as plt

plt.figure()
plt.hist(speaker_counts.values(), bins=20)
plt.ylabel("المتحدثون")
plt.xlabel ("أمثلة")
plt.show()
```

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/tts_speakers_histogram.png" alt="مخطط توزيع تكراري المتحدثين"/>
</div>

يكشف مخطط التوزيع التكراري أن حوالي ثلث المتحدثين في مجموعة البيانات لديهم أقل من 100 مثال، في حين أن حوالي عشرة متحدثين لديهم أكثر من 500 مثال. لتحسين كفاءة التدريب وتحقيق التوازن في مجموعة البيانات، يمكننا الحد من البيانات إلى متحدثين يتراوح عددهم بين 100 و400 مثال.

```py
def select_speaker(speaker_id):
return 100 <= speaker_counts[speaker_id] <= 400


dataset = dataset.filter(select_speaker, input_columns=["speaker_id"])
```

دعونا نتحقق من عدد المتحدثين المتبقين:

```py
len(set(dataset["speaker_id"]))
```

**الإخراج:**

```out
42
```

دعونا نرى عدد الأمثلة المتبقية:

```py
len(dataset)
```

**الإخراج:**

```out
9973
```

تبقى لديك أقل بقليل من 10000 مثال من حوالي 40 متحدثًا فريدًا، وهو ما يجب أن يكون كافيًا.

لاحظ أن بعض المتحدثين الذين لديهم أمثلة قليلة قد يكون لديهم في الواقع المزيد من الصوت المتاح إذا كانت الأمثلة طويلة. ومع ذلك، فإن تحديد إجمالي مقدار الصوت لكل متحدث يتطلب فحص مجموعة البيانات بأكملها، وهي عملية تستغرق وقتًا طويلاً تنطوي على تحميل وفك تشفير كل ملف صوتي. لذلك، اخترنا تخطي هذه الخطوة هنا.

### تضمين المتحدثين

لتمكين نموذج TTS من التمييز بين عدة متحدثين، ستحتاج إلى إنشاء تضمين متحدث لكل مثال. تضمين المتحدث هو إدخال إضافي في النموذج الذي يلتقط خصائص صوت متحدث معين.

لإنشاء هذه التضمينات المتحدث، استخدم نموذج [spkrec-xvect-voxceleb](https://huggingface.co/speechbrain/spkrec-xvect-voxceleb) المسبق التدريب من SpeechBrain.

قم بإنشاء دالة `create_speaker_embedding()` التي تأخذ موجة صوت إدخال وتخرج متجهًا مكونًا من 512 عنصرًا يحتوي على تضمين المتحدث المقابل.

```py
import os
import torch
from speechbrain.pretrained import EncoderClassifier

spk_model_name = "speechbrain/spkrec-xvect-voxceleb"

device = "cuda" if torch.cuda.is_available() else "cpu"
speaker_model = EncoderClassifier.from_hparams(
source=spk_model_name,
run_opts={"device": device},
savedir=os.path.join("/tmp"، spk_model_name)،
)


def create_speaker_embedding(waveform):
with torch.no_grad():
speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))
speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)
speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()
return speaker_embeddings
```

من المهم ملاحظة أن نموذج `speechbrain/spkrec-xvect-voxceleb` تم تدريبه على خطاب اللغة الإنجليزية من مجموعة بيانات VoxCeleb، في حين أن أمثلة التدريب في هذا الدليل باللغة الهولندية. في حين نعتقد أن هذا النموذج سيولد مع ذلك تضمينات متحدث معقولة لمجموعة البيانات الهولندية الخاصة بنا، فقد لا يكون هذا الافتراض صحيحًا في جميع الحالات.

للحصول على نتائج مثالية، سنحتاج إلى تدريب نموذج X-vector على الكلام المستهدف أولاً. سيكفل ذلك قدرة النموذج على التقاط خصائص الصوت الفريدة الموجودة في اللغة الهولندية بشكل أفضل. إذا كنت ترغب في تدريب نموذج X-vector الخاص بك، فيمكنك استخدام [هذا البرنامج النصي](https://huggingface.co/mechanicalsea/speecht5-vc/blob/main/manifest/utils/prep_cmu_arctic_spkemb.py) كمثال.

### معالجة مجموعة البيانات

أخيرًا، دعونا نقوم بمعالجة البيانات بتنسيق النموذج المتوقع. قم بإنشاء دالة `prepare_dataset` تأخذ مثالًا واحدًا وتستخدم كائن `SpeechT5Processor` لرموز تسلسل الإدخال النصي وتحميل الصوت المستهدف في مخطط Mel-spectrogram. يجب أن تضيف أيضًا تضمينات المتحدثين كإدخال إضافي.

```py
def prepare_dataset(example):
audio = example["audio"]

example = processor(
text=example["normalized_text"],
audio_target=audio["array"],
sampling_rate=audio["sampling_rate"],
return_attention_mask=False,
)

# قم بإزالة البعد الدفعي
example["labels"] = example["labels"][0]

# استخدم SpeechBrain للحصول على x-vector
example["speaker_embeddings"] = create_speaker_embedding(audio["array"])

return example
```

تحقق من صحة المعالجة عن طريق إلقاء نظرة على مثال واحد:

```py
processed_example = prepare_dataset(dataset[0])
list(processed_example.keys())
```

**الإخراج:**

```out
['input_ids'، 'labels'، 'stop_labels'، 'speaker_embeddings']
```

يجب أن تكون تضمينات المتحدثين عبارة عن متجه مكون من 512 عنصرًا:

```py
processed_example["speaker_embeddings"].shape
```

**الإخراج:**

```out
(512،)
```

يجب أن تكون التصنيفات مخطط Mel-spectrogram مع 80 نافذة Mel.

```py
import matplotlib.pyplot as plt

plt.figure()
plt.imshow(processed_example["labels"].T)
plt.show()
```

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/tts_logmelspectrogram_1.png" alt="مخطط Mel-spectrogram مع 80 نافذة Mel"/>
</div>

ملاحظة جانبية: إذا وجدت هذا المخطط الطيفي محيرًا، فقد يكون ذلك بسبب معرفتك باتفاقية وضع الترددات المنخفضة في الأسفل والترددات العالية في الجزء العلوي من المخطط. ومع ذلك، عند رسم المخططات الطيفية كصورة باستخدام مكتبة matplotlib، يتم قلب المحور y ويظهر المخطط الطيفي مقلوبًا
## مجمع البيانات

لدمج عدة أمثلة في دفعة واحدة، يجب عليك تحديد مجمع بيانات مخصص. وسيقوم هذا المجمع بملء التسلسلات الأقصر برموز الملء، مما يضمن أن يكون لجميع الأمثلة نفس الطول. وبالنسبة لعلامات طيف تردد، يتم استبدال الأجزاء المملوءة بالقيمة الخاصة "-100". وتوجه هذه القيمة الخاصة النموذج إلى تجاهل هذا الجزء من طيف الترددات عند حساب خسارة طيف الترددات.

```py
from dataclasses import dataclass
from typing import Any, Dict, List, Union

@dataclass
class TTSDataCollatorWithPadding:
    processor: Any

    def __call__(
        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        input_ids = [{"input_ids": feature["input_ids"]} for feature in features]
        label_features = [{"input_values": feature["labels"]} for feature in features]
        speaker_features = [feature["speaker_embeddings"] for feature in features]

        # تجميع المدخلات والأهداف في دفعة
        batch = processor.pad(
            input_ids=input_ids, labels=label_features, return_tensors="pt"
        )

        # استبدل الحشو بـ -100 لتجاهل الخسارة بشكل صحيح
        batch["labels"] = batch["labels"].masked_fill(
            batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100
        )

        # غير مستخدم أثناء الضبط الدقيق
        del batch["decoder_attention_mask"]

        # قم بتقريب أطوال الهدف لأسفل إلى عدد صحيح من عامل التخفيض
        if model.config.reduction_factor > 1:
            target_lengths = torch.tensor(
                [len(feature["input_values"]) for feature in label_features]
            )
            target_lengths = target_lengths.new(
                [
                    length - length % model.config.reduction_factor
                    for length in target_lengths
                ]
            )
            max_length = max(target_lengths)
            batch["labels"] = batch["labels"][:, :max_length]

        # أضف أيضًا تضمين المتحدث
        batch["speaker_embeddings"] = torch.tensor(speaker_features)

        return batch
```

في SpeechT5، يتم تقليل الإدخال إلى الجزء فك التشفير من النموذج بعامل 2. وبعبارة أخرى، فإنه يتخلص من كل خطوة زمنية أخرى من التسلسل المستهدف. ثم يقوم فك التشفير بالتنبؤ بتسلسل يبلغ ضعف الطول. نظرًا لأنه قد يكون للتسلسل المستهدف الأصلي طول فردي، فإن مجمع البيانات يتأكد من تقريب الطول الأقصى للدفعة لأسفل ليكون مضاعفًا لـ 2.

```py
data_collator = TTSDataCollatorWithPadding(processor=processor)
```

## تدريب النموذج

قم بتحميل النموذج المُدرب مسبقًا من نفس نقطة التفتيش التي استخدمتها لتحميل المعالج:

```py
from transformers import SpeechT5ForTextToSpeech

model = SpeechT5ForTextToSpeech.from_pretrained(checkpoint)
```

خيار `use_cache=True` غير متوافق مع تسجيل التدرج. قم بتعطيله للتدريب، وقم بتمكين ذاكرة التخزين المؤقت للتخليق لزيادة سرعة وقت الاستدلال:

```py
from functools import partial

# تعطيل ذاكرة التخزين المؤقت أثناء التدريب لأنه غير متوافق مع تسجيل التدرج
model.config.use_cache = False

# تعيين اللغة والمهمة للتخليق وإعادة تمكين ذاكرة التخزين المؤقت
model.generate = partial(model.generate, use_cache=True)
```

قم بتعريف الحجج التدريبية. هنا، لا نقوم بحساب أي مقاييس تقييم أثناء عملية التدريب، وسنتحدث عن التقييم لاحقًا في هذا الفصل. بدلاً من ذلك، سننظر فقط في الخسارة:

```python
from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="speecht5_finetuned_voxpopuli_nl"،  # تغيير إلى اسم مستودع من اختيارك
    per_device_train_batch_size=4،
    gradient_accumulation_steps=8،
    learning_rate=1e-5،
    warmup_steps=500،
    max_steps=4000،
    gradient_checkpointing=True،
    fp16=True،
    evaluation_strategy="steps"،
    per_device_eval_batch_size=2،
    save_steps=1000،
    eval_steps=1000،
    logging_steps=25،
    report_to=["tensorboard"]،
    load_best_model_at_end=True،
    greater_is_better=False،
    label_names=["labels"]،
    push_to_hub=True،
)
```

قم بتنفيذ كائن `Trainer` ومرر النموذج ومجموعة البيانات ومجمع البيانات إليه:

```py
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=dataset["train"]،
    eval_dataset=dataset["test"]،
    data_collator=data_collator،
    tokenizer=processor،
)
```

وبذلك، نكون مستعدين لبدء التدريب! سيستغرق التدريب عدة ساعات. اعتمادًا على وحدة معالجة الرسومات (GPU) الخاصة بك، من المحتمل أن تواجه خطأ "نفاد الذاكرة" CUDA عند بدء التدريب. في هذه الحالة، يمكنك تقليل `per_device_train_batch_size` بشكل تدريجي عن طريق عوامل 2 وزيادة `gradient_accumulation_steps` بمقدار 2x للتعويض.

```py
trainer.train()
```

قم بدفع النموذج النهائي إلى 🤗 Hub:

```py
trainer.push_to_hub()
```

## الاستنتاج

بمجرد ضبط نموذج بدقة، يمكنك استخدامه للاستنتاج! قم بتحميل النموذج من 🤗 Hub (تأكد من استخدام اسم حسابك في مقتطف التعليمات البرمجية التالي):

```py
model = SpeechT5ForTextToSpeech.from_pretrained(
    "YOUR_ACCOUNT/speecht5_finetuned_voxpopuli_nl"
)
```

اختر مثالاً، هنا سنأخذ واحدة من مجموعة بيانات الاختبار. احصل على تضمين المتحدث.

```py
example = dataset["test"][304]
speaker_embeddings = torch.tensor(example["speaker_embeddings"]).unsqueeze(0)
```

قم بتعريف نص الإدخال وتقسيمه إلى رموز:

```py
text = "hallo allemaal, ik praat nederlands. groetjes aan iedereen!"
```

قم بمعالجة نص الإدخال:

```py
inputs = processor(text=text, return_tensors="pt")
```

قم بتنفيذ فك التشفير وولّد الكلام:

```py
from transformers import SpeechT5HifiGan

vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")
speech = model.generate_speech(inputs["input_ids"], speaker_embeddings, vocoder=vocoder)
```

هل أنت مستعد للاستماع إلى النتيجة؟

```py
from IPython.display import Audio

Audio(speech.numpy(), rate=16000)
```

يمكن أن يكون الحصول على نتائج مرضية من هذا النموذج في لغة جديدة أمرًا صعبًا. يمكن أن تكون جودة تضمين المتحدث عاملاً مهمًا. نظرًا لأن SpeechT5 تم تدريبه مسبقًا باستخدام x-vectors الإنجليزية، فإنه يعمل بشكل أفضل عند استخدام تضمين المتحدث باللغة الإنجليزية. إذا كان الكلام المُخلَّق يبدو سيئًا، فجرّب استخدام تضمين متحدث مختلف.

ومن المرجح أيضًا أن يؤدي زيادة مدة التدريب إلى تحسين جودة النتائج. ومع ذلك، فإن الكلام باللغة الهولندية بدلاً من اللغة الإنجليزية، ويتم بالفعل التقاط خصائص صوت المتحدث (مقارنة بالصوت الأصلي في المثال).

شيء آخر يمكن تجربته هو تكوين النموذج. على سبيل المثال، جرب استخدام `config.reduction_factor = 1` لمعرفة ما إذا كان هذا يحسن النتائج.

في القسم التالي، سنتحدث عن كيفية تقييم نماذج تحويل النص إلى كلام.