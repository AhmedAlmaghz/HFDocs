# Intelยฎ Extension for PyTorch

[IPEX](https://github.com/intel/intel-extension-for-pytorch) ุชู ุชุญุณููู ูููุนุงูุฌุงุช ุงูุชู ุชุณุชุฎุฏู AVX-512 ุฃู ุฃุนููุ ููุนูู ุจุดูู ูุธููู ูููุนุงูุฌุงุช ุงูุชู ุชุณุชุฎุฏู AVX2 ููุท. ูุฐููุ ูู ุงููุชููุน ุฃู ูุญูู ูุงุฆุฏุฉ ูู ุงูุฃุฏุงุก ูุฃุฌูุงู ูุนุงูุฌุงุช Intel ูุน AVX-512 ุฃู ุฃุนููุ ูู ุญูู ุฃู ุงููุนุงูุฌุงุช ุงูุชู ุชุณุชุฎุฏู AVX2 ููุท (ูุซู ูุนุงูุฌุงุช AMD ุฃู ูุนุงูุฌุงุช Intel ุงูุฃูุฏู) ูุฏ ุชุคุฏู ุฅูู ุฃุฏุงุก ุฃูุถู ูุน IPEXุ ูููู ุฐูู ุบูุฑ ูุถููู. ูููุฑ IPEX ุชุญุณููุงุช ุฃุฏุงุก ูุชุฏุฑูุจ ุงููุนุงูุฌ ุจุงุณุชุฎุฏุงู ูู ูู Float32 ู BFloat16. ููุนุฏ ุงุณุชุฎุฏุงู BFloat16 ูู ูุญูุฑ ุงูุชุฑููุฒ ุงูุฑุฆูุณู ูููุฑูุน ุงูุชุงููุฉ.

ุชู ุฏุนู ููุน ุงูุจูุงูุงุช ููุฎูุถ ุงูุฏูุฉ BFloat16 ุจุดูู ุฃุตูู ุนูู ูุนุงูุฌุงุช Xeonยฎ Scalable ูู ุงูุฌูู ุงูุซุงูุซ (ุงููุนุฑููุฉ ุจุงุณู Cooper Lake) ูุน ูุฌููุนุฉ ุชุนูููุงุช AVX512ุ ูุณูุชู ุฏุนููุง ูู ุงูุฌูู ุงูุชุงูู ูู ูุนุงูุฌุงุช Intelยฎ Xeonยฎ Scalable ูุน ูุฌููุนุฉ ุชุนูููุงุช Intelยฎ Advanced Matrix Extensions (Intelยฎ AMX) ูุน ุชุนุฒูุฒ ุงูุฃุฏุงุก ุจุดูู ุฃูุจุฑ. ุชู ุชูููู ุงูุฏูุฉ ุงููุฎุชูุทุฉ ุงูุชููุงุฆูุฉ ูุฎูููุฉ ุงููุนุงูุฌ ููุฐ PyTorch-1.10. ููู ุงูููุช ููุณูุ ุชู ุชูููู ุฏุนู ุงูุฏูุฉ ุงููุฎุชูุทุฉ ุงูุชููุงุฆูุฉ ูุน BFloat16 ูููุนุงูุฌ ูBFloat16 ูุชุญุณูู ุงููุดุบููู ุจุดูู ูุจูุฑ ูู Intelยฎ Extension for PyTorchุ ูุชู ุฅุฑุณุงููุง ุฌุฒุฆููุง ุฅูู ูุฑุน PyTorch ุงูุฑุฆูุณู. ูููู ูููุณุชุฎุฏููู ุงูุญุตูู ุนูู ุฃุฏุงุก ุฃูุถู ูุชุฌุฑุจุฉ ูุณุชุฎุฏู ูุญุณูุฉ ูุน ุงูุฏูุฉ ุงููุฎุชูุทุฉ ุงูุชููุงุฆูุฉ IPEX.

## ุชุซุจูุช IPEX:

ูุชุจุน ุฅุตุฏุงุฑ IPEX ุฅุตุฏุงุฑ PyTorchุ ูุชุซุจูุชู ุนุจุฑ pip:

| ุฅุตุฏุงุฑ PyTorch | ุฅุตุฏุงุฑ IPEX |
| :---------------: | :----------: |
| 2.0               |  2.0.0         |
| 1.13              |  1.13.0        |
| 1.12              |  1.12.300      |
| 1.11              |  1.11.200      |
| 1.10              |  1.10.100      |

```
pip install intel_extension_for_pytorch==<version_name> -f https://developer.intel.com/ipex-whl-stable-cpu
```

ุชุญูู ูู ุงููุฒูุฏ ูู ุงูุฃุณุงููุจ ูุชุซุจูุช [IPEX](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/installation.html).

## ููู ูุนูู ูุชุญุณูู ุงูุชุฏุฑูุจ ุนูู ุงููุนุงูุฌ:

ูุงูุช ๐ค Accelerate ุจุชูุงูู [IPEX](https://github.com/intel/intel-extension-for-pytorch)ุ ูู ูุง ุนููู ูุนูู ูู ุชููููู ูู ุฎูุงู ุงูุชูููู.

**ุงูุณููุงุฑูู 1**: ุชุณุฑูุน ุงูุชุฏุฑูุจ ุนูู ูุนุงูุฌ CPU ุบูุฑ ุงูููุฒุน

ูู ุจุชุดุบูู <u>accelerate config</u> ุนูู ุฌูุงุฒู:

```bash
$ accelerate config
-----------------------------------------------------------------------------------------------------------------------------------------------------------
In which compute environment are you running?
This machine
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Which type of machine are you using?
No distributed training
Do you want to run your training on CPU only (even if a GPU / Apple Silicon device is available)? [yes/NO]:yes
Do you want to use Intel PyTorch Extension (IPEX) to speed up training on CPU? [yes/NO]:yes
Do you wish to optimize your script with torch dynamo?[yes/NO]:NO
Do you want to use DeepSpeed? [yes/NO]: NO
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Do you wish to use FP16 or BF16 (mixed precision)?
bf16
```

ุณูุคุฏู ูุฐุง ุฅูู ุฅูุดุงุก ููู ุชูููู ุณูุชู ุงุณุชุฎุฏุงูู ุชููุงุฆููุง ูุชุนููู ุงูุฎูุงุฑุงุช ุงูุงูุชุฑุงุถูุฉ ุจุดูู ุตุญูุญ ุนูุฏ ุงูููุงู ุจูุง ููู:

```bash
accelerate launch my_script.py --args_to_my_script
```

ุนูู ุณุจูู ุงููุซุงูุ ุฅููู ููููุฉ ุชุดุบูู ูุซุงู NLP `examples/nlp_example.py` (ูู ุงูุฌุฐุฑ ุงูุฎุงุต ุจุงููุณุชูุฏุน) ูุน ุชูููู IPEX.

default_config.yaml ุงูุฐู ูุชู ุฅูุดุงุคู ุจุนุฏ `accelerate config`

```bash
compute_environment: LOCAL_MACHINE
distributed_type: 'NO'
downcast_bf16: 'no'
ipex_config:
  ipex: true
machine_rank: 0
main_training_function: main
mixed_precision: bf16
num_machines: 1
num_processes: 1
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: true
```

```bash
accelerate launch examples/nlp_example.py
```

**ุงูุณููุงุฑูู 2**: ุชุณุฑูุน ุงูุชุฏุฑูุจ ุนูู ูุนุงูุฌ CPU ุงูููุฒุน

ูุณุชุฎุฏู Intel oneCCL ููุงุชุตุงูุ ููุชุฑููุง ุจููุชุจุฉ Intelยฎ MPI ูุชูุฏูู ุฑุณุงุฆู ูุฌููุนุงุช ูุฑูุฉ ููุนุงูุฉ ููุงุจูุฉ ููุชุทููุฑ ุนูู ุจููุฉ Intelยฎ. ููููู ุงูุฑุฌูุน ุฅูู [ููุง](https://huggingface.co/docs/transformers/perf_train_cpu_many) ููุญุตูู ุนูู ุฏููู ุงูุชุซุจูุช.

ูู ุจุชุดุบูู <u>accelerate config</u> ุนูู ุฌูุงุฒู (node0):

```bash
$ accelerate config
-----------------------------------------------------------------------------------------------------------------------------------------------------------
In which compute environment are you running?
This machine
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Which type of machine are you using?
multi-CPU
How many different machines will you use (use more than 1 for multi-node training)? [1]: 4
-----------------------------------------------------------------------------------------------------------------------------------------------------------
What is the rank of this machine?
0
What is the IP address of the machine that will host the main process? 36.112.23.24
What is the port you will use to communicate with the main process? 29500
Are all the machines on the same local network? Answer `no` if nodes are on the cloud and/or on different network hosts [YES/no]: yes
Do you want to use Intel PyTorch Extension (IPEX) to speed up training on CPU? [yes/NO]:yes
Do you want accelerate to launch mpirun? [yes/NO]: yes
Please enter the path to the hostfile to use with mpirun [~/hostfile]: ~/hostfile
Enter the number of oneCCL worker threads [1]: 1
Do you wish to optimize your script with torch dynamo?[yes/NO]:NO
How many processes should be used for distributed training? [1]:16
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Do you wish to use FP16 or BF16 (mixed precision)?
bf16
```

ุนูู ุณุจูู ุงููุซุงูุ ุฅููู ููููุฉ ุชุดุบูู ูุซุงู NLP `examples/nlp_example.py` (ูู ุงูุฌุฐุฑ ุงูุฎุงุต ุจุงููุณุชูุฏุน) ูุน ุชูููู IPEX ููุชุฏุฑูุจ ุนูู ูุนุงูุฌ CPU ุงูููุฒุน.

default_config.yaml ุงูุฐู ูุชู ุฅูุดุงุคู ุจุนุฏ `accelerate config`

```bash
compute_environment: LOCAL_MACHINE
distributed_type: MULTI_CPU
downcast_bf16: 'no'
ipex_config:
  ipex: true
machine_rank: 0
main_process_ip: 36.112.23.24
main_process_port: 29500
main_training_function: main
mixed_precision: bf16
mpirun_config:
  mpirun_ccl: '1'
  mpirun_hostfile: /home/user/hostfile
num_machines: 4
num_processes: 16
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: true
```

ูู ุจุชุนููู ูุง ููู ูุงุณุชุฎุฏุงู intel MPI ูุจุฏุก ุงูุชุฏุฑูุจ:

ูู node0ุ ุชุญุชุงุฌ ุฅูู ุฅูุดุงุก ููู ุชูููู ูุญุชูู ุนูู ุนูุงููู IP ููู ุนูุฏุฉ (ุนูู ุณุจูู ุงููุซุงู hostfile) ููุฑุฑ ูุณุงุฑ ููู ุงูุชูููู ูุญุฌุฉ.

ุฅุฐุง ุงุฎุชุฑุช ุฃู ูููู Accelerate ุจุชุดุบูู `mpirun`ุ ูุชุฃูุฏ ูู ูุทุงุจูุฉ ูููุน ููู ุงููุถูู ูููุณุงุฑ ุงูููุฌูุฏ ูู ุงูุชูููู.

```bash
$ cat hostfile
xxx.xxx.xxx.xxx #node0 ip
xxx.xxx.xxx.xxx #node1 ip
xxx.xxx.xxx.xxx #node2 ip
xxx.xxx.xxx.xxx #node3 ip
```

ุนูุฏูุง ูููู Accelerate ุจุชุดุบูู `mpirun`ุ ูู ุจุชูููุฐ ุงุฑุชุจุงุทุงุช oneCCL setvars.sh ููุญุตูู ุนูู ุจูุฆุฉ Intel MPI ุงูุฎุงุตุฉ ุจูุ ุซู ูู ุจุชุดุบูู ูุตู ุงูุจุฑูุฌู ุจุงุณุชุฎุฏุงู `accelerate launch`. ูุงุญุธ ุฃู ุงููุต ุงูุจุฑูุฌู ูู Python ูุงูุจูุฆุฉ ูุฌุจ ุฃู ููููุง ููุฌูุฏูู ุนูู ุฌููุน ุงูุขูุงุช ุงููุณุชุฎุฏูุฉ ููุชุฏุฑูุจ ุนูู ูุนุงูุฌุงุช CPU ูุชุนุฏุฏุฉ.

```bash
oneccl_bindings_for_pytorch_path=$(python -c "from oneccl_bindings_for_pytorch import cwd; print(cwd)")
source $oneccl_bindings_for_pytorch_path/env/setvars.sh

accelerate launch examples/nlp_example.py
```

ูู ูุงุญูุฉ ุฃุฎุฑูุ ุฅุฐุง ุงุฎุชุฑุช ุนุฏู ููุงู Accelerate ุจุชุดุบูู `mpirun`ุ ููู ุจุชุดุบูู ุงูุฃูุฑ ุงูุชุงูู ูู node0 ูุณูุชู
ุชูููู **16DDP** ูู node0 ูnode1 ูnode2 ูnode3 ูุน ุงูุฏูุฉ ุงููุฎุชูุทุฉ BF16. ุนูุฏ ุงุณุชุฎุฏุงู ูุฐู ุงูุทุฑููุฉุ ูุฌุจ ุฃู ูููู ูุต Python ุงูุจุฑูุฌู ูุจูุฆุฉ Python ูููู ุชูููู Accelerate ููุฌูุฏูู ุนูู ุฌููุน ุงูุขูุงุช ุงููุณุชุฎุฏูุฉ ููุชุฏุฑูุจ ุนูู ูุนุงูุฌุงุช CPU ูุชุนุฏุฏุฉ.

```bash
oneccl_bindings_for_pytorch_path=$(python -c "from oneccl_bindings_for_pytorch import cwd; print(cwd)")
source $oneccl_bindings_for_pytorch_path/env/setvars.sh
export CCL_WORKER_COUNT=1
export MASTER_ADDR=xxx.xxx.xxx.xxx #node0 ip
export CCL_ATL_TRANSPORT=ofi
mpirun -f hostfile -n 16 -ppn 4 accelerate launch examples/nlp_example.py
```

## ุงูููุงุฑุฏ ุฐุงุช ุงูุตูุฉ:

- [ูุดุฑูุน ุนูู GitHub](https://github.com/intel/intel-extension-for-pytorch)
- [ูุซุงุฆู ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/api_doc.html)
- [ุฏููู ุงูุถุจุท ุงูุฏููู](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html)
- [ุงููุฏููุงุช ูุงูููุดูุฑุงุช](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/blogs_publications.html)