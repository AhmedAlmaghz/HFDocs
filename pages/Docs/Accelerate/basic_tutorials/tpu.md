# لمحة عن التدريب باستخدام وحدة معالجة tensor

[وحدة معالجة tensor (TPU)](https://cloud.google.com/tpu/docs/intro-to-tpu) هي نوع من الأجهزة المصممة خصيصًا لتدريب النماذج بكفاءة. تدعم Accelerate التدريب على وحدة معالجة tensor، ولكن هناك بعض الأمور التي يجب أن تكون على دراية بها، وهي تجميع الرسوم البيانية على وجه التحديد. تتناول هذه الورشة التدريبية بإيجاز موضوع التجميع، وللحصول على مزيد من التفاصيل، يمكنك الاطلاع على دليل [التدريب على وحدات معالجة tensor باستخدام Accelerate](../concept_guides/training_tpu).

## التجميع

ينشئ TPU رسمًا بيانيًا لجميع العمليات في خطوة التدريب، مثل التغذية الأمامية، والتغذية العكسية، وخطوة المحسن. وهذا هو السبب في أن خطوة التدريب الأولى تستغرق دائمًا بعض الوقت لأن بناء هذا الرسم البياني وتجميعه يستغرق وقتًا. ولكن بمجرد اكتمال التجميع، يتم تخزينه مؤقتًا في الذاكرة، وتكون جميع الخطوات اللاحقة أسرع بكثير.

الشيء الأساسي هو تجنب إعادة تجميع الكود الخاص بك؛ لأن ذلك يجعل التدريب بطيئًا للغاية. وهذا يعني أن جميع عملياتك يجب أن تكون متطابقة تمامًا:

- يجب أن يكون لجميع المصفوفات في دفعاتك نفس الطول (على سبيل المثال، لا يوجد حشو ديناميكي لمهام معالجة اللغات الطبيعية)
- يجب أن يكون الكود الخاص بك ثابتًا (على سبيل المثال، لا توجد طبقات ذات حلقات for ذات أطوال مختلفة اعتمادًا على الإدخال، مثل LSTM)

## ربط الأوزان

يتمثل أحد التصميمات الشائعة لنماذج اللغة في ربط أوزان طبقات التضمين وsoftmax. ومع ذلك، فإن نقل النموذج إلى وحدة معالجة tensor (سواء قمت بذلك بنفسك أو مررته إلى طريقة [`~Accelerator.prepare`]) يؤدي إلى كسر ربط الأوزان، وسيتعين عليك إعادة ربط الأوزان.

لإضافة سلوك خاص (مثل ربط الأوزان) في البرنامج النصي لوحدات معالجة tensor، قم أولاً بتعيين [`~Accelerator.distributed_type`] على `DistributedType.TPU`. بعد ذلك، يمكنك استخدام طريقة [`~transformers.PreTrainedModel.tie_weights`] لربط الأوزان.

```py
if accelerator.distributed_type == DistributedType.TPU:
model.tie_weights()
```