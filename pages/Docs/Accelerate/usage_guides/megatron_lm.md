ูู ูุชู ุชุฑุฌูุฉ ุงูุฃุฌุฒุงุก ุงูุชู ุทูุจุช ุนุฏู ุชุฑุฌูุชูุงุ ูุซู ุงูุฑูุงุจุท ูุฑููุฒ HTML ูCSS ูุงูุดูุฑุงุช ุงูุจุฑูุฌูุฉ.

# Megatron-LM

ูุชูุญ [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) ุชุฏุฑูุจ ููุงุฐุฌ ุงููุบุฉ ุงูุถุฎูุฉ ุงููุญููุฉ ุนูู ูุทุงู ูุงุณุน. ูููุฑ ููุงุฒุงุฉ ูุนุงูุฉ ูููุณูุฌ ูุฎุท ุงูุฃูุงุจูุจ ูุงูุชุณูุณู ูููุงุฐุฌ ูุง ูุจู ุงูุชุฏุฑูุจ ุงููุณุชูุฏุฉ ุฅูู ุงููุญูู ูุซู [GPT](https://arxiv.org/abs/2005.14165) (ูู ุงูุชุดููุฑ ููุท) ู [BERT](https://arxiv.org/pdf/1810.04805.pdf) (ุงูุชุฑููุฒ ููุท) ู [T5](https://arxiv.org/abs/1910.10683) (ุงูุชุฑููุฒ ููู ุงูุชุดููุฑ). ููุญุตูู ุนูู ูุนูููุงุช ููุตูุฉ ูููููุฉ ุนูู ุงูุฃุดูุงุก ุฎูู ุงูููุงููุณุ ูุฑุฌู ุงูุฑุฌูุน ุฅูู [repo](https://github.com/NVIDIA/Megatron-LM) ุนูู GitHub.

## ูุง ูู ุงููุฏูุฌุ

ูุฏูุฌ Accelerate ุงูููุฒุฉ ุงูุชุงููุฉ ูู Megatron-LM ูุชูููู ูุง ูุจู ุงูุชุฏุฑูุจ/ุงูุชุฏุฑูุจ ุงูุฏููู ุนูู ูุทุงู ูุงุณุน ูู BERT (Encoder) ุฃู GPT (Decoder) ุฃู ููุงุฐุฌ T5 (Encoder and Decoder):

ุฃ. **ููุงุฒุงุฉ ุงููุณูุฌ (TP)**: ุชููู ูู ุงูุจุตูุฉ ุงูุฐุงูุฑุฉ ุฏูู ุงููุซูุฑ ูู ุงูุงุชุตุงู ุงูุฅุถุงูู ุนูู ุฑุชุจ ุงูุนูุฏ ุฏุงุฎู ุงูุนูุฏุฉ. ูุชู ุชูุณูู ูู ุชูุณูู ุฅูู ุนุฏุฉ ูุทุนุ ูุน ูุฌูุฏ ูู ุดุฑูุญุฉ ุนูู ูุญุฏุฉ GPU ูููุตูุฉ. ูู ูู ุฎุทูุฉุ ุชุชู ูุนุงูุฌุฉ ููุณ ุงูุฏูุนุฉ ุงูุตุบูุฑุฉ ูู ุงูุจูุงูุงุช ุจุดูู ูุณุชูู ูุจุงูุชูุงุฒู ูู ูุจู ูู ุดุฑูุญุฉุ ููููุง ุงููุฒุงููุฉ ุนุจุฑ ุฌููุน ูุญุฏุงุช GPU (`ุนูููุฉ all-reduce`). ูู ุทุจูุฉ ูุญูู ุจุณูุทุฉุ ูุคุฏู ูุฐุง ุฅูู `all-reduces` 2 ูู ุงููุณุงุฑ ุงูุฃูุงูู ู 2 ูู ุงููุณุงุฑ ุงูุฎููู. ููุฒูุฏ ูู ุงูุชูุงุตููุ ูุฑุฌู ุงูุฑุฌูุน ุฅูู ุงููุฑูุฉ ุงูุจุญุซูุฉ [Megatron-LM: ุชุฏุฑูุจ ููุงุฐุฌ ูุบุฉ ูุนููุงุช ูุชุนุฏุฏุฉ ุงููููุงุฑุงุช ุจุงุณุชุฎุฏุงู ููุงุฒุงุฉ ุงููููุฐุฌ](https://arxiv.org/pdf/1909.08053.pdf) ููุฐุง ุงููุณู ูู ููุดูุฑ ุงููุฏููุฉ [ุงูุชูููููุฌูุง ูุฑุงุก ุชุฏุฑูุจ BLOOM](https://huggingface.co/blog/bloom-megatron-deepspeed#tensor-parallelism).

ุจ. **ููุงุฒุงุฉ ุฎุท ุงูุฃูุงุจูุจ (PP)**: ุชูููู ุงูุจุตูุฉ ุงูุฐุงูุฑุฉ ูุชูููู ุงูุชุฏุฑูุจ ูุงุณุน ุงููุทุงู ูู ุฎูุงู ููุงุฒุงุฉ ุงูุนูุฏุฉ ุจูู ุงูุนูุฏุฉ. ุชููู ููุงุนุฉ PP ุงูุณุงุฐุฌุฉ ูู ุฎูุงู ุฌุฏูู PipeDream-Flush schedule/1F1B ูุฌุฏูู 1F1B ูุชุฏุงุฎู. ูุชู ุชูุฒูุน ุงูุทุจูุงุช ุจุงูุชุณุงูู ุนุจุฑ ูุฑุงุญู PP. ุนูู ุณุจูู ุงููุซุงูุ ุฅุฐุง ูุงู ูุฏู ุงููููุฐุฌ "24" ุทุจูุฉ ููุฏููุง "4" ูุญุฏุงุช GPU ูููุงุฒุงุฉ ุฎุท ุงูุฃูุงุจูุจุ ูุณุชุญุชูู ูู ูุญุฏุฉ GPU ุนูู "6" ุทุจูุงุช (24/4). ููุฒูุฏ ูู ุงูุชูุงุตูู ุญูู ุงูุฌุฏุงูู ุงูุฒูููุฉ ูุชูููู ููุช ุงูุฎููู ูู PPุ ูุฑุฌู ุงูุฑุฌูุน ุฅูู ุงููุฑูุฉ ุงูุจุญุซูุฉ [ุชุฏุฑูุจ ููุงุฐุฌ ูุบุฉ ูุงุณุนุฉ ุงููุทุงู ุจููุงุกุฉ ุนูู ูุฌููุนุงุช GPU ุจุงุณุชุฎุฏุงู Megatron-LM](https://arxiv.org/pdf/2104.04473.pdf) ููุฐุง ุงููุณู ูู ููุดูุฑ ุงููุฏููุฉ [ุงูุชูููููุฌูุง ูุฑุงุก ุชุฏุฑูุจ BLOOM](https://huggingface.co/blog/bloom-megatron-deepspeed#pipeline-parallelism).

ุฌ. **ุชุณูุณู ุงูููุงุฒุงุฉ (SP)**: ุชูููู ุงูุจุตูุฉ ุงูุฐุงูุฑุฉ ุฏูู ุฃู ุงุชุตุงู ุฅุถุงูู. ูุง ููุทุจู ุฅูุง ุนูุฏ ุงุณุชุฎุฏุงู TP. ุฅูู ูููู ูู ุฐุงูุฑุฉ ุงูุชูุดูุท ุงููุทููุจุฉ ูุฃูู ูููุน ูุฌูุฏ ููุณ ุงููุณุฎ ุนูู ุฑุชุจ ููุงุฒุงุฉ ุงููุณูุฌ ุจุนุฏ `all-reduce` ุนู ุทุฑูู ุงุณุชุจุฏุงููุง ุจุนูููุฉ `reduce-scatter` ูุณูุชู ุงุณุชุจุฏุงู ุนูููุฉ `no-op` ุจุนูููุฉ `all-gather`. ูุธุฑูุง ูุฃู `all-reduce = reduce-scatter + all-gather`ุ ูุฅู ูุฐุง ูููุฑ ุงููุซูุฑ ูู ุฐุงูุฑุฉ ุงูุชูุดูุท ุฏูู ุฃู ุชูููุฉ ุฅุถุงููุฉ ููุงุชุตุงู. ุจุจุณุงุทุฉุ ูููู ุจุชุดุทูุฑ ูุฎุฑุฌุงุช ูู ุทุจูุฉ ูุญูู ุนูู ุทูู ุงูุจุนุฏ ุงูุชุณูุณููุ ุนูู ุณุจูู ุงููุซุงูุ ุฅุฐุง ูุงู ุทูู ุงูุชุณูุณู ูู "1024" ููุงู ุญุฌู TP ูู "4"ุ ูุณุชุญุชูู ูู ูุญุฏุฉ GPU ุนูู "256" ุฑูุฒูุง (1024/4) ููู ุนููุฉ. ููุฐุง ูุฒูุฏ ูู ุญุฌู ุงูุฏูุนุฉ ุงูุชู ูููู ุฏุนููุง ููุชุฏุฑูุจ. ููุฒูุฏ ูู ุงูุชูุงุตููุ ูุฑุฌู ุงูุฑุฌูุน ุฅูู ุงููุฑูุฉ ุงูุจุญุซูุฉ [ุชูููู ุฅุนุงุฏุฉ ุญุณุงุจ ุงูุชูุดูุท ูู ููุงุฐุฌ ุงููุญูู ุงููุจูุฑุฉ](https://arxiv.org/pdf/2205.05198.pdf).

ุฏ. **ููุงุฒุงุฉ ุงูุจูุงูุงุช (DP)** ุนุจุฑ ุงูููุฒุน ุงูููููุณููู: ุชููู ูู ุงูุจุตูุฉ ุงูุฐุงูุฑุฉ ุนู ุทุฑูู ุชุฌุฒุฆุฉ ุญุงูุงุช ุงูููุญูุณููู ูุงูุชุฏุฑุฌุงุช ุนุจุฑ ุฑุชุจ DP (ููุงุจู ุงูุทุฑููุฉ ุงูุชูููุฏูุฉ ูุชูุฑุงุฑ ุญุงูุฉ ุงูููุญูุณููู ุนุจุฑ ุฑุชุจ ููุงุฒุงุฉ ุงูุจูุงูุงุช). ุนูู ุณุจูู ุงููุซุงูุ ุนูุฏ ุงุณุชุฎุฏุงู ูุญุณู Adam ูุน ุงูุชุฏุฑูุจ ุงูุฏูููุ ูุญุชูู ูู ูุนููุฉ ุนูู 12 ุจุงูุช ูู ุงูุฐุงูุฑุฉ. ูุชู ุชูุฒูุน ูุฐุง ุจุงูุชุณุงูู ุนุจุฑ ูุญุฏุงุช GPUุ ุฃู ุฃู ูู ูุนููุฉ ุณุชุญุชูู ุนูู 3 ุจุงูุช (12/4) ุฅุฐุง ูุงู ูุฏููุง 4 ูุญุฏุงุช GPU. ููุฒูุฏ ูู ุงูุชูุงุตููุ ูุฑุฌู ุงูุฑุฌูุน ุฅูู ุงููุฑูุฉ ุงูุจุญุซูุฉ [ZeRO: ุชุญุณูู ุงูุฐุงูุฑุฉ ูุญู ุชุฏุฑูุจ ููุงุฐุฌ ุงููุนููุงุช ุงูุชุฑููููููุฉ](https://arxiv.org/pdf/1910.02054.pdf) ูุงููุณู ุงูุชุงูู ูู ๐ค blog [ุงูุชูููููุฌูุง ูุฑุงุก ุชุฏุฑูุจ BLOOM](https://huggingface.co/blog/bloom-megatron-deepspeed#zero-data-parallelism).

ูู. **ุฅุนุงุฏุฉ ุญุณุงุจ ุงูุชูุดูุท ุงูุงูุชูุงุฆู**: ุชูููู ุงูุจุตูุฉ ุงูุฐุงูุฑุฉ ููุชูุดูุท ุจุดูู ูุจูุฑ ูู ุฎูุงู ุชุณุฌูู ููุงุท ุงูุชูุดูุท ุงูุฐููุฉ. ูุง ูููู ุจุชุฎุฒูู ุงูุชูุดูุทุงุช ุงูุชู ุชุดุบู ุฐุงูุฑุฉ ูุจูุฑุฉ ุฃุซูุงุก ุฅุนุงุฏุฉ ุงูุญุณุงุจ ุจุณุฑุนุฉุ ูุจุงูุชุงูู ุชุญููู ุชูุงุฒู ุฑุงุฆุน ุจูู ุงูุฐุงูุฑุฉ ูุฅุนุงุฏุฉ ุงูุญุณุงุจ. ุนูู ุณุจูู ุงููุซุงูุ ุจุงููุณุจุฉ ูู GPT-3ุ ูุคุฏู ูุฐุง ุฅูู ุชูููู ุจูุณุจุฉ 70% ูู ุงูุฐุงูุฑุฉ ุงููุทููุจุฉ ููุชูุดูุท ุนูู ุญุณุงุจ 2.7% ููุท ูู ูููุงุช FLOPs ูุฅุนุงุฏุฉ ุญุณุงุจ ุงูุชูุดูุทุงุช. ููุฒูุฏ ูู ุงูุชูุงุตููุ ูุฑุฌู ุงูุฑุฌูุน ุฅูู ุงููุฑูุฉ ุงูุจุญุซูุฉ [ุชูููู ุฅุนุงุฏุฉ ุญุณุงุจ ุงูุชูุดูุท ูู ููุงุฐุฌ ุงููุญูู ุงููุจูุฑุฉ](https://arxiv.org/pdf/2205.05198.pdf).

ู. **ุงูููู ุงูููุฏูุฌุฉ**: Softmax ุงูููุฏูุฌุ ูุฏูุฉ ูุฎุชูุทุฉ ููุฏูุฌุฉ ุทุจูุฉ ุงูุชุทุจูุนุ ูุชุฌููุน ุงูุชุฏุฑุฌุงุช ุงูููุฏูุฌุฉ ูุญุณุงุจ ุงูุชุฏุฑุฌ ุงููุฒูู ูุทุจูุฉ ุฎุทูุฉ. PyTorch JIT ูุงู ุจุชุฌููุน GeLU ุงูููุฏูุฌ ูุงูุงูุญูุงุฒ ุงูููุฏูุฌ + ุฅุณูุงุท + ุฅุถุงูุฉ ุจูุงูุง.

ุฒ. **ุฏุนู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงููููุฑุณุฉ**: ุชูุณูู ุซูุงุฆู ูุนุงู ููุฌููุนุงุช ุงูุจูุงูุงุช ููุชุฏุฑูุจ ูุงุณุน ุงููุทุงู. ุฏุนู ูู `mmap`ุ ูููู ููุฑุณ `cached`ุ ูุชูุณูู ูุญูู `lazy`.

ุญ. **ุงูุชุญูู ูู ุดูู ููุทุฉ ุงูุชูุชูุด ููุงุจููุฉ ุงูุชุดุบูู ุงูุจููู**: ุฃุฏุงุฉ ูุณุงุนุฏุฉ ูุฅุนุงุฏุฉ ุชุดููู ููุงุท ุชูุชูุด Megatron-LM ุฐุงุช ุฃุญุฌุงู ูุชูุงุฒูุฉ ูุชุบูุฑุฉ ูููุณูุฌ ูุฎุท ุงูุฃูุงุจูุจ ุฅูู ููุงุท ุชูุชูุด ูุฌุฒุฃุฉ ๐ค Transformers ุงููุญุจูุจุฉ ูุธุฑูุง ูุฏุนููุง ุงูุฑุงุฆุน ูุน ูุฌููุนุฉ ุฃุฏูุงุช ูููุฑุฉ ูุซู ๐ค Accelerate Big Model Inference ู Megatron-DeepSpeed Inferenceุ ุฅูุฎ. ูุชููุฑ ุงูุฏุนู ุฃูุถูุง ูุชุญููู ููุงุท ุชูุชูุด ๐ค Transformers ุงููุฌุฒุฃุฉ ุฅูู ููุทุฉ ุชูุชูุด Megatron-LM ุฐุงุช ุฃุญุฌุงู ูุชูุงุฒูุฉ ูุชุบูุฑุฉ ูููุณูุฌ ูุฎุท ุงูุฃูุงุจูุจ ููุชุฏุฑูุจ ูุงุณุน ุงููุทุงู.

## ุงููุชุทูุจุงุช ุงูุฃุณุงุณูุฉ

ุณุชุญุชุงุฌ ุฅูู ุชุซุจูุช ุฃุญุฏุซ ุฅุตุฏุงุฑุงุช PyTorch ูcuda ูnccl ู[APEX](https://github.com/NVIDIA/apex#quick-start) ูู NVIDIA ูููุชุจุฉ nltk. ุฑุงุฌุน [ุงููุซุงุฆู](https://github.com/NVIDIA/Megatron-LM#setup) ููุฒูุฏ ูู ุงูุชูุงุตูู.

ุทุฑููุฉ ุฃุฎุฑู ูุฅุนุฏุงุฏ ุงูุจูุฆุฉ ูู ุณุญุจ ุญุงููุฉ PyTorch ูู NVIDIA ุชุฃุชู ูุน ุฌููุน ุงูุชุซุจูุชุงุช ุงููุทููุจุฉ ูู NGC.

ูููุง ููู ุทุฑููุฉ ุฎุทูุฉ ุจุฎุทูุฉ ูุฅุนุฏุงุฏ ุจูุฆุฉ conda:

1. ูู ุจุฅูุดุงุก ุจูุฆุฉ ุงูุชุฑุงุถูุฉ:

```
conda create --name ml
```

2. ุงูุชุฑุถ ุฃู ุงูุขูุฉ ุจูุง CUDA 11.3 ูุซุจุชุ ูู ุจุชุซุจูุช ุฅุตุฏุงุฑ GPU ุงูููุงุจู ูู PyTorch:

```
conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch
```

3. ุชุซุจูุช Nvidia APEX:

```
git clone https://github.com/NVIDIA/apex
cd apex
pip install -v --disable-pip-version-check --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./
cd ..
```

4. ุชุซุจูุช Megatron-LM:

```
git clone https://github.com/NVIDIA/Megatron-LM.git
cd Megatron-LM
git checkout core_r0.5.0
pip install --no-use-pep517 -e .
```
## ุชุณุฑูุน Megatron-LM Plugin

ุชูุฏุนู ุงูููุฒุงุช ุงููููุฉ ูุจุงุดุฑุฉ ุนุจุฑ ุฃูุฑ `accelerate config`. ููููุง ููู ูุซุงู ุนูู ุงูุฃุณุฆูุฉ ุงูููุงุจูุฉ ูุงุณุชุฎุฏุงู ููุฒุงุช Megatron-LM:

```bash
:~$ accelerate config --config_file "megatron_gpt_config.yaml"
ูู ุฃู ุจูุฆุฉ ุญูุณุจุฉ ุชุนููุ ([0] ูุฐู ุงูุขูุฉุ [1] AWS (Amazon SageMaker)): 0
ูุง ููุน ุงูุขูุฉ ุงูุชู ุชุณุชุฎุฏููุงุ ([0] ูุง ููุฌุฏ ุชุฏุฑูุจ ููุฒุนุ [1] ูุชุนุฏุฏ ูุญุฏุงุช ุงููุนุงูุฌุฉ ุงููุฑูุฒูุฉุ [2] ูุชุนุฏุฏ ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุชุ [3] ูุญุฏุฉ ูุนุงูุฌุฉ ูุงุฆูุฉ): 2
ูู ุนุฏุฏ ุงูุขูุงุช ุงููุฎุชููุฉ ุงูุชู ุณุชุณุชุฎุฏููุง (ุงุณุชุฎุฏู ุฃูุซุฑ ูู 1 ููุชุฏุฑูุจ ูุชุนุฏุฏ ุงูุนูุฏ)ุ [1]:
ูู ุชุฑูุฏ ุงุณุชุฎุฏุงู DeepSpeedุ [ูุนู/ูุง]:
ูู ุชุฑูุฏ ุงุณุชุฎุฏุงู FullyShardedDataParallelุ [ูุนู/ูุง]:
ูู ุชุฑูุฏ ุงุณุชุฎุฏุงู Megatron-LMุ [ูุนู/ูุง]: ูุนู
ูุง ูู ุฏุฑุฌุฉ/ุญุฌู ุงูุชูุงุฒู ูู ุงููุนุงูุฌุฉ ุงูุชูุณูุฑูุฉุ [1]:2
ูู ุชุฑูุฏ ุชูููู ุงูุชุณูุณู ุงููุชูุงุฒูุ [ูุนู/ูุง]:
ูุง ูู ุฏุฑุฌุฉ/ุญุฌู ุงูุชูุงุฒู ูู ุงูุฃูุงุจูุจุ [1]:2
ูุง ูู ุนุฏุฏ ุงูุฏูุนุงุช ุงูุตุบุฑูุ [1]:2
ูู ุชุฑูุฏ ุชูููู ุฅุนุงุฏุฉ ุญุณุงุจ ุงูุชูุดูุท ุงูุงูุชูุงุฆูุ [ูุนู/ูุง]:
ูู ุชุฑูุฏ ุงุณุชุฎุฏุงู ุงููุญุณู ุงูููุฒุน ุงูุฐู ููุณู ุญุงูุฉ ุงููุญุณู ูุงูุชุฏุฑุฌุงุช ุนุจุฑ ุฑุชุจ ุงูุชูุงุฒู ูู ุงูุจูุงูุงุชุ [ูุนู/ูุง]:
ูุง ูู ูููุฉ ุชูููู ุงูุชุฏุฑุฌ ุจูุงุกู ุนูู ุงููุนูุงุฑ L2 ุงูุนุงููู (0 ูุฅููุงู ุงูุชุดุบูู)ุ [1.0]:
ูู ุนุฏุฏ ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุช (GPU) ุงูุชู ูุฌุจ ุงุณุชุฎุฏุงููุง ููุชุฏุฑูุจ ุงูููุฒุนุ [1]:4
ูู ุชุฑูุฏ ุงุณุชุฎุฏุงู ุงูุฏูุฉ ุงูุนุงุฆูุฉ FP16 ุฃู BF16 (ุงูุฏูุฉ ุงููุฎุชูุทุฉ)ุ [ูุง/FP16/BF16]: BF16
```

ููุธูุฑ ุงูุชูููู ุงููุงุชุฌ ุฃุฏูุงู:

```
~$ cat megatron_gpt_config.yaml
compute_environment: LOCAL_MACHINE
deepspeed_config: {}
distributed_type: MEGATRON_LM
downcast_bf16: 'no'
fsdp_config: {}
machine_rank: 0
main_process_ip: null
main_process_port: null
main_training_function: main
megatron_lm_config:
megatron_lm_gradient_clipping: 1.0
megatron_lm_num_micro_batches: 2
megatron_lm_pp_degree: 2
megatronMultiplier_lm_recompute_activations: true
megatron_lm_sequence_parallelism: true
megatron_lm_tp_degree: 2
megatron_lm_use_distributed_optimizer: true
mixed_precision: bf16
num_machines: 1
num_processes: 4
rdzv_backend: static
same_network: true
use_cpu: false
```

ุณูุฃุฎุฐ ูุซุงููุง ุนูู ุงูุชุฏุฑูุจ ุงููุณุจู ูู GPT. ููููุง ููู ุงูุชุบููุฑุงุช ุงูุฏููุง ุงููุทููุจุฉ ูู `run_clm_no_trainer.py` ุงูุฑุณูู ูุงุณุชุฎุฏุงู Megatron-LM:

1. ูุธุฑูุง ูุฃู Megatron-LM ูุณุชุฎุฏู ุชูููุฐู ุงูุฎุงุต ูู ุงููุญุณูุ ูุฌุจ ุงุณุชุฎุฏุงู ุงูุฌุฏููุฉ ุงููุชูุงููุฉ ูุนู. ูุจุงูุชุงููุ ูุฅู ุงูุฏุนู ูุชุงุญ ููุท ูุฌุฏููุฉ Megatron-LM. ูุฌุจ ุนูู ุงููุณุชุฎุฏู ุฅูุดุงุก `accelerate.utils.MegatronLMDummyScheduler`. ููููุง ููู ูุซุงู ุนูู ุฐูู:

```python
from accelerate.utils import MegatronLMDummyScheduler

if accelerator.distributed_type == DistributedType.MEGATRON_LM:
lr_scheduler = MegatronLMDummyScheduler(
optimizer=optimizer,
total_num_steps=args.max_train_steps,
warmup_num_steps=args.num_warmup_steps,
)
else:
lr_scheduler = get_scheduler(
name=args.lr_scheduler_type,
optimizer=optimizer,
num_warmup_steps=args.num_warmup_steps * args.gradient_accumulation_steps,
num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,
)
```

2. ูุชุทูุจ ุงูุญุตูู ุนูู ุชูุงุตูู ุญุฌู ุงูุฏูุนุฉ ุงูุฅุฌูุงููุฉ ุงูุขู ูุนุฑูุฉ ุฃุญุฌุงู ุงูุชูุงุฒู ูู ุงููุนุงูุฌุฉ ุงูุชูุณูุฑูุฉ ูุงูุฃูุงุจูุจ. ููููุง ููู ูุซุงู ุนูู ููููุฉ ุงูุญุตูู ุนูู ุญุฌู ุงูุฏูุนุฉ ุงูุฅุฌูุงููุฉ ุงููุนุงูุฉ:

```python
if accelerator.distributed_type == DistributedType.MEGATRON_LM:
total_batch_size = accelerator.state.megatron_lm_plugin.global_batch_size
else:
total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps
```

3. ุนูุฏ ุงุณุชุฎุฏุงู Megatron-LMุ ูุชู ุจุงููุนู ุญุณุงุจ ูุชูุณุท ุงูุฎุณุงุฆุฑ ุนุจุฑ ูุฌููุนุฉ ุงูุชูุงุฒู ูู ุงูุจูุงูุงุช:

```python
if accelerator.distributed_type == DistributedType.MEGATRON_LM:
losses.append(loss)
else:
losses.append(accelerator.gather_for_metrics(loss.repeat(args.per_device_eval_batch_size)))

if accelerator.distributed_type == DistributedType.MEGATRON_LM:
losses = torch.tensor(losses)
else:
losses = torch.cat(losses)
```

4. ุจุงููุณุจุฉ ุฅูู Megatron-LMุ ูุฌุจ ุนูููุง ุญูุธ ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู `accelerator.save_state`:

```python
if accelerator.distributed_type == DistributedType.MEGATRON_LM:
accelerator.save_state(args.output_dir)
else:
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(
args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
)
```

ูุฐุง ูู ุดูุก! ูุญู ูุณุชุนุฏูู ุงูุขู ููุงูุทูุงู ๐. ููููู ุงูุนุซูุฑ ุนูู ูุซุงู ูููุต ุงูุจุฑูุฌู ูู ูุฌูุฏ ุงูุฃูุซูุฉ ุนูู ุงููุณุงุฑ `accelerate/examples/by_feature/megatron_lm_gpt_pretraining.py`.

ุฏุนููุง ูููุฐ ุฐูู ููููุฐุฌ `gpt-large` ุงููุนูุงุฑู ุจุงุณุชุฎุฏุงู 4 ูุญุฏุงุช ูุนุงูุฌุฉ ุฑุณูููุฉ A100-80GB.

```bash
accelerate launch --config_file megatron_gpt_config.yaml \
examples/by_feature/megatron_lm_gpt_pretraining.py \
--config_name "gpt2-large" \
--tokenizer_name "gpt2-large" \
--dataset_name wikitext \
--dataset_config_name wikitext-2-raw-v1 \
--block_size 1024 \
--learning_rate 5e-5 \
--per_device_train_batch_size 24 \
--per_device_eval_batch_size 24 \
--num_train_epochs 5 \
--with_tracking \
--report_to "wandb" \
--output_dir "awesome_model"
```

ูููุง ููู ุจุนุถ ุงูููุชุทูุงุช ุงููููุฉ ูู ุณุฌูุงุช ุงูุฅุฎุฑุงุฌ:

```bash
Loading extension module fused_dense_cuda...
>>> done with compiling and loading fused kernels. Compilation time: 3.569 seconds
> padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
Building gpt model in the pre-training mode.
The Megatron LM model weights are initialized at random in `accelerator.prepare`. Please use `accelerator.load_checkpoint` to load a pre-trained checkpoint matching the distributed setup.
Preparing dataloader
Preparing dataloader
Preparing model
> number of parameters on (tensor, pipeline) model parallel rank (1, 0): 210753280
> number of parameters on (tensor, pipeline) model parallel rank (1, 1): 209445120
> number of parameters on (tensor, pipeline) model parallel rank (0, 0): 210753280
> number of parameters on (tensor, pipeline) model parallel rank (0, 1): 209445120
Preparing optimizer
Preparing scheduler
> learning rate decay style: linear
10/10/2022 22:57:22 - INFO - __main__ - ***** Running training *****
10/10/2022 22:57:22 - INFO - __main__ -   Num examples = 2318
10/10/2022 22:57:22 - INFO - __main__ -   Num Epochs = 5
10/10/2022 22:57:22 - INFO - __main__ -   Instantaneous batch size per device = 24
10/10/2022 22:57:22 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 48
10/10/2022 22:57:22 - INFO - __main__ -   Gradient Accumulation steps = 1
10/10/2022 22:57:22 - INFO - __main__ -   Total optimization steps = 245
20%|โโโโโโโโโโโโโ                                                 | 49/245 [01:04<04:09,  1.27s/it]
10/10/2022 22:58:29 - INFO - __main__ - epoch 0: perplexity: 1222.1594275215962 eval_loss: 7.10837459564209
40%|โโโโโโโโโโโโโโโโโโโโโโโโโ                                     | 98/245 [02:10<03:07,  1.28s/it]
10/10/2022 22:59:35 - INFO - __main__ - epoch 1: perplexity: 894.5236583794557 eval_loss: 6.796291351318359
60%|โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ                        | 147/245 [03:16<02:05,  1.28s/it]
10/10/2022 23:00:40 - INFO - __main__ - epoch 2: perplexity: 702.8458788508042 eval_loss: 6.555137634277344
80%|โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ            | 196/245 [04:22<01:02,  1.28s/it]
10/10/2022 23:01:46 - INFO - __main__ - epoch 3: perplexity: 600.3220028695281 eval_loss: 6.39746618270874
100%|โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ| 245/245 [05:27<00:00,  1.28s/it]
```

ููุงู ุนุฏุฏ ูุจูุฑ ูู ุงูุฎูุงุฑุงุช/ุงูููุฒุงุช ุงูุฃุฎุฑู ุงูุชู ูููู ุชุนููููุง ุจุงุณุชุฎุฏุงู `accelerate.utils.MegatronLMPlugin`.

## ููุฒุงุช ูุชูุฏูุฉ ููุงุณุชูุงุฏุฉ ูู ูุชุงุจุฉ ุฎุทูุฉ ุงูุชุฏุฑูุจ ุงููุฎุตุตุฉ ููุฌููุนุงุช ุจูุงูุงุช Megatron-LM ุงููููุฑุณุฉ

ูุงุณุชุบูุงู ุงููุฒูุฏ ูู ุงูููุฒุงุชุ ูุฑุฌู ุงูุงุทูุงุน ุนูู ุงูุชูุงุตูู ุฃุฏูุงู.

1. ูููุง ููู ูุซุงู ุนูู ุงูุชุบููุฑุงุช ุงููุทููุจุฉ ูุชุฎุตูุต ุฎุทูุฉ ุงูุชุฏุฑูุจ ุฃุซูุงุก ุงุณุชุฎุฏุงู Megatron-LM. ุณุชููู ุจุชูููุฐ `accelerate.utils.AbstractTrainStep` ุฃู ูุฑุงุซุฉ ุฃุญุฏ ุฃุทูุงูููุง `accelerate.utils.GPTTrainStep`ุ `accelerate.utils.BertTrainStep` ุฃู `accelerate.utils.T5TrainStep`.

```python
from accelerate.utils import MegatronLMDummyScheduler, GPTTrainStep, avg_losses_across_data_parallel_group

# Custom loss function for the Megatron model
class GPTTrainStepWithCustomLoss(GPTTrainStep):
def __init__(self, megatron_args, **kwargs):
super().__init__(megatron_args)
self.kwargs = kwargs

def get_loss_func(self):
def loss_func(inputs, loss_mask, output_tensor):
batch_size, seq_length = output_tensor.shape
losses = output_tensor.float()
loss_mask = loss_mask.view(-1).float()
loss = losses.view(-1) * loss_mask

# Resize and average loss per sample
loss_per_sample = loss.view(batch_size, seq_length).sum(axis=1)
loss_mask_per_sample = loss_mask.view(batch_size, seq_length).sum(axis=1)
loss_per_sample = loss_per_sample / loss_mask_per_sample

# Calculate and scale weighting
weights = torch.stack([(inputs == kt).float() for kt in self.kwargs["keytoken_ids"]]).sum(axis=[0, 2])
weights = 1.0 + self.kwargs["alpha"] * weights
# Calculate weighted average
weighted_loss = (loss_per_sample * weights).mean()

# Reduce loss across data parallel groups
averaged_loss = avg_losses_across_data_parallel_group([weighted_loss])

return weighted_loss, {"lm loss": averaged_loss[0]}

return loss_func

def get_forward_step_func(self):
def forward_step(data_iterator, model):
"""Forward step."""
# Get the batch.
tokens, labels, loss_mask, attention_mask, position_ids = self.get_batch(data_iterator)
output_tensor = model(tokens, position_ids, attention_mask, labels=labels)

return output_tensor, partial(self.loss_func, tokens, loss_mask)

return forward_step


def main():
# Custom loss function for the Megatron model
keytoken_ids = []
keywords = ["plt", "pd", "sk", "fit", "predict", " plt", " pd", " sk", " fit", " predict"]
for keyword in keywords:
ids = tokenizer([keyword]).input_ids[0]
if len(ids) == 1:
keytoken_ids.append(ids[0])
accelerator.print(f"Keytoken ids: {keytoken_ids}")
accelerator.state.megatron_lm_plugin.custom_train_step_class = GPTTrainStepWithCustomLoss
accelerator.state.megatron_lm_plugin.custom_train_step_kwargs = {
"keytoken_ids": keytoken_ids,
"alpha": 0.25,
}
```

2. ูุงุณุชุฎุฏุงู ูุฌููุนุงุช ุจูุงูุงุช Megatron-LMุ ููุงู ุจุนุถ ุงูุชุบููุฑุงุช ุงูุฅุถุงููุฉ ุงููุทููุจุฉ. ูุชุงุญ ุจุฑุงูุฌ ุชุญููู ุงูุจูุงูุงุช ููุฐู ุงููุฌููุนุงุช ูู ุงูุจูุงูุงุช ููุท ุนูู ุงูุฑุชุจุฉ 0 ูู ูู ูุฌููุนุฉ ุชูุงุฒู ุชูุณูุฑูุฉ. ูุจุงูุชุงููุ ููุงู ุฑุชุจ ุญูุซ ูู ูููู ุจุฑูุงูุฌ ุชุญููู ุงูุจูุงูุงุช ูุชุงุญูุงุ ููุชุทูุจ ุฐูู ุฅุฌุฑุงุก ุชุนุฏููุงุช ุนูู ุญููุฉ ุงูุชุฏุฑูุจ. ุฅู ุงููุฏุฑุฉ ุนูู ุงูููุงู ุจุฐูู ุชูุธูุฑ ูุฏู ูุฑููุฉ ููุงุจููุฉ ุงูุชุฏุงุฏ ููุชุจุฉ ๐ค Accelerate. ููููุง ููู ุงูุชุบููุฑุงุช ุงููุทููุจุฉ:

   - ุจุงููุณุจุฉ ุฅูู ูุฌููุนุงุช ุจูุงูุงุช Megatron-LM ุงููููุฑุณุฉุ ูุฌุจ ุนูููุง ุงุณุชุฎุฏุงู `MegatronLMDummyDataLoader` ูุชูุฑูุฑ ูุณุงุฆุท ูุฌููุนุฉ ุงูุจูุงูุงุช ุงููุทููุจุฉ ุฅููู ูุซู `data_path`ุ `seq_length`ุ ููุง ุฅูู ุฐูู. ุงูุธุฑ [ููุง](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/arguments.py#L804) ููุญุตูู ุนูู ูุงุฆูุฉ ุจุงููุณุงุฆุท ุงููุชุงุญุฉ.

   ```python
   from accelerate.utils import MegatronLMDummyDataLoader

   megatron_dataloader_config = {
   "data_path": args.data_path,
   "splits_string": args.splits_string,
   "seq_length": args.block_size,
   "micro_batch_size": args.per_device_train_batch_size,
   }
   megatron_dataloader = MegatronLMDummyDataLoader(**megatron_dataloader_config)
   accelerator.state.megatron_lm_plugin.megatron_dataset_flag = True
   ```

   - ูุชู ุชูุฑุงุฑ `megatron_dataloader` 3 ูุฑุงุช ููุญุตูู ุนูู ุจุฑุงูุฌ ุชุญููู ุจูุงูุงุช ุงูุชุฏุฑูุจ ูุงูุชุญูู ูุงูุงุฎุชุจุงุฑ ููููุง ููุณุจ `args.splits_string`

   ```python
   model, optimizer, lr_scheduler, train_dataloader, eval_dataloader, _ = accelerator.prepare(
   model, optimizer, lr
## ุฃุฏุงุฉ ูุชุญููู ููุทุฉ ุงูุชูุชูุด ูุงูุชูุงูู ุงูุชุดุบููู 
1. ุชุชููุฑ ุงููุตูุต ุงูุจุฑูุฌูุฉ ููุฐู ุงูุฃุฏุงุฉ ูู ููุชุจุฉ ๐ค Transformers ุชุญุช ุงูููุงุฐุฌ ุงูููุงุจูุฉ.
ููู ูุชููุฑุฉ ุญุงูููุง ููููุฐุฌ GPT [checkpoint_reshaping_and_interoperability.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py) 
2. ูููุง ููู ูุซุงู ุนูู ุชุญููู ููุทุฉ ุชูุชูุด ูู Megatron-LM ุฅูู ููุทุฉ ุชูุชูุด ูุฌุฒุฃุฉ ุนุงูููุฉ ูู ๐ค Transformers.
```bash
python checkpoint_reshaping_and_interoperability.py \
--convert_checkpoint_from_megatron_to_transformers \
--load_path "gpt/iter_0005000" \
--save_path "gpt/trfs_checkpoint" \
--max_shard_size "200MB" \
--tokenizer_name "gpt2" \
--print-checkpoint-structure
``` 
3. ุชุญููู ููุทุฉ ุงูุชูุชูุด ูู Transformers ุฅูู Megatron ูุน `tp_size=2`ุ `pp_size=2` ู `dp_size=2`.
```bash
python checkpoint_utils/megatgron_gpt2/checkpoint_reshaping_and_interoperability.py \
--load_path "gpt/trfs_checkpoint" \
--save_path "gpt/megatron_lm_checkpoint" \
--target_tensor_model_parallel_size 2 \
--target_pipeline_model_parallel_size 2 \
--target_data_parallel_size 2 \
--target_params_dtype "bf16" \
--make_vocab_size_divisible_by 128 \
--use_distributed_optimizer \
--print-checkpoint-structure
```

## ุฏุนู ููุงุฐุฌ Megatron-LM GPT ูุฅุฑุฌุงุน logits ูุฏุงูุฉ `megatron_generate` ูุชูููุฏ ุงููุต 
1. ูุชุทูุจ ุฅุฑุฌุงุน logits ุชุนููู `require_logits=True` ูู MegatronLMPlugin ููุง ูู ููุถุญ ุฃุฏูุงู.
ุณุชููู ูุฐู ุงูููู ูุชุงุญุฉ ูู ุงููุฑุญูุฉ ุงูุฃุฎูุฑุฉ ูู ุงูุฃูุงุจูุจ.
```python
megatron_lm_plugin = MegatronLMPlugin(return_logits=True)
``` 
2. ุทุฑููุฉ `megatron_generate` ููููุฐุฌ Megatron-LM GPT: ุณุชุณุชุฎุฏู ูุฐู ุงูุทุฑููุฉ ุงูุชูุงุฒู ุงูุชูุชุฑู ูุฃูุงุจูุจ ุงูุชูุงุฒู ูุฅููุงู ุงูุฃุฌูุงู ููุฌููุนุฉ ูู ุงููุฏุฎูุงุช ุนูุฏ ุงุณุชุฎุฏุงู ุงูุฌุดุน ูุน/ุจุฏูู ุนููุงุช top_k/top_p ูููุฏุฎูุงุช ุงููุทุงู ุงููุฑุฏูุฉ ุนูุฏ ุงุณุชุฎุฏุงู ูู ุชุดููุฑ ุงูุจุญุซ ุงูุดุนุงุนู.
ูุชู ุฏุนู ูุฌููุนุฉ ูุฑุนูุฉ ููุท ูู ููุฒุงุช generate transformers. ุณูุณุงุนุฏ ูุฐุง ูู ุงุณุชุฎุฏุงู ุงูููุงุฐุฌ ุงููุจูุฑุฉ ุนุจุฑ ุงูุชูุงุฒู ุงูุชูุชุฑู ูุฃูุงุจูุจ ุงูุชูุงุฒู ููุชูููุฏ (ูุชู ุจุงููุนู ุงูุชุฎุฒูู ุงููุคูุช ูููููุฉ ุงูุฑุฆูุณูุฉ ูุงุณุชุฎุฏุงู ุงูููุงุฉ ุงูููุฏูุฌุฉ ุจุดูู ุงูุชุฑุงุถู).
ูุชุทูุจ ูุฐุง ุฃู ูููู ุญุฌู ุงูุจูุงูุงุช ุงูููุงุฒูุฉ 1ุ ูุฃู ูุชู ุชุนุทูู ุงูุชูุงุฒู ุงูุชุณูุณูู ูููุทุฉ ุชูุชูุด ุงูุชูุดูุท.
ููุง ูุชุทูุจ ุฃูุถูุง ุชุญุฏูุฏ ูุณุงุฑ ููู ุงูููุฑุฏุงุช ูููู ุงูุงูุฏูุงุฌ ูู ุงููุญูู ุงููุบูู.
ููุถุญ ุงููุซุงู ุงูุชุงูู ููููุฉ ุชูููู ูุงุณุชุฎุฏุงู ุทุฑููุฉ `megatron_generate` ููููุฐุฌ Megatron-LM GPT.
```python
# ุชุญุฏูุฏ ููู ุงูููุฑุฏุงุช ูููู ุงูุงูุฏูุงุฌ ูู ุงููุญูู ุงููุบูู
vocab_file = os.path.join(args.resume_from_checkpoint, "vocab.json")
merge_file = os.path.join(args.resume_from_checkpoint, "merges.txt")
other_megatron_args = {"vocab_file": vocab_file, "merge_file": merge_file}
megatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)

# ุงูุงุณุชุฏูุงู ุจุงุณุชุฎุฏุงู ูุธููุฉ `megatron_generate`
tokenizer.pad_token = tokenizer.eos_token
max_new_tokens = 64
batch_texts = [
"Are you human?",
"The purpose of life is",
"The arsenal was constructed at the request of",
"How are you doing these days?",
]
batch_encodings = tokenizer(batch_texts, return_tensors="pt", padding=True)

# ุนููุฉ top-p
generated_tokens = model.megatron_generate(
batch_encodings["input_ids"],
batch_encodings["attention_mask"],
max_new_tokens=max_new_tokens,
top_p=0.8,
top_p_decay=0.5,
temperature=0.9,
)
decoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())
accelerator.print(decoded_preds)

# ุนููุฉ top-k
generated_tokens = model.megatron_generate(
batch_encodings["input_ids"],
batch_encodings["attention_mask"],
max_new_tokens=max_new_tokens,
top_k=50,
temperature=0.9,
)
decoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())
accelerator.print(decoded_preds)

# ุฅุถุงูุฉ ุฑูุฒ `bos` ูู ุงูุจุฏุงูุฉ
generated_tokens = model.megatron_generate(
batch_encodings["input_ids"], batch_encodings["attention_mask"], max_new_tokens=max_new_tokens, add_BOS=True
)
decoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())
accelerator.print(decoded_preds)

# ุจุญุซ ุดุนุงุนู => ูุฃุฎุฐ ููุฌู ูุงุญุฏ ููุท
batch_texts = ["The purpose of life is"]
batch_encodings = tokenizer(batch_texts, return_tensors="pt", padding=True)
generated_tokens = model.megatron_generate(
batch_encodings["input_ids"],
batch_encodings["attention_mask"],
max_new_tokens=max_new_tokens,
num_beams=20,
length_penalty=1.5,
)
decoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())
accelerator.print(decoded_preds)
``` 
3. ูุชููุฑ ูุซุงู ุดุงูู ุนูู ุงุณุชุฎุฏุงู ุทุฑููุฉ `megatron_generate` ููููุฐุฌ Megatron-LM GPT ูู
[megatron_gpt2_generation.py](https://github.com/pacman100/accelerate-megatron-test/blob/main/src/inference/megatron_gpt2_generation.py) ูุน
ููู ุชูููู [megatron_lm_gpt_generate_config.yaml](https://github.com/pacman100/accelerate-megatron-test/blob/main/src/Configs/megatron_lm_gpt_generate_config.yaml).
ูุชููุฑ ูุต Bash ูุน ุฃูุฑ ุงูุฅุทูุงู ูู [megatron_lm_gpt_generate.sh](https://github.com/pacman100/accelerate-megatron-test/blob/main/megatron_lm_gpt_generate.sh).
ุชุชููุฑ ุณุฌูุงุช ุฅุฎุฑุงุฌ ุงููุต ูู [megatron_lm_gpt_generate.log](https://github.com/pacman100/accelerate-megatron-test/blob/main/output_logs/megatron_lm_gpt_generate.log).

## ุฏุนู ุชุถููู ุงูููุถุน ROPE ูALiBi ูMulti-Query Attention 
1. ุจุงููุณุจุฉ ูุงูุชูุงู ROPE/ALiBiุ ูู ุจุชูุฑูุฑ `position_embedding_type` ูุน `("absolute" | "rotary" | "alibi")` ุฅูู `MegatronLMPlugin` ููุง ูู ููุถุญ ุฃุฏูุงู.
```python
other_megatron_args = {"position_embedding_type": "alibi"}
megatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)
``` 
2. ุจุงููุณุจุฉ ูุงูุชูุงู Multi-Queryุ ูู ุจุชูุฑูุฑ `attention_head_type` ูุน `("multihead" | "multiquery")` ุฅูู `MegatronLMPlugin` ููุง ูู ููุถุญ ุฃุฏูุงู.
```python
other_megatron_args = {"attention_head_type": "multiquery"}
megatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)
```

## ุงูุชุญุฐูุฑุงุช 
1. ูุฏุนู ููุงุฐุฌ Transformers GPT2 ูMegatron-BERT ูT5.
ูุบุทู ูุฐุง ูุฆุงุช ุงูููุงุฐุฌ ูู ุงูุชุดููุฑ ููุท ูุงูุชุฑููุฒ ููุท ูุงูุชุฑููุฒ ููู ุงูุชุดููุฑ. 
2. ูุชู ุฅุฑุฌุงุน ุงูุฎุณุงุฑุฉ ููุท ูู ุชูุฑูุฑ ุงููููุฐุฌ ุฅูู ุงูุฃูุงู
ูุธุฑูุง ููุฌูุฏ ุชูุงุนู ูุนูุฏ ุจูู ุงูุฃูุงุจูุจ ูุงูุชูุงุฒู ุงูุชูุชุฑู ูุงูุชูุงุฒู ููุจูุงูุงุช ุฎูู ุงูููุงููุณ.
ุชูุฑุฌุน ููุงููุฉ `model(**batch_data)` ุงูุฎุณุงุฆุฑ ุงูุชู ุชู ุญุณุงุจ ูุชูุณุทูุง ุนุจุฑ ูุฑุงุชุจ ุงูุชูุงุฒู ููุจูุงูุงุช.
ูุฐุง ุฌูุฏ ููุนุธู ุงูุญุงูุงุช ุงูุชู ูุชู ูููุง ุชุดุบูู ูุธุงุฆู ูุง ูุจู ุงูุชุฏุฑูุจ ุจุงุณุชุฎุฏุงู ููุฒุงุช Megatron-LM
ููููู ุจุณูููุฉ ุญุณุงุจ "perplexity" ุจุงุณุชุฎุฏุงู ุงูุฎุณุงุฑุฉ.
ุจุงููุณุจุฉ ููููุฐุฌ GPTุ ูุชู ุฏุนู ุฅุฑุฌุงุน logits ุจุงูุฅุถุงูุฉ ุฅูู ุงูุฎุณุงุฆุฑ.
ูุง ูุชู ุฌูุน ูุฐู logits ุนุจุฑ ูุฑุงุชุจ ุงูุชูุงุฒู ููุจูุงูุงุช. ุงุณุชุฎุฏู `accelerator.utils.gather_across_data_parallel_groups`
ูุฌูุน logits ุนุจุฑ ูุฑุงุชุจ ุงูุชูุงุฒู ููุจูุงูุงุช. ูููู ุงุณุชุฎุฏุงู ูุฐู ุงูููู ุงูุตุญูุญุฉ ูุน ุงูุนูุงูุงุช ูุญุณุงุจ ููุงููุณ ุงูุฃุฏุงุก ุงููุฎุชููุฉ. 
3. ุงูุนูููุฉ ุงูุฑุฆูุณูุฉ ูู ุงููุฑุชุจุฉ ุงูุฃุฎูุฑุฉ ุญูุซ ุชุชููุฑ ุงูุฎุณุงุฆุฑ/logits ูู ุงููุฑุญูุฉ ุงูุฃุฎูุฑุฉ ูู ุงูุฃูุงุจูุจ.
`accelerator.is_main_process` ู `accelerator.is_local_main_process` ุฅุฑุฌุงุน `True` ูููุฑุชุจุฉ ุงูุฃุฎูุฑุฉ ุนูุฏ ุงุณุชุฎุฏุงู
ุฏูุฌ Megatron-LM. 
4. ูู ููุงููุฉ `accelerator.prepare`ุ ูุชู ุฅูุดุงุก ูููุฐุฌ Megatron-LM ุงูููุงุจู ููููุฐุฌ Transformers ูุนูู
ูุน ุฃูุฒุงู ุนุดูุงุฆูุฉ. ูุฑุฌู ุงุณุชุฎุฏุงู `accelerator.load_state` ูุชุญููู ููุทุฉ ุชูุชูุด Megatron-LM ูุน ุฃูุณุงู TP ูPP ูDP ุงููุทุงุจูุฉ. 
5. ุญุงูููุงุ ูุชููุฑ ุฏุนู ุชุญููู ููุทุฉ ุงูุชูุชูุด ูุงูุชูุงูู ุงูุชุดุบููู ููููุฐุฌ GPT ููุท.
ุณูุชู ุชูุณูุนู ูุฑูุจูุง ููุดูู BERT ูT5. 
6. ูุฌุจ ุฃู ุชููู `gradient_accumulation_steps` 1. ุนูุฏ ุงุณุชุฎุฏุงู Megatron-LMุ ูุฅู ุงูุฏูุนุงุช ุงูุตุบูุฑุฉ ูู ุฅุนุฏุงุฏ ุงูุชูุงุฒู ุงูุฃูุจูุจู
ูุฑุงุฏู ูุชุฑุงูู ุงูุชุฏุฑุฌุงุช. 
7. ุนูุฏ ุงุณุชุฎุฏุงู Megatron-LMุ ุงุณุชุฎุฏู `accelerator.save_state` ู `accelerator.load_state` ูุญูุธ ูุชุญููู ููุงุท ุงูุชูุชูุด. 
8. ูููุง ููู ุฎุฑูุทุฉ ูุนูุงุฑูุงุช ูููุฐุฌ Megatron-LM ุฅูู ูุนูุงุฑูุงุช ูููุฐุฌ ๐ค transformers ุงูููุงูุฆุฉ.
ูุชู ุฏุนู ููุงุฐุฌ ๐ค transformers ูุฐู ููุท. 
ุฃ. Megatron-LM [BertModel](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/bert_model.py) :
๐ค ููุงุฐุฌ ุงููุญููุงุช ูุน `megatron-bert` ูู ููุน ูููุฐุฌ ุงูุชููููุ ุนูู ุณุจูู ุงููุซุงูุ
[MegatronBERT](https://huggingface.co/docs/transformers/model_doc/megatron-bert) 
ุจ. Megatron-LM [GPTModel](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py) :
๐ค ููุงุฐุฌ ุงููุญููุงุช ูุน `gpt2` ูู ููุน ูููุฐุฌ ุงูุชููููุ ุนูู ุณุจูู ุงููุซุงูุ
[OpenAI GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2) 
ุฌ. Megatron-LM [T5Model](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/t5_model.py) :
๐ค ููุงุฐุฌ ุงููุญููุงุช ูุน `t5` ูู ููุน ุงูุชููููุ ุนูู ุณุจูู ุงููุซุงูุ
[T5](https://huggingface.co/docs/transformers/model_doc/t5) ู
[MT5](https://huggingface.co/docs/transformers/model_doc/mt5)