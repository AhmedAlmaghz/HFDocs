# ุงูุงุณุชุฏูุงู ุงูููุฒุน ุจุงุณุชุฎุฏุงู ๐ค Accelerate

ูููู ุฃู ููุน ุงูุงุณุชุฏูุงู ุงูููุฒุน ูู ุซูุงุซ ูุฆุงุช:

1. ุชุญููู ูููุฐุฌ ูุงูู ุนูู ูู GPU ูุฅุฑุณุงู ุฃุฌุฒุงุก ูู ุฏูุนุฉ ุนุจุฑ ูุณุฎุฉ ุงููููุฐุฌ ููู GPU ูู ููุช ูุงุญุฏ
2. ุชุญููู ุฃุฌุฒุงุก ูู ูููุฐุฌ ุนูู ูู GPU ููุนุงูุฌุฉ ุฅุฏุฎุงู ูุงุญุฏ ูู ููุช ูุงุญุฏ
3. ุชุญููู ุฃุฌุฒุงุก ูู ูููุฐุฌ ุนูู ูู GPU ูุงุณุชุฎุฏุงู ูุง ูุณูู ููุงุฒุงุฉ ุงูุฃูุงุจูุจ ุงููุฌุฏููุฉ ูุฏูุฌ ุงูุทุฑููุชูู ุงูุณุงุจูุชูู.

ุณููุฑ ุนุจุฑ ุงูููุณ ุงูุฃูู ูุงูุฃุฎูุฑุ ููุนุฑุถ ููููุฉ ุงูููุงู ุจูู ููููุง ูุฃูููุง ุณููุงุฑูููุงุช ุฃูุซุฑ ูุงูุนูุฉ.

## ุฅุฑุณุงู ุฃุฌุฒุงุก ูู ุฏูุนุฉ ุชููุงุฆููุง ุฅูู ูู ูููุฐุฌ ูุญูู

ูุฐุง ูู ุงูุญู ุงูุฃูุซุฑ ูุซุงูุฉ ูู ุงูุฐุงูุฑุฉุ ูุฃูู ูุชุทูุจ ูู ูู GPU ุงูุงุญุชูุงุธ ุจูุณุฎุฉ ูุงููุฉ ูู ุงููููุฐุฌ ูู ุงูุฐุงูุฑุฉ ูู ููุช ูุนูู.

ุนุงุฏุฉู ุนูุฏ ุงูููุงู ุจุฐููุ ูููู ุงููุณุชุฎุฏููู ุจุฅุฑุณุงู ุงููููุฐุฌ ุฅูู ุฌูุงุฒ ูุญุฏุฏ ูุชุญูููู ูู ูุญุฏุฉ ุงููุนุงูุฌุฉ ุงููุฑูุฒูุฉุ ุซู ููู ูู ููุฌู ุฅูู ุฌูุงุฒ ูุฎุชูู.

ูุฏ ูุจุฏู ุฎุท ุงูุฃูุงุจูุจ ุงูุฃุณุงุณู ุจุงุณุชุฎุฏุงู ููุชุจุฉ `diffusers` ุดูุฆูุง ูุง ุนูู ุงููุญู ุงูุชุงูู:

```python
import torch
import torch.distributed as dist
from diffusers import DiffusionPipeline

pipe = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16)
```

ุซู ูููู ุฅุฌุฑุงุก ุงูุงุณุชุฏูุงู ุจูุงุกู ุนูู ุงูููุฌู ุงููุญุฏุฏ:

```python
def run_inference(rank, world_size):
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    pipe.to(rank)

    if torch.distributed.get_rank() == 0:
        prompt = "a dog"
    elif torch.distributed.get_rank() == 1:
        prompt = "a cat"

    result = pipe(prompt).images[0]
    result.save(f"result_{rank}.png")
```

ุณูููุงุญุธ ุงููุฑุก ููู ูุชุนูู ุนูููุง ุงูุชุญูู ูู ุงูุชุฑุชูุจ ููุนุฑูุฉ ุงูููุฌู ุงูุฐู ุณูุชู ุฅุฑุณุงููุ ูุงูุฐู ูุฏ ูููู ูุฑูููุง ุจุนุถ ุงูุดูุก.

ูุฏ ูููุฑ ุงููุณุชุฎุฏู ุฃูุถูุง ุฃูู ุจุงุณุชุฎุฏุงู ๐ค Accelerateุ ูุฅู ุงุณุชุฎุฏุงู `Accelerator` ูุฅุนุฏุงุฏ ุจุฑูุงูุฌ ุชุญููู ุงูุจูุงูุงุช ููููุฉ ููุฐู ูุฏ ูููู ุฃูุถูุง ุทุฑููุฉ ุจุณูุทุฉ ูุฅุฏุงุฑุชูุง. (ููุนุฑูุฉ ุงููุฒูุฏุ ุฑุงุฌุน ุงููุณู ุฐู ุงูุตูุฉ ูู [ุงูุฌููุฉ ุงูุณุฑูุนุฉ](../quicktour#distributed-evaluation))

ูู ููููู ุฅุฏุงุฑุชูุงุ ูุนู. ูููู ูู ูุถูู ุฃูุถูุง ุฑูุฒูุง ุฅุถุงูููุง ุบูุฑ ุถุฑูุฑู: ูุนู ุฃูุถูุง.

ูุน ๐ค Accelerateุ ูููููุง ุชุจุณูุท ูุฐู ุงูุนูููุฉ ุจุงุณุชุฎุฏุงู ูุฏูุฑ ุณูุงู [`Accelerator.split_between_processes`] (ุงูุฐู ููุฌุฏ ุฃูุถูุง ูู `PartialState` ู`AcceleratorState`).

ุณุชููู ูุฐู ุงูุฏุงูุฉ ุชููุงุฆููุง ุจุชูุณูู ุฃู ุจูุงูุงุช ุชููู ุจุฅูุฑุงุฑูุง ุฅูููุง (ุณูุงุก ูุงู ููุฌููุง ุฃู ูุฌููุนุฉ ูู ุงูููุณูุฌุงุช ุฃู ูุงููุณูุง ููุจูุงูุงุช ุงูุณุงุจูุฉุ ุฅูุฎ.) ุนุจุฑ ุฌููุน ุงูุนูููุงุช (ูุน ุฅููุงููุฉ ุงูุชุจุทูู) ูุชุชููู ูู ุงุณุชุฎุฏุงููุง ุนูู ุงูููุฑ.

ุฏุนูุง ูุนูุฏ ูุชุงุจุฉ ุงููุซุงู ุฃุนูุงู ุจุงุณุชุฎุฏุงู ูุฏูุฑ ุงูุณูุงู ูุฐุง:

```python
from accelerate import PartialState  # Can also be Accelerator or AcceleratorState
from diffusers import DiffusionPipeline

pipe = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16)
distributed_state = PartialState()
pipe.to(distributed_state.device)

# Assume two processes
with distributed_state.split_between_processes(["a dog", "a cat"]) as prompt:
    result = pipe(prompt).images[0]
    result.save(f"result_{distributed_state.process_index}.png")
```

ููุฅุทูุงู ุงูููุฏุ ูููููุง ุงุณุชุฎุฏุงู ๐ค Accelerate:

ุฅุฐุง ููุช ุจุชูููุฏ ููู ุชูููู ูุงุณุชุฎุฏุงูู ุจุงุณุชุฎุฏุงู `accelerate config`:

```bash
accelerate launch distributed_inference.py
```

ุฅุฐุง ูุงู ูุฏูู ููู ุชูููู ูุญุฏุฏ ุชุฑูุฏ ุงุณุชุฎุฏุงูู:

```bash
accelerate launch --config_file my_config.json distributed_inference.py
```

ุฃู ุฅุฐุง ููุช ูุง ุชุฑุบุจ ูู ุฅูุดุงุก ุฃู ูููุงุช ุชูููู ูุชุดุบูููุง ุนูู ูุญุฏุชู GPU:

> ููุงุญุธุฉ: ุณุชุญุตู ุนูู ุจุนุถ ุงูุชุญุฐูุฑุงุช ุจุดุฃู ุงูููู ุงูุชู ูุชู ุชุฎููููุง ุจูุงุกู ุนูู ูุธุงูู. ูุฅุฒุงูุฉ ูุฐู ุงูุชุญุฐูุฑุงุชุ ููููู ุชุดุบูู `accelerate config default` ุฃู ุงูุงูุชูุงู ุนุจุฑ `accelerate config` ูุฅูุดุงุก ููู ุชูููู.

```bash
accelerate launch --num_processes 2 distributed_inference.py
```

ููุฏ ูููุง ุงูุขู ุจุชุฎููุถ ุฑูุฒ ุงูุญุดู ุงููุทููุจ ูุชูุณูู ูุฐู ุงูุจูุงูุงุช ุฅูู ุจุถุน ุณุทูุฑ ูู ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ุจุณูููุฉ.

ูููู ูุงุฐุง ูู ูุงู ูุฏููุง ุชูุฒูุน ุบุฑูุจ ููููุฌูุงุช ุฅูู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณููุงุชุ ุนูู ุณุจูู ุงููุซุงูุ ูุงุฐุง ูู ูุงู ูุฏููุง 3 ููุฌูุงุชุ ูููู ููุท 2 GPUุ

ูู ุฅุทุงุฑ ูุฏูุฑ ุงูุณูุงูุ ุณุชุชููู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณููุงุช ุงูุฃููู ุงูููุฌููู ุงูุฃูููู ูุณุชุชููู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณููุงุช ุงูุซุงููุฉ ุงูุซุงูุซุฉุ ููุง ูุถูู ุชูุณูู ุฌููุน ุงูููุฌูุงุช ูุนุฏู ูุฌูุฏ ูููุงุช ุนุงูุฉ.

ููุน ุฐููุ ูุงุฐุง ูู ุฃุฑุฏูุง ุจุนุฏ ุฐูู ุงูููุงู ุจุดูุก ูุง ุจูุชุงุฆุฌ *ุฌููุน ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณููุงุช*ุ (ูู ุฌูุนูุง ุฌููุนูุง ูุฃุฏุงุก ุจุนุถ ุฃููุงุน ูุง ุจุนุฏ ุงููุนุงูุฌุฉ)

ููููู ุชูุฑูุฑ `apply_padding=True` ูุถูุงู ุฃู ููุงุฆู ุงูููุฌูุงุช ูุจุทูุฉ ุฅูู ููุณ ุงูุทููุ ูุน ุฃุฎุฐ ุงูุจูุงูุงุช ุงูุฅุถุงููุฉ ูู ุงูุนููุฉ ุงูุฃุฎูุฑุฉ. ุจูุฐู ุงูุทุฑููุฉุ ุณูููู ูุฏู ุฌููุน ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณููุงุช ููุณ ุนุฏุฏ ุงูููุฌูุงุชุ ูููููู ุจุนุฏ ุฐูู ุฌูุน ุงููุชุงุฆุฌ.

<Tip>
ูุฐุง ูุทููุจ ููุท ุนูุฏ ูุญุงููุฉ ุฅุฌุฑุงุก ุฅุฌุฑุงุก ูุซู ุฌูุน ุงููุชุงุฆุฌุ ุญูุซ ุชุญุชุงุฌ ุงูุจูุงูุงุช ุนูู ูู ุฌูุงุฒ ุฅูู ุฃู ูููู ููุง ููุณ ุงูุทูู. ูุง ูุชุทูุจ ุงูุงุณุชุฏูุงู ุงูุฃุณุงุณู ุฐูู.
</Tip>

ุนูู ุณุจูู ุงููุซุงู:

```python
from accelerate import PartialState  # Can also be Accelerator or AcceleratorState
from diffusers import DiffusionPipeline

pipe = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16)
distributed_state = PartialState()
pipe.to(distributed_state.device)

# Assume two processes
with distributed_state.split_between_processes(["a dog", "a cat", "a chicken"], apply_padding=True) as prompt:
    result = pipe(prompt).images
```

ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณููุงุช ุงูุฃูููุ ุณุชููู ุงูููุฌูุงุช `[โa dogโุ โa catโ]`ุ ูุนูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณููุงุช ุงูุซุงููุฉ ุณุชููู `[โa chickenโุ โa chickenโ]`.

ุชุฃูุฏ ูู ุฅุณูุงุท ุงูุนููุฉ ุงูุฃุฎูุฑุฉุ ูุฃููุง ุณุชููู ูุณุฎุฉ ููุฑุฑุฉ ูู ุงูุณุงุจูุฉ.

ููููู ุงูุนุซูุฑ ุนูู ุฃูุซูุฉ ุฃูุซุฑ ุชุนููุฏูุง [ููุง](https://github.com/huggingface/accelerate/tree/main/examples/inference/distributed) ูุซู ููููุฉ ุงุณุชุฎุฏุงูู ูุน LLMs.

## ููุงุฒุงุฉ ุงูุฃูุงุจูุจ ุงููููุฑุฉ ููุฐุงูุฑุฉ (ุชุฌุฑูุจู)

ุณููุงูุด ูุฐุง ุงูุฌุฒุก ุงูุชุงูู ุงุณุชุฎุฏุงู *ููุงุฒุงุฉ ุงูุฃูุงุจูุจ*. ูุฐุง ูู ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช **ุงูุชุฌุฑูุจูุฉ** ุงูุชู ุชุณุชุฎุฏู [ููุชุจุฉ PiPPy ูู PyTorch](https://github.com/pytorch/PiPPy/) ูุญู ุฃุตูู.

ุงูููุฑุฉ ุงูุนุงูุฉ ูููุงุฒุงุฉ ุงูุฃูุงุจูุจ ูู: ูููุชุฑุถ ุฃู ูุฏูู 4 ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณููุงุช ููููุฐุฌูุง ูุจูุฑูุง ุจูุง ูููู ูููู ุชูุณููู ุนูู ุฃุฑุจุน ูุญุฏุงุช ูุนุงูุฌุฉ ุฑุณููุงุช ุจุงุณุชุฎุฏุงู `device_map="auto"`. ุจุงุณุชุฎุฏุงู ูุฐู ุงูุทุฑููุฉุ ููููู ุฅุฑุณุงู 4 ูุฏุฎูุงุช ูู ููุช ูุงุญุฏ (ุนูู ุณุจูู ุงููุซุงู ููุงุ ูุนูู ุฃู ูุจูุบ) ูุณูุนูู ูู ุฌุฒุก ูู ุงููููุฐุฌ ุนูู ุฅุฏุฎุงูุ ุซู ูุชููู ุงูุฅุฏุฎุงู ุงูุชุงูู ุจูุฌุฑุฏ ุงูุชูุงุก ุงูุฌุฒุก ุงูุณุงุจูุ ููุง ูุฌุนูู *ุฃูุซุฑ* ููุงุกุฉ **ูุฃุณุฑุน** ูู ุงูุทุฑููุฉ ุงูููุถุญุฉ ุณุงุจููุง. ุฅููู ุฑุณู ุชูุถูุญู ูุฃุฎูุฐ ูู ูุณุชูุฏุน PyTorch:

![ูุซุงู PiPPy](https://camo.githubusercontent.com/681d7f415d6142face9dd1b837bdb2e340e5e01a58c3a4b119dea6c0d99e2ce0/68747470733a2f2f692e696d6775722e636f6d2f65793555633934372e706e67)

ูุชูุถูุญ ููููุฉ ุงุณุชุฎุฏุงู ูุฐุง ูุน Accelerateุ ููุฏ ุฃูุดุฃูุง [ูุซุงูุงู ููุญููุงูุงุช ุงูุฃูููุฉ](https://github.com/huggingface/accelerate/tree/main/examples/inference) ููุธูุฑ ุนุฏุฏูุง ูู ุงูููุงุฐุฌ ูุงูููุงูู ุงููุฎุชููุฉ. ูู ูุฐุง ุงูุจุฑูุงูุฌ ุงูุชุนููููุ ุณููุธูุฑ ูุฐู ุงูุทุฑููุฉ ูู GPT2 ุนุจุฑ ูุญุฏุชู GPU.

ูุจู ุงููุชุงุจุนุฉุ ูุฑุฌู ุงูุชุฃูุฏ ูู ุชุซุจูุช ุฃุญุฏุซ ุฅุตุฏุงุฑ ูู PiPPy ุนู ุทุฑูู ุชุดุบูู ูุง ููู:

```bash
pip install torchpippy
```

ูุญู ูุทูุจ ุงูุฅุตุฏุงุฑ 0.2.0 ุนูู ุงูุฃูู. ููุชุฃูุฏ ูู ุฃู ูุฏูู ุงูุฅุตุฏุงุฑ ุงูุตุญูุญุ ูู ุจุชุดุบูู `pip show torchpippy`.

ุงุจุฏุฃ ุจุฅูุดุงุก ุงููููุฐุฌ ุนูู ูุญุฏุฉ ุงููุนุงูุฌุฉ ุงููุฑูุฒูุฉ:

```{python}
from transformers import GPT2ForSequenceClassification, GPT2Config

config = GPT2Config()
model = GPT2ForSequenceClassification(config)
model.eval()
```

ุจุนุฏ ุฐููุ ุณุชุญุชุงุฌ ุฅูู ุฅูุดุงุก ุจุนุถ ุฅุฏุฎุงูุงุช ุงููุซุงู ูุงุณุชุฎุฏุงููุง. ุชุณุงุนุฏ ูุฐู ุงูุฅุฏุฎุงูุงุช PiPPy ูู ุชุชุจุน ุงููููุฐุฌ.

<Tip warning={true}>
ููุน ุฐููุ ูุฅู ุงูุทุฑููุฉ ุงูุชู ุชููู ุจูุง ุจูุฐุง ุงููุซุงู ุณุชุญุฏุฏ ุญุฌู ุงูุฏูุนุฉ ุงููุณุจูุฉ ุงูุชู ุณูุชู ุงุณุชุฎุฏุงููุง/ุชูุฑูุฑูุง
ูู ุฎูุงู ุงููููุฐุฌ ูู ููุช ูุนููุ ูุฐุง ุชุฃูุฏ ูู ุชุฐูุฑ ุนุฏุฏ ุงูุนูุงุตุฑ ุงูููุฌูุฏุฉ!
</Tip>

```{python}
input = torch.randint(
low=0ุ
high=config.vocab_sizeุ
size=(2, 1024)ุ # bs x seq_len
device="cpu"ุ
dtype=torch.int64ุ
requires_grad=Falseุ
)
```

ุจุนุฏ ุฐููุ ูุญุชุงุฌ ุฅูู ุฅุฌุฑุงุก ุงูุชุชุจุน ุจุงููุนู ูุฅุนุฏุงุฏ ุงููููุฐุฌ. ููููุงู ุจุฐููุ ุงุณุชุฎุฏู ุฏุงูุฉ [`inference.prepare_pippy`] ูุณูุชู ูู ุงููููุฐุฌ ุจุงููุงูู ูููุงุฒุงุฉ ุงูุฃูุงุจูุจ ุชููุงุฆููุง:

```{python}
from accelerate.inference import prepare_pippy
example_inputs = {"input_ids": input}
model = prepare_pippy(model, example_args=(input,))
```

<Tip>
ููุงู ูุฌููุนุฉ ูุชููุนุฉ ูู ุงููุนููุงุช ุงูุชู ููููู ุชูุฑูุฑูุง ุฅูู `prepare_pippy`:
* ูุณูุญ `split_points` ุจุชุญุฏูุฏ ุงูุทุจูุงุช ูุชูุณูู ุงููููุฐุฌ ุนูุฏูุง. ุจุดูู ุงูุชุฑุงุถูุ ูุณุชุฎุฏู ุงูููุงู ุงูุฐู ูุนูู ููู `device_map="auto"`ุ ูุซู `fc` ุฃู `conv1`.
* ูุญุฏุฏ `num_chunks` ููููุฉ ุชูุณูู ุงูุฏูุนุฉ ูุฅุฑุณุงููุง ุฅูู ุงููููุฐุฌ ููุณู (ูุฐุง ูุฅู `num_chunks=1` ูุน ุฃุฑุจุน ููุงุท ุชูุณูู/ุฃุฑุจุน ูุญุฏุงุช ูุนุงูุฌุฉ ุฑุณููุงุช ุณูููู ููุง MP ุณุงุฐุฌ ุญูุซ ูุชู ุชูุฑูุฑ ุฅุฏุฎุงู ูุงุญุฏ ุจูู ููุงุท ุงูุชูุณูู ุงูุฃุฑุจุน ููุทุจูุฉ)
</Tip>

ูู ููุงุ ูู ูุง ุชุจูู ูู ุฅุฌุฑุงุก ุงูุงุณุชุฏูุงู ุงูููุฒุน ุจุงููุนู!

<Tip warning={true}>
ุนูุฏ ุชูุฑูุฑ ุงูุฅุฏุฎุงูุงุชุ ููุตู ุจุดุฏุฉ ุจุชูุฑูุฑูุง ููุฌููุนุฉ ูู ุงูุญุฌุฌ. ูุชู ุฏุนู ุงุณุชุฎุฏุงู `kwargs`ุ ููุน ุฐููุ ูุฅู ูุฐุง ุงูููุฌ ุชุฌุฑูุจู.
</Tip>

```python
args = some_more_arguments
with torch.no_grad ():
    output = model (* args)
```

ุนูุฏูุง ุชูุชูู ุฌููุน ุงูุจูุงูุงุช ุณุชููู ุนูู ุงูุนูููุฉ ุงูุฃุฎูุฑุฉ ููุท:

```python
from accelerate import PartialState
if PartialState().is_last_process:
    print(output)
```

<Tip>
ุฅุฐุง ููุช ุจุชูุฑูุฑ `gather_output=True` ุฅูู [`inference.prepare_pippy`]ุ ูุณูุชู ุฅุฑุณุงู ุงูุฅุฎุฑุงุฌ
ุนุจุฑ ุฌููุน ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณููุงุช ุจุนุฏ ุฐูู ุฏูู ุงูุญุงุฌุฉ ุฅูู ุงูุชุญูู ูู `is_last_process`. ูุฐุง ูู
`False` ุจุดูู ุงูุชุฑุงุถู ูุฃูู ูุชุทูุจ ููุงููุฉ ุงุชุตุงู.
</Tip>

ููุฐุง ูู ุดูุก! ููุฒูุฏ ูู ุงูุงุณุชูุดุงูุ ูุฑุฌู ุงูุงุทูุงุน ุนูู ุฃูุซูุฉ ุงูุงุณุชุฏูุงู ูู [ูุณุชูุฏุน Accelerate](https://github.com/huggingface/accelerate/tree/main/examples/inference/pippy) ููุซุงุฆููุง [](../package_reference/inference) ุฃุซูุงุก ุนูููุง ุนูู ุชุญุณูู ูุฐุง ุงูุชูุงูู.