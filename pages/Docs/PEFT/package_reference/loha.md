# LoHa

Low-Rank Hadamard Product ([LoHa](https://huggingface.co/papers/2108.06098)), طريقة مشابهة لـ LoRA باستثناء أنها تقارب مصفوفة الأوزان الكبيرة مع المزيد من المصفوفات منخفضة الرتبة وتجمعها مع المنتج الهادامارد. هذه الطريقة أكثر كفاءة في المعلمات من LoRA وتحقق أداءً قابلاً للمقارنة.

ملخص الورقة هو:

*في هذا العمل، نقترح معلمة فعالة من حيث التواصل، FedPara، للتعلم الاتحادي (FL) للتغلب على الأعباء المترتبة على عمليات تحميل وتنزيل النماذج المتكررة. تعيد طريقةنا معلمة معلمات الأوزان للطبقات باستخدام أوزان منخفضة الرتبة تليها المنتج الهادامارد. مقارنة بالمعلمة منخفضة الرتبة التقليدية، لا تقتصر طريقة FedPara الخاصة بنا على قيود الرتبة المنخفضة، وبالتالي لديها سعة أكبر بكثير. تمكن هذه الخاصية من تحقيق أداء قابل للمقارنة مع تقليل تكاليف الاتصال بمقدار 3 إلى 10 مرات مقارنة بالطراز الذي يحتوي على الطبقات الأصلية، وهو ما لا يمكن تحقيقه بواسطة الطرق التقليدية منخفضة الرتبة. يمكن تحسين كفاءة طريقةنا بشكل أكبر من خلال الجمع بينها وبين محسنات FL الأخرى الفعالة. بالإضافة إلى ذلك، نقوم بتوسيع طريقةنا إلى تطبيق FL شخصي، pFedPara، والذي يفصل المعلمات إلى معلمات عالمية ومحلية. نُظهر أن pFedPara يتفوق على طرق FL الشخصية المنافسة بأكثر من ثلاثة أضعاف عدد المعلمات*

## LoHaConfig

[[autodoc]] tuners.loha.config.LoHaConfig

## LoHaModel

[[autodoc]] tuners.loha.model.LoHaModel