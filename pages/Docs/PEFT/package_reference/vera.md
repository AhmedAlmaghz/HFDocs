# VeRA: Vector-based Random Matrix Adaptation

[VeRA](https://huggingface.co/papers/2310.11454) هي تقنية ضبط دقيق فعالة من حيث المعلمات تشبه LoRA ولكنها تتطلب عددًا أقل من المعلمات الإضافية مع الوعد بأداء مماثل أو أفضل. وبالتالي، فهي مفيدة بشكل خاص عندما تكون ميزانية المعلمات محدودة للغاية، على سبيل المثال، عند التوسع إلى نماذج كبيرة جدًا. يتم تحقيق تقليل عدد المعلمات القابلة للتدريب من خلال مشاركة نفس المصفوفات من الرتبة المنخفضة عبر جميع الطبقات، وتدريب متجهين إضافيين فقط لكل طبقة.

عند حفظ معلمات المحول، يمكن الاستغناء عن تخزين المصفوفات من الرتبة المنخفضة عن طريق تعيين `save_projection=False` على `VeraConfig`. في هذه الحالة، سيتم استعادة هذه المصفوفات بناءً على البذرة العشوائية الثابتة من حجة `projection_prng_key`. يقلل هذا من حجم نقطة التفتيش، ولكن لا يمكننا ضمان إمكانية إعادة الإنتاج على جميع الأجهزة وجميع الإصدارات المستقبلية من PyTorch. إذا كنت تريد ضمان إمكانية إعادة الإنتاج، فقم بتعيين `save_projection=True` (وهو الإعداد الافتراضي).

لتعامل مع أشكال مختلفة من الطبقات المكيفة، يقوم VeRA بتinitializing المصفوفات A و B المشتركة بأكبر حجم مطلوب لكل بُعد. أثناء التمرير الأمامي، يتم استخراج المصفوفات الفرعية A و B لطبقة معينة من هذه المصفوفات المشتركة ويتم استخدامها كما هو موصوف في الورقة. على سبيل المثال، يؤدي تكييف طبقتين خطيتين من الأشكال (100، 20) و (80، 50) إلى إنشاء مصفوفات A و B من الأشكال (الرتبة، 50) و (100، الرتبة) على التوالي. بعد ذلك، لتكييف طبقة من الشكل (100، 20)، يتم استخراج المصفوفات الفرعية A و B من الأشكال (الرتبة، 20) و (100، الرتبة).

في الوقت الحالي، يخضع VeRA للقيود التالية:

- تدعم الطبقات `nn.Linear` فقط.
- لا تدعم الطبقات الكمية.

إذا لم تكن هذه القيود مناسبة لحالتك الاستخدام، فيُرجى استخدام LoRA بدلاً من ذلك.

الملخص من الورقة هو:

> التكييف منخفض الرتبة (LoRA) هو طريقة شائعة تقلل عدد المعلمات القابلة للتدريب عند ضبط دقة نماذج اللغة الكبيرة، ولكنها لا تزال تواجه تحديات حادة في التخزين عند التوسع إلى نماذج أكبر أو نشر نماذج مكيفة متعددة لكل مستخدم أو لكل مهمة. في هذا العمل، نقدم Vector-based Random Matrix Adaptation (VeRA)، والتي تقلل بشكل كبير من عدد المعلمات القابلة للتدريب مقارنة بـ LoRA، مع الحفاظ على نفس الأداء. ويتحقق ذلك من خلال استخدام زوج واحد من المصفوفات منخفضة الرتبة المشتركة عبر جميع الطبقات وتعلم متجهات قياس صغيرة بدلاً من ذلك. نحن نثبت فعاليتها على معايير GLUE و E2E، ومهام تصنيف الصور، ونظهر تطبيقها في ضبط التعليمات لنموذجي لغة بحجم 7B و 13B.

## VeRAConfig

[[autodoc]] tuners.vera.config.VeraConfig

## VeRAModel

[[autodoc]] tuners.vera.model.VeraModel