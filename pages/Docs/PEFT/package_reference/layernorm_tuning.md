# LayerNorm Tuning

LayerNorm Tuning ([LN Tuning](https://huggingface.co/papers/2312.11420)) هي طريقة PEFT تقوم بضبط دقيق لمعلمات طبقات LayerNorm فقط في النموذج. وقد اختبرت الورقة أداء هذه الطريقة على نماذج اللغة الكبيرة وأظهرت أنها يمكن أن تحقق أداءً قوياً مع انخفاض كبير في عدد المعلمات القابلة للتدريب واستخدام ذاكرة GPU.

ومع ذلك، لا تقتصر الطريقة على نماذج اللغة ويمكن تطبيقها على أي نموذج يستخدم طبقات LayerNorm. في هذا التنفيذ، يكون الافتراضي هو ضبط جميع طبقات LayerNorm داخل النموذج بشكل دقيق، ولكن يمكن استخدامها لاستهداف أنواع طبقات أخرى مثل طبقات `MLP` أو `Attention`، ويمكن القيام بذلك من خلال تحديد `target_modules` في `LNTuningConfig`.

ملخص الورقة هو:

> يقدم هذا البحث استراتيجية فعالة لتحويل نماذج اللغة الكبيرة (LLMs) إلى نماذج لغة كبيرة متعددة الوسائط (MLLMs). ومن خلال تصور هذا التحول كعملية تكيف مجال، أي الانتقال من فهم النص إلى تبني وسائط متعددة، نلاحظ بشكل مثير للاهتمام أنه، ضمن كل كتلة اهتمام، يكفي ضبط LayerNorm لتحقيق أداء قوي. علاوة على ذلك، عندما تتم مقارنته بأساليب الضبط الدقيق الأخرى مثل الضبط الدقيق لمعلمات النموذج بالكامل أو LoRA، فإن فوائدها في الكفاءة كبيرة. على سبيل المثال، عند مقارنته بـ LoRA على نطاق نموذج 13B، يمكن تحسين الأداء بمتوسط يزيد عن 20% عبر خمس مهام متعددة الوسائط، وفي الوقت نفسه، يؤدي إلى انخفاض كبير في المعلمات القابلة للتدريب بنسبة 41.9% وانخفاض في استخدام ذاكرة GPU بنسبة 17.6%. بالإضافة إلى هذه الاستراتيجية LayerNorm، نوضح أن الضبط الانتقائي فقط باستخدام بيانات المحادثة يمكن أن يحسن الكفاءة بشكل أكبر. وبعيداً عن هذه النتائج التجريبية، نقدم تحليلاً شاملاً لاستكشاف دور LayerNorm في تكييف نماذج اللغة الكبيرة مع مجال الوسائط المتعددة وتحسين القوة التعبيرية للنموذج.

## LNTuningConfig

[[autodoc]] tuners.ln_tuning.config.LNTuningConfig

## LNTuningModel

[[autodoc]] tuners.ln_tuning.model.LNTuningModel