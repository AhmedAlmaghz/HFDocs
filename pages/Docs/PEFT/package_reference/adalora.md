# AdaLoRA

[AdaLoRA](https://hf.co/papers/2303.10512) هي طريقة لتحسين عدد المعلمات القابلة للتدريب وتعيينها لمصفوفات الأوزان والطبقات، على عكس LoRA، التي توزع المعلمات بالتساوي عبر جميع الوحدات النمطية. ويتم تخصيص ميزانية أكبر من المعلمات لمصفوفات الأوزان والطبقات المهمة، في حين تتلقى الأقل أهمية عددًا أقل من المعلمات.

الملخص من الورقة هو:

* "أصبح ضبط دقيق لنماذج اللغة الكبيرة المُدربة مسبقًا على مهام اللغة الطبيعية أسلوبًا مهمًا في معالجة اللغات الطبيعية. ومع ذلك، فإن الممارسة الشائعة تضبط دقة جميع المعلمات في نموذج مُدرب مسبقًا، وهو ما يصبح محظورًا عندما يكون هناك عدد كبير من مهام اللغة الطبيعية. لذلك، تم اقتراح العديد من طرق الضبط الدقيق لتعلم التحديثات التراكمية للأوزان المُدربة مسبقًا بطريقة فعالة من حيث المعلمات، على سبيل المثال، الزيادات منخفضة الرتبة. غالبًا ما توزع هذه الطرق ميزانية التحديثات التراكمية بالتساوي عبر جميع مصفوفات الأوزان المُدربة مسبقًا، وتتجاهل أهمية معلمات الأوزان المختلفة. وكنتيجة لذلك، يكون أداء الضبط الدقيق دون المستوى الأمثل. لسد هذه الفجوة، نقترح AdaLoRA، والتي تخصص ميزانية المعلمات بشكل تكيفي بين مصفوفات الأوزان وفقًا لدرجة أهميتها. وعلى وجه التحديد، تقوم AdaLoRA بمعلمية التحديثات التراكمية على شكل تحليل للمفردات المنفردة. يسمح لنا هذا النهج الجديد بتقليم القيم المنفردة للتحديثات غير المهمة بشكل فعال، وهو ما يقلل من ميزانية معلماتها ولكنه يتجنب حسابات SVD الدقيقة المكثفة. نجري تجارب واسعة النطاق مع عدة نماذج مُدربة مسبقًا على معالجة اللغة الطبيعية، والإجابة على الأسئلة، وتوليد اللغة الطبيعية للتحقق من فعالية AdaLoRA. وتظهر النتائج أن AdaLoRA يظهر تحسنًا ملحوظًا على خط الأساس، خاصة في إعدادات الميزانية المنخفضة. الكود الخاص بنا متاح للجمهور على https://github.com/QingruZhang/AdaLoRA*.

## AdaLoraConfig

[[autodoc]] tuners.adalora.config.AdaLoraConfig

## AdaLoraModel

[[autodoc]] tuners.adalora.model.AdaLoraModel