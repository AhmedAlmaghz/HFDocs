لم يتم ترجمة الأجزاء التي طلبت عدم ترجمتها، مثل النصوص البرمجية والروابط ورموز HTML و CSS، وتمت ترجمة بقية النص إلى اللغة العربية كما هو مطلوب:

# وحدات التكييف

تضيف الطرق المستندة إلى المحول مقاييس تدريب إضافية بعد طبقات الاهتمام والاتصال الكامل لنموذج مسبق التدريب مجمد لتقليل استخدام الذاكرة وتسريع التدريب. تختلف الطريقة باختلاف المحول، فقد تكون مجرد طبقة إضافية أو قد تكون عبارة عن تعبير عن تحديثات الأوزان ∆W على أنها تحليل رتبي منخفض لوزن المصفوفة. في كلتا الحالتين، تكون المحولات صغيرة عادةً ولكنها تظهر أداءً مماثلاً لنموذج تمت معايرته بشكل كامل وتمكّن من تدريب نماذج أكبر باستخدام موارد أقل.

سيعطيك هذا الدليل نظرة عامة موجزة عن طرق المحول التي يدعمها PEFT (إذا كنت مهتمًا بمعرفة المزيد من التفاصيل حول طريقة محددة، فألق نظرة على الورقة المرتبطة).

## التكيف منخفض الرتبة (LoRA)

<Tip>

LoRA هي واحدة من أكثر طرق PEFT شعبية ونقطة انطلاق جيدة إذا كنت تبدأ للتو مع PEFT. تم تطويره في الأصل للنماذج اللغوية الكبيرة ولكنه طريقة تدريب شائعة للغاية للنماذج الانتشارية بسبب كفاءتها وفعاليتها.

</Tip>

كما ذكرنا بإيجاز سابقًا، فإن [LoRA](https://hf.co/papers/2106.09685) هي تقنية تسرع من معايرة النماذج الكبيرة مع استهلاك ذاكرة أقل.

يمثل LoRA تحديثات الأوزان ∆W بمصفوفتين أصغر (تسمى *مصفوفات التحديث*) من خلال التحليل الرتبي المنخفض. يمكن تدريب هذه المصفوفات الجديدة للتكيف مع البيانات الجديدة مع الحفاظ على العدد الإجمالي للثوابت منخفضًا. تظل مصفوفة الأوزان الأصلية مجمدة ولا تتلقى أي تحديثات أخرى. لإنتاج النتائج النهائية، يتم دمج الأوزان الأصلية والإضافية المكيفة. يمكنك أيضًا دمج أوزان المحول مع النموذج الأساسي للقضاء على الكمون الاستنتاجي.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_animated.gif"/>
</div>

لهذا النهج عدد من المزايا:

- يجعل LoRA المعايرة أكثر كفاءة من خلال تقليل عدد المعلمات القابلة للتدريب بشكل كبير.
- تظل الأوزان المسبقة التدريب الأصلية مجمدة، مما يعني أنه يمكنك الحصول على عدة نماذج LoRA خفيفة الوزن ومحمولة لمختلف المهام النهائية المبنية عليها.
- LoRA متعامد مع طرق كفاءة المعلمات الأخرى ويمكن الجمع بينه وبين العديد منها.
- أداء النماذج المعايرة باستخدام LoRA قابل للمقارنة مع أداء النماذج المعايرة بالكامل.

من حيث المبدأ، يمكن تطبيق LoRA على أي مجموعة فرعية من مصفوفات الأوزان في شبكة عصبية لخفض عدد المعلمات القابلة للتدريب. ومع ذلك، ولأغراض البساطة وكفاءة المعلمات الإضافية، يتم تطبيق LoRA عادةً فقط على كتل الاهتمام في نماذج المحول. يعتمد عدد المعلمات القابلة للتدريب الناتجة في نموذج LoRA على حجم مصفوفات التحديث، والذي يتحدد بشكل أساسي بالرتبة `r` وشكل مصفوفة الأوزان الأصلية.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora.png"/>
</div>

<small><a href="https://hf.co/papers/2103.10385">التنقل في التخصيص من النص إلى الصورة: من ضبط دقيق لـ LyCORIS إلى تقييم النموذج</a></small>

## الضرب المنخفض الرتبة لهادامارد (LoHa)

يمكن أن يؤثر التحليل الرتبي المنخفض على الأداء لأن تحديثات الأوزان تقتصر على مساحة الرتبة المنخفضة، والتي يمكن أن تقيد تعبيرية النموذج. ومع ذلك، فأنت لا تريد بالضرورة استخدام رتبة أكبر لأنها تزيد من عدد المعلمات القابلة للتدريب. لمعالجة هذا، تم تطبيق [LoHa](https://huggingface.co/papers/2108.06098) (وهي طريقة تم تطويرها في الأصل لرؤية الكمبيوتر) على نماذج الانتشار حيث تعد القدرة على إنشاء صور متنوعة اعتبارًا مهمًا. يجب أن يعمل LoHa أيضًا مع أنواع النماذج العامة، ولكن طبقات التضمين غير مطبقة حاليًا في PEFT.

يستخدم LoHa [ضرب هادامارد](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) (ضرب العنصر) بدلاً من ضرب المصفوفة. يتم تمثيل ∆W بأربع مصفوفات أصغر بدلاً من اثنتين - كما هو الحال في LoRA - ويتم دمج كل زوج من هذه المصفوفات ذات الرتبة المنخفضة مع ضرب هادامارد. ونتيجة لذلك، يمكن أن يكون لـ ∆W نفس عدد المعلمات القابلة للتدريب ولكن برتبة أعلى وقابلية للتعبير.

## الضرب الكرونيكي منخفض الرتبة (LoKr)

[LoKr](https://hf.co/papers/2309.14859) مشابه جدًا لـ LoRA و LoHa، ويتم تطبيقه بشكل أساسي على نماذج الانتشار، على الرغم من أنه يمكنك أيضًا استخدامه مع أنواع نماذج أخرى. يستبدل LoKr ضرب المصفوفة بضرب [كرونكر](https://en.wikipedia.org/wiki/Kronecker_product) بدلاً من ذلك. ينشئ تحليل المنتج الكرونيكي مصفوفة كتلة تحافظ على رتبة مصفوفة الأوزان الأصلية. تتمثل إحدى مزايا المنتج الكرونيكي في أنه يمكن تحويله إلى ناقل عن طريق تكديس أعمدة المصفوفة. يمكن أن يسرع هذا العملية لأنك تتجنب إعادة بناء ∆W بالكامل.

## المعايرة المتعامدة (OFT)

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/oft.png"/>
</div>

<small><a href="https://hf.co/papers/2306.07280">التحكم في الانتشار من النص إلى الصورة عن طريق المعايرة المتعامدة</a></small>

[OFT](https://hf.co/papers/2306.07280) هي طريقة تركز بشكل أساسي على الحفاظ على الأداء التوليدي للنموذج المسبق التدريب في النموذج المعاير. إنه يحاول الحفاظ على نفس التشابه التماثلي (الطاقة الكروية الفائقة) بين جميع العصبونات الزوجية في طبقة لأنه يلتقط المعلومات الدلالية بين العصبونات بشكل أفضل. وهذا يعني أن OFT أكثر قدرة على الحفاظ على الموضوع وأنه أفضل للتوليد القابل للتحكم (مشابه لـ [ControlNet](https://huggingface.co/docs/diffusers/using-diffusers/controlnet)).

يحافظ OFT على الطاقة الفائقة الكروية من خلال تعلم تحويل متعامد للعصبونات للحفاظ على التشابه التماثلي بينها دون تغيير. في الممارسة العملية، يعني ذلك أخذ حاصل ضرب المصفوفة لمصفوفة متعامدة مع مصفوفة أوزان مسبقة التدريب. ومع ذلك، لتكون كفاءة المعلمات، يتم تمثيل المصفوفة المتعامدة كمصفوفة قطرية كتلية بمصفوفات رتبة `r`. في حين أن LoRA تقلل عدد المعلمات القابلة للتدريب بهياكل ذات رتبة منخفضة، فإن OFT تقلل عدد المعلمات القابلة للتدريب بهيكل مصفوفة قطرية متقطعة.

## الفراشة المتعامدة (BOFT)

[BOFT](https://hf.co/papers/2311.06243) هي طريقة تركز بشكل أساسي على الحفاظ على الأداء التوليدي للنموذج المسبق التدريب في النموذج المعاير. إنه يحاول الحفاظ على نفس التشابه التماثلي (الطاقة الكروية الفائقة) بين جميع العصبونات الزوجية في طبقة لأنه يلتقط المعلومات الدلالية بين العصبونات بشكل أفضل. وهذا يعني أن OFT أكثر قدرة على الحفاظ على الموضوع وأنه أفضل للتوليد القابل للتحكم (مشابه لـ [ControlNet](https://huggingface.co/docs/diffusers/using-diffusers/controlnet)).

يحافظ OFT على الطاقة الفائقة الكروية من خلال تعلم تحويل متعامد للعصبونات للحفاظ على التشابه التماثلي بينها دون تغيير. في الممارسة العملية، يعني ذلك أخذ حاصل ضرب المصفوفة لمصفوفة متعامدة مع مصفوفة أوزان مسبقة التدريب. ومع ذلك، لتكون كفاءة المعلمات، يتم تمثيل المصفوفة المتعامدة كمصفوفة قطرية كتلية بمصفوفات رتبة `r`. في حين أن LoRA تقلل عدد المعلمات القابلة للتدريب بهياكل ذات رتبة منخفضة، فإن OFT تقلل عدد المعلمات القابلة للتدريب بهيكل مصفوفة قطرية متقطعة.

## التكيف المنخفض الرتبة التكيفي (AdaLoRA)

[AdaLoRA](https://hf.co/papers/2303.10512) يدير ميزانية المعلمات المقدمة من LoRA عن طريق تخصيص المزيد من المعلمات - بعبارة أخرى، رتبة أعلى `r` - لمصفوفات الأوزان المهمة التي تتكيف بشكل أفضل مع المهمة وتشذيب تلك الأقل أهمية. يتم التحكم في الرتبة بواسطة طريقة مشابهة لتحليل القيمة المنفردة (SVD). يتم معلمجة ∆W بمصفوفتين متعامدتين ومصفوفة قطرية تحتوي على قيم منفردة. تتجنب طريقة المعلمجة هذه التطبيق المتكرر لـ SVD المكلف حسابياً. بناءً على هذه الطريقة، يتم ضبط رتبة ∆W وفقًا لدرجة الأهمية. يتم تقسيم ∆W إلى ثلاثيات، ويتم تقييم كل ثلاثي وفقًا لمساهمته في أداء النموذج. يتم تشذيب الثلاثيات ذات درجات الأهمية المنخفضة والاحتفاظ بالثلاثيات ذات درجات الأهمية العالية للمعايرة الدقيقة.

## محول Llama

[Llama-Adapter](https://hf.co/papers/2303.16199) هي طريقة لتكييف Llama إلى نموذج اتباع التعليمات. للمساعدة في تكييف النموذج لاتباع التعليمات، يتم تدريب المحول باستخدام مجموعة بيانات إخراج التعليمات التي تحتوي على 52 ألف تعليمات.

يتم إلحاق مجموعة من مطالبات التكيف القابلة للتعلم بالرموز المميزة للتعليمات الإدخال. يتم إدراج هذه الرموز في الطبقات العليا للنموذج لأنه من الأفضل التعلم باستخدام الدلالات عالية المستوى للنموذج المسبق التدريب. توجه رموز إخراج التعليمات الملحقة بالإدخال مطالبة التكيف لتوليد استجابة سياقية.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/llama-adapter.png"/>
</div>

<small><a href="https://hf.co/papers/2303.16199">محول LLaMA: معايرة دقيقة فعالة لنماذج اللغة باستخدام اهتمام التهيئة الصفرية</a></small>

لمنع إضافة الضوضاء إلى الرموز المميزة، يستخدم المحول اهتمام التهيئة الصفرية. بالإضافة إلى ذلك، يضيف المحول عامل بوابة قابل للتعلم (مبدئيًا بصفر) لإضافة معلومات تدريجية إلى النموذج أثناء التدريب. يمنع هذا النموذج من إرباك معرفته المسبقة بالتعليمات المكتسبة حديثًا.