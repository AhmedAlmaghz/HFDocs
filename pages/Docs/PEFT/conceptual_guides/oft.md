# Orthogonal Finetuning (OFT and BOFT)

هذا الدليل المفاهيمي يقدم نظرة عامة موجزة على [OFT](https://arxiv.org/abs/2306.07280) و [BOFT](https://arxiv.org/abs/2311.06243)، وهي تقنية ضبط دقيق فعالة للمعلمات تستخدم مصفوفة متعامدة لتحويل مصفوفات الأوزان المعلمَة مسبقًا بشكل ضربي.

لتحقيق الضبط الدقيق الفعال، يمثل OFT تحديثات الأوزان باستخدام تحويل متعامد. ويتم معلمَة التحول المتعامد بواسطة مصفوفة متعامدة يتم ضاربها في مصفوفة الأوزان المعلمَة مسبقًا. يمكن تدريب هذه المصفوفات الجديدة على التكيف مع البيانات الجديدة مع الحفاظ على العدد الإجمالي للتغييرات منخفضًا. تظل مصفوفة الأوزان الأصلية مجمدة ولا تتلقى أي تعديلات أخرى. ولإنتاج النتائج النهائية، يتم ضرب الأوزان الأصلية والمحوَّلة معًا.

يعمم Orthogonal Butterfly (BOFT) OFT باستخدام تحليل Butterfly ويحسن بشكل أكبر كفاءة معلماته ومرونته في الضبط الدقيق. وباختصار، يمكن اعتبار OFT حالة خاصة من BOFT. على عكس LoRA الذي يستخدم تحديثات الأوزان ذات الرتبة المنخفضة الإضافية، يستخدم BOFT تحديثات الأوزان المتعامدة الضربية. ويظهر المقارنة أدناه.

<div class="flex justify-center">
<img src="https://raw.githubusercontent.com/wy1iu/butterfly-oft/main/assets/BOFT_comparison.png"/>
</div>

لـ BOFT بعض المزايا مقارنة بـ LoRA:

- يقترح BOFT طريقة بسيطة وعامة لضبط دقيق للنماذج المعلمَة مسبقًا لمهام المصب، مما يؤدي إلى حفظ أفضل لمعرفة المعلمَة مسبقًا وكفاءة أفضل للمعلمات.
- من خلال التعامد، يقدم BOFT قيدًا هيكليًا، أي الحفاظ على [الطاقة الكروية الفائقة](https://arxiv.org/abs/1805.09298) دون تغيير أثناء الضبط الدقيق. يمكن أن يقلل هذا بشكل فعال من نسيان المعرفة المعلمَة مسبقًا.
- يستخدم BOFT تحليل الفراشة لمعلمَة المصفوفة المتعامدة بكفاءة، مما يؤدي إلى مساحة تعلم مدمجة ومعبرة (أي فئة افتراضية).
- يجلب تحليل المصفوفات المتناثرة في BOFT تحيزات استقرائية إضافية مفيدة للتعميم.

من حيث المبدأ، يمكن تطبيق BOFT على أي مجموعة فرعية من مصفوفات الأوزان في شبكة عصبية للحد من عدد المعلمات القابلة للتدريب. بالنظر إلى الطبقات المستهدفة لحقن معلمات BOFT، يمكن تحديد عدد المعلمات القابلة للتدريب بناءً على حجم مصفوفات الأوزان.

## دمج أوزان OFT/BOFT في النموذج الأساسي

على غرار LoRA، يمكن دمج الأوزان التي تعلمها OFT/BOFT في مصفوفات الأوزان المعلمَة مسبقًا باستخدام دالة merge_and_unload(). تدمج هذه الدالة أوزان المحول مع النموذج الأساسي، مما يتيح لك استخدام النموذج المندمج حديثًا كنموذج مستقل بفعالية.

<div class="flex justify-center">
<img src="https://raw.githubusercontent.com/wy1iu/butterfly-oft/main/assets/boft_merge.png"/>
</div>

يعمل هذا لأن المصفوفة الوزنية المتعامدة (R في الرسم التخطيطي أعلاه) ومصفوفات الأوزان المعلمَة مسبقًا منفصلة أثناء التدريب. ولكن بمجرد اكتمال التدريب، يمكن دمج هذه الأوزان بالفعل (ضربها) في مصفوفة أوزان جديدة مكافئة.

## المرافق لـ OFT / BOFT

### معلمات OFT / BOFT الشائعة في PEFT

كما هو الحال مع الطرق الأخرى التي يدعمها PEFT، لضبط نموذج دقيق باستخدام OFT أو BOFT، تحتاج إلى:

1. إنشاء مثيل لنموذج أساسي.
2. إنشاء تكوين (`OFTConfig` أو `BOFTConfig`) حيث تحدد المعلمات الخاصة بـ OFT/BOFT.
3. لف النموذج الأساسي باستخدام `get_peft_model()` للحصول على `PeftModel` قابل للتدريب.
4. تدريب `PeftModel` كما تفعل عادة تدريب النموذج الأساسي.

### المعلمات المحددة لـ BOFT

يسمح لك `BOFTConfig` بالتحكم في كيفية تطبيق OFT/BOFT على النموذج الأساسي من خلال المعلمات التالية:

- `boft_block_size`: حجم كتلة BOFT عبر الطبقات المختلفة، معبر عنها في `int`. يؤدي حجم الكتلة الأصغر إلى مصفوفات تحديث أكثر ندرة بمعلمات قابلة للتدريب أقل. **ملاحظة**، يرجى اختيار `boft_block_size` ليكون قابلاً للقسمة على البعد المدخلي لمعظم الطبقات (`in_features`)، على سبيل المثال، 4 أو 8 أو 16. أيضًا، يرجى تحديد إما `boft_block_size` أو `boft_block_num`، ولكن ليس كليهما في نفس الوقت أو تركهما عند 0، لأن `boft_block_size` x `boft_block_num` يجب أن يساوي البعد المدخلي للطبقة.

- `boft_block_num`: عدد كتل BOFT عبر الطبقات المختلفة، معبر عنها في `int`. يؤدي عدد أقل من الكتل إلى مصفوفات تحديث أكثر ندرة بمعلمات قابلة للتدريب أقل. **ملاحظة**، يرجى اختيار `boft_block_num` ليكون قابلاً للقسمة على البعد المدخلي لمعظم الطبقات (`in_features`)، على سبيل المثال، 4 أو 8 أو 16. أيضًا، يرجى تحديد إما `boft_block_size` أو `boft_block_num`، ولكن ليس كليهما في نفس الوقت أو تركهما عند 0، لأن `boft_block_size` x `boft_block_num` يجب أن يساوي البعد المدخلي للطبقة.

- `boft_n_butterfly_factor`: عدد عوامل الفراشة. **ملاحظة**، بالنسبة لـ `boft_n_butterfly_factor=1`، يكون BOFT هو نفسه OFT العادي، وبالنسبة لـ `boft_n_butterfly_factor=2`، يصبح حجم الكتلة الفعال لـ OFT ضعف الحجم وعدد الكتل نصفها.

- `bias`: حدد ما إذا كان يجب تدريب معلمات `bias`. يمكن أن يكون "none" أو "all" أو "boft_only".

- `boft_dropout`: حدد احتمال الإسقاط الضربي.

- `target_modules`: الوحدات النمطية (على سبيل المثال، كتل الاهتمام) لإدخال مصفوفات OFT/BOFT.

- `modules_to_save`: قائمة الوحدات النمطية بخلاف مصفوفات OFT/BOFT ليتم تعيينها كقابلة للتدريب ومحفوظة في نقطة التفتيش النهائية. تشمل هذه عادة رأس النموذج المخصص الذي يتم تهيئته بشكل عشوائي لمهمة الضبط الدقيق.

## مثال على استخدام BOFT

للحصول على مثال على تطبيق طريقة BOFT على مهام المصب المختلفة، يرجى الرجوع إلى الأدلة التالية:

الق نظرة على الأدلة التالية خطوة بخطوة حول كيفية ضبط نموذج دقيق باستخدام BOFT:

- [ضبط Dreambooth الدقيق باستخدام BOFT](../task_guides/boft_dreambooth)

- [الضبط الدقيق للتحكم في التوليد باستخدام BOFT (ControlNet)](../task_guides/boft_controlnet)

لمهمة تصنيف الصور، يمكن تهيئة تكوين BOFT لنموذج DinoV2 كما يلي:

```py
import transformers
from transformers import AutoModelForSeq2SeqLM, BOFTConfig
from peft import BOFTConfig, get_peft_model

config = BOFTConfig(
boft_block_size=4,
boft_n_butterfly_factor=2,
target_modules=["query", "value", "key", "output.dense", "mlp.fc1", "mlp.fc2"],
boft_dropout=0.1,
bias="boft_only",
modules_to_save=["classifier"],
)

model = transformers.Dinov2ForImageClassification.from_pretrained(
"facebook/dinov2-large",
num_labels=100,
)

boft_model = get_peft_model(model, config)
```