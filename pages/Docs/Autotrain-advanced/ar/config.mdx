# تكوينات AutoTrain

تكوينات AutoTrain هي الطريقة التي يمكن من خلالها استخدام وتدريب النماذج باستخدام AutoTrain محليًا. بمجرد تثبيت AutoTrain Advanced، يمكنك استخدام الأمر التالي لتدريب النماذج باستخدام ملفات تكوين AutoTrain:

```bash
$ export HF_USERNAME=your_hugging_face_username
$ export HF_TOKEN=your_hugging_face_write_token

$ autotrain --config path/to/config.yaml
```

يمكن العثور على تكوينات المثال لجميع المهام في دليل `configs` في [مستودع AutoTrain Advanced GitHub](https://github.com/huggingface/autotrain-advanced).

فيما يلي مثال على ملف تكوين AutoTrain:

```yaml
task: llm
base_model: meta-llama/Meta-Llama-3-8B-Instruct
project_name: autotrain-llama3-8b-orpo
log: tensorboard
backend: local

data:
path: argilla/distilabel-capybara-dpo-7k-binarized
train_split: train
valid_split: null
chat_template: chatml
column_mapping:
text_column: chosen
rejected_text_column: rejected

params:
trainer: orpo
block_size: 1024
model_max_length: 2048
max_prompt_length: 512
epochs: 3
batch_size: 2
lr: 3e-5
peft: true
quantization: int4
target_modules: all-linear
padding: right
optimizer: adamw_torch
scheduler: linear
gradient_accumulation: 4
mixed_precision: bf16

hub:
username: ${HF_USERNAME}
token: ${HF_TOKEN}
push_to_hub: true
```

في هذا التكوين، نقوم بتعديل دقيق لنموذج `meta-llama/Meta-Llama-3-8B-Instruct` على مجموعة البيانات `argilla/distilabel-capybara-dpo-7k-binarized` باستخدام مدرب `orpo` لعدد 3 حقبات مع حجم دفعة 2 ومعدل تعلم `3e-5`. يمكن العثور على مزيد من المعلومات حول المعلمات المتاحة في قسم "تنسيقات البيانات والمعلمات".

إذا كنت لا تريد دفع النموذج إلى المركز، فيمكنك تعيين `push_to_hub` على `false` في ملف التكوين. إذا لم يتم دفع النموذج إلى المركز، فلن تكون أسماء المستخدمين والرموز المميزة مطلوبة. ملاحظة: قد تكون لا تزال هناك حاجة إليها إذا كنت تحاول الوصول إلى النماذج أو مجموعات البيانات المحظورة.