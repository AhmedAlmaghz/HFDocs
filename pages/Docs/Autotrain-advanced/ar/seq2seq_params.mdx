# معلمات Seq2Seq 

```
--batch-size BATCH_SIZE
حجم الدفعة المستخدمة في التدريب
--seed SEED           البذرة العشوائية لإمكانية إعادة الإنتاج
--epochs EPOCHS       عدد دورات التدريب
--gradient_accumulation GRADIENT_ACCUMULATION
خطوات تجميع التدرجات
--disable_gradient_checkpointing
تعطيل نقاط تفتيش التدرجات
--lr LR               معدل التعلم
--log {none,wandb,tensorboard}
استخدم تتبع التجربة
--text-column TEXT_COLUMN
حدد اسم العمود في مجموعة البيانات الذي يحتوي على بيانات النص. مفيد للتمييز بين حقول النص المتعددة.
الافتراضي هو 'text'.
--target-column TARGET_COLUMN
حدد اسم العمود الذي يحتوي على بيانات الهدف للتدريب. يساعد في التمييز بين المخرجات المحتملة المختلفة.
الافتراضي هو 'target'.
--max-seq-length MAX_SEQ_LENGTH
قم بتعيين أقصى طول للتسلسل (عدد الرموز) الذي يجب أن يتعامل معه النموذج في إدخال واحد. يتم اقتطاع التسلسلات الأطول. يؤثر ذلك على كل من استخدام الذاكرة والمتطلبات الحسابية. الافتراضي هو 128 رمزًا.
--max-target-length MAX_TARGET_LENGTH
قم بتعريف الحد الأقصى لعدد الرموز لتسلسل الهدف في كل إدخال. مفيد للنماذج التي تولد مخرجات، مما يضمن التوحيد في طول التسلسل. الافتراضي هو 128 رمزًا.
--warmup-ratio WARMUP_RATIO
قم بتعريف نسبة التدريب المخصصة للتسخين الخطي حيث يزيد معدل التعلم تدريجياً. يمكن أن يساعد ذلك في استقرار عملية التدريب في وقت مبكر. نسبة الافتراضي هي 0.1.
--optimizer OPTIMIZER
اختر خوارزمية المحسن لتدريب النموذج. يمكن أن تؤثر المحسنات المختلفة على سرعة التدريب وأداء النموذج. الافتراضي هو 'adamw_torch'.
--scheduler SCHEDULER
حدد جدول معدل التعلم لتعديل معدل التعلم بناءً على عدد دورات التدريب. يقلل "linear" من معدل التعلم الخطي من معدل التعلم الأولي المحدد. الافتراضي هو "linear". جرب "cosine" لجدول انخفاض معدل التعلم.
--weight-decay WEIGHT_DECAY
قم بتعيين معدل انخفاض الوزن لتطبيقه على الانتظام. يساعد في منع النموذج من الإفراط في التكيّف عن طريق معاقبة الأوزان الكبيرة. الافتراضي هو 0.0، مما يعني عدم تطبيق انخفاض الوزن.
--max-grad-norm MAX_GRAD_NORM
حدد القيمة العظمى لمعيار التدرجات من أجل قص التدرجات. يستخدم قص التدرجات لمنع مشكلة انفجار التدرجات في الشبكات العصبية العميقة. الافتراضي هو 1.0.
--logging-steps LOGGING_STEPS
تحديد مدى تكرار تسجيل تقدم التدريب. قم بتعيين هذا على عدد الخطوات بين كل إخراج للسجل. تحدد -1 خطوات السجل تلقائيًا. الافتراضي هو -1.
--eval-strategy eval_strategy
حدد مدى تكرار تقييم أداء النموذج. تشمل الخيارات "no" و"steps" و"epoch". يقوم "epoch" بشكل افتراضي بالتقييم في نهاية كل دورة تدريب.
--save-total-limit SAVE_TOTAL_LIMIT
حدد الحد الأقصى لعدد نقاط تفتيش النموذج التي سيتم حفظها. يساعد في إدارة مساحة القرص عن طريق الاحتفاظ فقط بأحدث نقاط التفتيش. الافتراضي هو حفظ أحدث نقطة تفتيش فقط.
--auto-find-batch-size
تمكين تحديد حجم الدفعة التلقائي بناءً على قدرات الأجهزة الخاصة بك. عندما يتم تعيينه، فإنه يحاول العثور على أكبر حجم دفعة يناسب الذاكرة.
--mixed-precision {fp16,bf16,None}
اختر وضع الدقة للتدريب لتحسين الأداء واستخدام الذاكرة. الخيارات هي 'fp16' أو 'bf16' أو None للدقة الافتراضية. الافتراضي هو None.
--peft                تمكين LoRA-PEFT
--quantization {int8,None}
حدد وضع التقريب لخفض حجم النموذج وزيادة سرعة الاستدلال المحتملة. تشمل الخيارات "int8" للتقريب الصحيح بـ 8 بت أو None لعدم التقريب. الافتراضي هو None
--lora-r LORA_R       تعيين الرتبة "R" لتقنية LoRA (Low-Rank Adaptation). الافتراضي هو 16.
--lora-alpha LORA_ALPHA
حدد معلمة "Alpha" لتقنية LoRA. الافتراضي هو 32.
--lora-dropout LORA_DROPOUT
تحديد معدل الإسقاط لتطبيقه في طبقات LoRA، والذي يمكن أن يساعد في منع الإفراط في التكيّف عن طريق تعطيل جزء عشوائي من العصبونات أثناء التدريب. معدل الافتراضي هو 0.05.
--target-modules TARGET_MODULES
قم بإدراج الوحدات ضمن بنية النموذج التي يجب استهدافها لتقنيات محددة مثل تكييفات LoRA. مفيد لضبط مكونات محددة من النماذج الكبيرة. يتم استهداف جميع الطبقات الخطية بشكل افتراضي.
```