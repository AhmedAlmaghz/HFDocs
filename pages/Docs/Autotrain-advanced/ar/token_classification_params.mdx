هذا النص يصف معلمات تصنيف الرموز المميزة (Token Classification) التي تستخدم في تدريب نماذج معالجة اللغات الطبيعية.

# معلمات تصنيف الرموز المميزة 

```
--batch-size BATCH_SIZE
حجم دفعة التدريب التي سيتم استخدامها
--seed SEED           البذرة العشوائية لإمكانية إعادة الإنتاج
--epochs EPOCHS       عدد دورات التدريب
--gradient_accumulation GRADIENT_ACCUMULATION
خطوات تجميع التدرجات
--disable_gradient_checkpointing
تعطيل نقاط تفتيش التدرجات
--lr LR               معدل التعلم
--log {none,wandb,tensorboard}
استخدام تتبع التجربة
--tokens-column TOKENS_COLUMN
عمود الرموز المميزة الذي سيتم استخدامه. يجب أن يكون قائمة من الرموز المميزة المتسلسلة إذا كنت تستخدم ملف CSV. الافتراضي هو 'tokens'.
--tags-column TAGS_COLUMN
عمود العلامات الذي سيتم استخدامه. يجب أن تكون قائمة من العلامات المتسلسلة إذا كنت تستخدم ملف CSV. الافتراضي هو 'tags'.
--max-seq-length MAX_SEQ_LENGTH
تعيين أقصى طول للتسلسل (عدد الرموز المميزة) الذي يجب أن يتعامل معه النموذج في إدخال واحد. يتم اقتطاع التسلسلات الأطول. يؤثر هذا على كل من استخدام الذاكرة والمتطلبات الحسابية. الافتراضي هو 128 رمزًا مميزًا.
--warmup-ratio WARMUP_RATIO
تحديد نسبة التدريب المخصصة للتسخين الخطي حيث يزيد معدل التعلم تدريجياً. يمكن أن يساعد ذلك في استقرار عملية التدريب في وقت مبكر. نسبة الافتراضي هي 0.1.
--optimizer OPTIMIZER
اختيار خوارزمية المحسن لتدريب النموذج. يمكن أن تؤثر المحسنات المختلفة على سرعة التدريب وأداء النموذج. الافتراضي هو 'adamw_torch'.
--scheduler SCHEDULER
تحديد جدول معدل التعلم لتعديل معدل التعلم بناءً على عدد دورات التدريب. يقلل 'linear' من معدل التعلم بشكل خطي من القيمة الأولية المحددة. الافتراضي هو 'linear'. جرّب 'cosine' لجدول انخفاض معدل التعلم وفقًا لجدول Cosine Annealing.
--weight-decay WEIGHT_DECAY
تعيين معدل انخفاض الوزن لتطبيق الانتظام. يساعد في منع الإفراط في ملاءمة النموذج عن طريق معاقبة الأوزان الكبيرة. الافتراضي هو 0.0، مما يعني عدم تطبيق انخفاض الوزن.
--max-grad-norm MAX_GRAD_NORM
تحديد الحد الأقصى لقيمة التدرجات من أجل قص التدرجات. يستخدم قص التدرجات لمنع مشكلة انفجار التدرجات في الشبكات العصبية العميقة. القيمة الافتراضية هي 1.0.
--logging-steps LOGGING_STEPS
تحديد مدى تكرار تسجيل تقدم التدريب. قم بتعيين هذا إلى عدد الخطوات بين كل إخراج للسجل. تقوم القيمة الافتراضية -1 بتحديد خطوات السجل تلقائيًا.
--eval-strategy {steps,epoch,no}
تحديد مدى تكرار تقييم أداء النموذج. تشمل الخيارات 'no' و 'steps' و 'epoch'. يقوم 'epoch' بشكل افتراضي بالتقييم في نهاية كل دورة تدريب.
--save-total-limit SAVE_TOTAL_LIMIT
تحديد الحد الأقصى لعدد نقاط تفتيش النموذج التي سيتم حفظها. يساعد ذلك في إدارة مساحة القرص عن طريق الاحتفاظ فقط بأحدث نقاط التفتيش. الافتراضي هو حفظ أحدث نقطة تفتيش فقط.
--auto-find-batch-size
تمكين تحديد حجم الدفعة التلقائي بناءً على قدرات الأجهزة لديك. عند تعيينه، يحاول العثور على أكبر حجم دفعة يناسب الذاكرة.
--mixed-precision {fp16,bf16,None}
اختيار وضع الدقة للتدريب لتحسين الأداء واستخدام الذاكرة. الخيارات هي 'fp16' أو 'bf16' أو None للدقة الافتراضية. الافتراضي هو None.
```