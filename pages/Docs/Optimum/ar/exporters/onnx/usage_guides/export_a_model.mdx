# ุชุตุฏูุฑ ูููุฐุฌ ุฅูู ONNX ุจุงุณุชุฎุฏุงู optimum.exporters.onnx

## ููุฎุต

ุชุตุฏูุฑ ูููุฐุฌ ุฅูู ONNX ุจุณูุท ูุซู:

```bash
optimum-cli export onnx --model gpt2 gpt2_onnx/
```

ุฑุงุฌุน ุงููุณุงุนุฏุฉ ููุญุตูู ุนูู ุงููุฒูุฏ ูู ุงูุฎูุงุฑุงุช:

```bash
optimum-cli export onnx --help
```

## ููุงุฐุง ุชุณุชุฎุฏู ONNXุ

ุฅุฐุง ููุช ุจุญุงุฌุฉ ุฅูู ูุดุฑ ููุงุฐุฌ ๐ค Transformers ุฃู ๐ค Diffusers ูู ุจูุฆุงุช ุงูุฅูุชุงุฌุ ูุฅููุง ููุตู ุจุชุตุฏูุฑูุง ุฅูู ุชูุณูู ูุณูุณู ูููู ุชุญูููู ูุชุดุบููู ุนูู ุฃุฌูุฒุฉ ูุจุฑุงูุฌ ูุชุฎุตุตุฉ. ูู ูุฐุง ุงูุฏูููุ ุณูุฑููู ููููุฉ ุชุตุฏูุฑ ูุฐู ุงูููุงุฐุฌ ุฅูู [ONNX (Open Neural Network eXchange)](http://onnx.ai).

ONNX ูู ูุนูุงุฑ ููุชูุญ ูุญุฏุฏ ูุฌููุนุฉ ูุดุชุฑูุฉ ูู ุงููุดุบููู ูุชูุณูู ููู ูุดุชุฑู ูุชูุซูู ููุงุฐุฌ ุงูุชุนูู ุงูุนููู ูู ูุฌููุนุฉ ูุชููุนุฉ ูู ุงูุฃุทุฑุ ุจูุง ูู ุฐูู PyTorch ูTensorFlow. ุนูุฏูุง ูุชู ุชุตุฏูุฑ ูููุฐุฌ ุฅูู ุชูุณูู ONNXุ ูุชู ุงุณุชุฎุฏุงู ูุฐู ุงููุดุบููู ูุจูุงุก ุฑุณู ุจูุงูู ุญุณุงุจู (ููุทูู ุนููู ุบุงูุจูุง ุงุณู _ุชูุซูู ูุณูุท_) ูุงูุฐู ููุซู ุชุฏูู ุงูุจูุงูุงุช ุนุจุฑ ุงูุดุจูุฉ ุงูุนุตุจูุฉ.

ูู ุฎูุงู ุนุฑุถ ุฑุณู ุจูุงูู ุจูุดุบููู ูุฃููุงุน ุจูุงูุงุช ููุญุฏุฉุ ุชุฌุนู ONNX ูู ุงูุณูู ุงูุชุจุฏูู ุจูู ุงูุฃุทุฑ. ุนูู ุณุจูู ุงููุซุงูุ ูููู ุชุตุฏูุฑ ูููุฐุฌ ูุฏุฑุจ ูู PyTorch ุฅูู ุชูุณูู ONNX ุซู ุงุณุชูุฑุงุฏู ูู TensorRT ุฃู OpenVINO.

<Tip>
ุจูุฌุฑุฏ ุชุตุฏูุฑูุ ูููู ุชุญุณูู ุงููููุฐุฌ ููุงุณุชุฏูุงู ูู ุฎูุงู ุชูููุงุช ูุซู ุชุญุณูู ุงูุฑุณู ุงูุจูุงูู ูุงูุชุญุฌูู. ุชุญูู ูู ุญุฒูุฉ `optimum.onnxruntime` ูุชุญุณูู ููุงุฐุฌ ONNX ูุชุดุบูููุง!
</Tip>

ูููุฑ ๐ค Optimum ุงูุฏุนู ูุชุตุฏูุฑ ONNX ูู ุฎูุงู ุงูุงุณุชูุงุฏุฉ ูู ูุงุฆูุงุช ุงูุชูููู.
ุชุฃุชู ูุงุฆูุงุช ุงูุชูููู ูุฐู ุฌุงูุฒุฉ ูุนุฏุฏ ูู ููุฏุณุงุช ุงูููุงุฐุฌุ ููุฏ ุชู ุชุตููููุง ูุชููู ูุงุจูุฉ ููุชูุณูุน ุจุณูููุฉ ุฅูู ููุฏุณุงุช ุฃุฎุฑู.

**ููุชุญูู ูู ุงูููุฏุณุงุช ุงููุฏุนููุฉุ ุงูุชูู ุฅูู [ุตูุญุฉ ุงููุฑุฌุน](../package_reference/configuration#supported-architectures).**

## ุชุตุฏูุฑ ูููุฐุฌ ุฅูู ONNX ุจุงุณุชุฎุฏุงู CLI

ูุชุตุฏูุฑ ูููุฐุฌ ๐ค Transformers ุฃู ๐ค Diffusers ุฅูู ONNXุ ุณุชุญุชุงุฌ ุฃููุงู ุฅูู ุชุซุจูุช ุจุนุถ ุงูุชุจุนูุงุช ุงูุฅุถุงููุฉ:

```bash
pip install optimum[exporters]
```

ูููู ุงุณุชุฎุฏุงู ุชุตุฏูุฑ ONNX Optimum ูู ุฎูุงู ุณุทุฑ ุฃูุงูุฑ Optimum:

```bash
optimum-cli export onnx --help

usage: optimum-cli <command> [<args>] export onnx [-h] -m MODEL [--task TASK] [--monolith] [--device DEVICE] [--opset OPSET] [--atol ATOL]
[--framework {pt,tf}] [--pad_token_id PAD_TOKEN_ID] [--cache_dir CACHE_DIR] [--trust-remote-code]
[--no-post-process] [--optimize {O1,O2,O3,O4}] [--batch_size BATCH_SIZE]
[--sequence_length SEQUENCE_LENGTH] [--num_choices NUM_CHOICES] [--width WIDTH] [--height HEIGHT]
[--num_channels NUM_CHANNELS] [--feature_size FEATURE_SIZE] [--nb_max_frames NB_MAX_FRAMES]
[--audio_sequence_length AUDIO_SEQUENCE_LENGTH]
output

ุงูุญุฌุฌ ุงูุงุฎุชูุงุฑูุฉ:
-hุ --help ุฅุธูุงุฑ ุฑุณุงูุฉ ุงููุณุงุนุฏุฉ ูุฐู ูุงูุฎุฑูุฌ

ุงูุญุฌุฌ ุงููุทููุจุฉ:
-m MODELุ --model MODEL
ูุนุฑู ุงููููุฐุฌ ุนูู huggingface.co ุฃู ุงููุณุงุฑ ุนูู ุงููุฑุต ูุชุญููู ุงููููุฐุฌ ููู.
ุงูุฅุฎุฑุงุฌ ูุณุงุฑ ูุดูุฑ ุฅูู ุงูุฏููู ุญูุซ ุณูุชู ุชุฎุฒูู ูููุฐุฌ ONNX ุงููููุฏ.

ุงูุญุฌุฌ ุงูุงุฎุชูุงุฑูุฉ:
--task TASK ุงููููุฉ ูุชุตุฏูุฑ ุงููููุฐุฌ ููุง. ุฅุฐุง ูู ูุชู ุชุญุฏูุฏูุ ูุณูุชู ุงุณุชูุชุงุฌ ุงููููุฉ ุชููุงุฆููุง ุจูุงุกู ุนูู ุงููููุฐุฌ. ุชุฎุชูู ุงูููุงู ุงููุชุงุญุฉ ุญุณุจ ุงููููุฐุฌุ ูููููุง ูู ุจูู: ['default'ุ 'fill-mask'ุ 'text-generation'ุ 'text2text-generation'ุ 'text-classification'ุ 'token-classification'ุ 'multiple-choice'ุ 'object-detection'ุ 'question-answering'ุ 'image-classification'ุ 'image-segmentation'ุ 'masked-im'ุ 'semantic-segmentation'ุ 'automatic-speech-recognition'ุ 'audio-classification'ุ 'audio-frame-classification'ุ 'automatic-speech-recognition'ุ 'audio-xvector'ุ 'image-to-text'ุ 'stable-diffusion'ุ 'zero-shot-object-detection']. ุจุงููุณุจุฉ ูููุงุฐุฌ ูู ุงูุชุดููุฑุ ุงุณุชุฎุฏู `xxx-with-past` ูุชุตุฏูุฑ ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู ููู ุงูููุชุงุญ ุงููุงุถู ูู ูู ุงูุชุดููุฑ.
--monolith ูุฑุถ ุชุตุฏูุฑ ุงููููุฐุฌ ูููู ONNX ูุงุญุฏ. ุจุดูู ุงูุชุฑุงุถูุ ูุฏ ูููู ูุตุฏุฑ ONNX ุจุชูุณูู ุงููููุฐุฌ ุฅูู ุนุฏุฉ ูููุงุช ONNXุ ุนูู ุณุจูู ุงููุซุงู ุจุงููุณุจุฉ ูููุงุฐุฌ ุงูุชุฑููุฒ ููู ุงูุชุดููุฑ ุญูุซ ูุฌุจ ุชุดุบูู ุงูุชุฑููุฒ ูุฑุฉ ูุงุญุฏุฉ ููุท ุจูููุง ูุชู ุชุดุบูู ูู ุงูุชุดููุฑ ุนุฏุฉ ูุฑุงุช.
--device DEVICE ุงูุฌูุงุฒ ุงูุฐู ุณูุชู ุงุณุชุฎุฏุงูู ููุชุตุฏูุฑ. ุงูุงูุชุฑุงุถู ูู "cpu".
--opset OPSET ุฅุฐุง ุชู ุชุญุฏูุฏูุ ุฅุตุฏุงุฑ ONNX opset ูุชุตุฏูุฑ ุงููููุฐุฌ ุจู. ูุฅูุงุ ุณูุชู ุงุณุชุฎุฏุงู ุงูุฅุตุฏุงุฑ ุงูุงูุชุฑุงุถู ูู opset.
--atol ATOL ุฅุฐุง ุชู ุชุญุฏูุฏูุ ูุฅู ุงููุฑู ุงููุทูู ูู ุงูุชุณุงูุญ ุนูุฏ ุงูุชุญูู ูู ุตุญุฉ ุงููููุฐุฌ. ูุฅูุงุ ุณูุชู ุงุณุชุฎุฏุงู atol ุงูุงูุชุฑุงุถู ูููููุฐุฌ.
--framework {ptุ tf} ุงูุฅุทุงุฑ ุงูุฐู ุณูุชู ุงุณุชุฎุฏุงูู ูุชุตุฏูุฑ ONNX. ุฅุฐุง ูู ูุชู ุชูููุฑูุ ูุณูุชู ูุญุงููุฉ ุงุณุชุฎุฏุงู ุงูุฅุทุงุฑ ุงูุฃุตูู ูููุทุฉ ุงูุชูุชูุด ุงููุญููุฉ ุฃู ูุง ูู ูุชุงุญ ูู ุงูุจูุฆุฉ.
--pad_token_id PAD_TOKEN_ID
ูุฐุง ูุทููุจ ูู ูุจู ุจุนุถ ุงูููุงุฐุฌุ ูุจุนุถ ุงูููุงู. ุฅุฐุง ูู ูุชู ุชูููุฑูุ ูุณูุชู ูุญุงููุฉ ุงุณุชุฎุฏุงู ุงูุฑูุฒ ุงููููุฒ ูุงุณุชูุชุงุฌู.
--cache_dir CACHE_DIR
ูุณุงุฑ ูุดูุฑ ุฅูู ุงูููุงู ุงูุฐู ุณูุชู ุชุฎุฒูู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ููู.
--trust-remote-code ูุณูุญ ุจุงุณุชุฎุฏุงู ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉ ุงููุฎุตุตุฉ ููููุฐุฌุฉ ุงููุณุชุถุงูุฉ ูู ูุณุชูุฏุน ุงููููุฐุฌ. ูุฌุจ ุชุนููู ูุฐุง ุงูุฎูุงุฑ ููุท ูููุณุชูุฏุนุงุช ุงูุชู ุชุซู ุจูุง ูุงูุชู ูุฑุฃุช ูููุง ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉุ ุญูุซ ุณูุชู ุชูููุฐ ุฃู ุชุนูููุงุช ุจุฑูุฌูุฉ ููุฌูุฏุฉ ูู ูุณุชูุฏุน ุงููููุฐุฌ ุนูู ุฌูุงุฒู ุงููุญูู.
--no-post-process ูุณูุญ ุจุชุนุทูู ุฃู ูุนุงูุฌุฉ ูุงุญูุฉ ุชุชู ุจุดูู ุงูุชุฑุงุถู ุนูู ููุงุฐุฌ ONNX ุงููุตุฏุฑุฉ. ุนูู ุณุจูู ุงููุซุงูุ ุฏูุฌ ููุงุฐุฌ ูู ุงูุชุดููุฑ ููู ุงูุชุดููุฑ ุจุงุณุชุฎุฏุงู ูููุงุช ุงููุงุถู ูู ููู ONNX ูุงุญุฏ ูุชูููู ุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉ.
--optimize {O1ุ O2ุ O3ุ O4}
ูุณูุญ ุจุชุดุบูู ุชุญุณููุงุช ููุช ุชุดุบูู ONNX ูุจุงุดุฑุฉ ุฃุซูุงุก ุงูุชุตุฏูุฑ. ุจุนุถ ูุฐู ุงูุชุญุณููุงุช ุฎุงุตุฉ ุจู ONNX Runtimeุ ููู ูููู ONNX ุงููุงุชุฌ ูุงุจู ููุงุณุชุฎุฏุงู ูุน ููุช ุชุดุบูู ุขุฎุฑ ูุซู OpenVINO ุฃู TensorRT. ุงูุฎูุงุฑุงุช ุงูููููุฉ:
- O1: ุชุญุณููุงุช ุนุงูุฉ ุฃุณุงุณูุฉ
- O2: ุชุญุณููุงุช ุนุงูุฉ ุฃุณุงุณูุฉ ูููุชุฏุฉุ ุนูููุงุช ุฏูุฌ ูุญููุงุช ูุญุฏุฏุฉ
- O3: ููุณ O2 ูุน ุชูุฑูุจ GELU
- O4: ููุณ O3 ูุน ุงูุฏูุฉ ุงููุฎุชูุทุฉ (fp16ุ GPU ููุทุ ูุชุทูุจ `--device cuda`)

```

```bash
optimum-cli export onnx --help

usage: optimum-cli <command> [<args>] export onnx [-h] -m MODEL [--task TASK] [--monolith] [--device DEVICE] [--opset OPSET] [--atol ATOL]
                                                  [--framework {pt,tf}] [--pad_token_id PAD_TOKEN_ID] [--cache_dir CACHE_DIR] [--trust-remote-code]
                                                  [--no-post-process] [--optimize {O1,O2,O3,O4}] [--batch_size BATCH_SIZE]
                                                  [--sequence_length SEQUENCE_LENGTH] [--num_choices NUM_CHOICES] [--width WIDTH] [--height HEIGHT]
                                                  [--num_channels NUM_CHANNELS] [--feature_size FEATURE_SIZE] [--nb_max_frames NB_MAX_FRAMES]
                                                  [--audio_sequence_length AUDIO_SEQUENCE_LENGTH]
                                                  output

optional arguments:
  -h, --help            show this help message and exit

Required arguments:
  -m MODEL, --model MODEL
                        Model ID on huggingface.co or path on disk to load model from.
  output                Path indicating the directory where to store generated ONNX model.

Optional arguments:
  --task TASK           The task to export the model for. If not specified, the task will be auto-inferred based on the model. Available tasks depend on the model, but are among: ['default', 'fill-mask', 'text-generation', 'text2text-generation', 'text-classification', 'token-classification', 'multiple-choice', 'object-detection', 'question-answering', 'image-classification', 'image-segmentation', 'masked-im', 'semantic-segmentation', 'automatic-speech-recognition', 'audio-classification', 'audio-frame-classification', 'automatic-speech-recognition', 'audio-xvector', 'image-to-text', 'stable-diffusion', 'zero-shot-object-detection']. For decoder models, use `xxx-with-past` to export the model using past key values in the decoder.
  --monolith            Force to export the model as a single ONNX file. By default, the ONNX exporter may break the model in several ONNX files, for example for encoder-decoder models where the encoder should be run only once while the decoder is looped over.
  --device DEVICE       The device to use to do the export. Defaults to "cpu".
  --opset OPSET         If specified, ONNX opset version to export the model with. Otherwise, the default opset will be used.
  --atol ATOL           If specified, the absolute difference tolerance when validating the model. Otherwise, the default atol for the model will be used.
  --framework {pt,tf}   The framework to use for the ONNX export. If not provided, will attempt to use the local checkpoint's original framework or what is available in the environment.
  --pad_token_id PAD_TOKEN_ID
                        This is needed by some models, for some tasks. If not provided, will attempt to use the tokenizer to guess it.
  --cache_dir CACHE_DIR
                        Path indicating where to store cache.
  --trust-remote-code   Allows to use custom code for the modeling hosted in the model repository. This option should only be set for repositories you trust and in which you have read the code, as it will execute on your local machine arbitrary code present in the model repository.
  --no-post-process     Allows to disable any post-processing done by default on the exported ONNX models. For example, the merging of decoder and decoder-with-past models into a single ONNX model file to reduce memory usage.
  --optimize {O1,O2,O3,O4}
                        Allows to run ONNX Runtime optimizations directly during the export. Some of these optimizations are specific to ONNX Runtime, and the resulting ONNX will not be usable with other runtime as OpenVINO or TensorRT. Possible options:
                            - O1: Basic general optimizations
                            - O2: Basic and extended general optimizations, transformers-specific fusions
                            - O3: Same as O2 with GELU approximation
                            - O4: Same as O3 with mixed precision (fp16, GPU-only, requires `--device cuda`)

```

ูููู ุชุตุฏูุฑ ููุทุฉ ุชูุชูุด ููุง ููู:

```bash
optimum-cli export onnx --model distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/
```

ูุฌุจ ุฃู ุชุดุงูุฏ ุงูุณุฌูุงุช ุงูุชุงููุฉ (ุฌูุจูุง ุฅูู ุฌูุจ ูุน ุณุฌูุงุช PyTorch / TensorFlow ุงููุญุชููุฉ ุงูุชู ุชู ุฅุฎูุงุคูุง ููุง ูู ุฃุฌู ุงููุถูุญ):

```bash
ุงููุดู ุงูุชููุงุฆู ุนู ุงููููุฉ ููุงุณุชุฌูุงุจ.
ูู ูุชู ุชุญุฏูุฏ ุงูุฅุทุงุฑ. ุงุณุชุฎุฏุงู pt ูุชุตุฏูุฑ ุงููููุฐุฌ.
ุงุณุชุฎุฏุงู ุฅุทุงุฑ PyTorch: 1.12.1

ุงูุชุญูู ูู ุตุญุฉ ูููุฐุฌ ONNX...
-[โ] ุชุทุงุจู ุฃุณูุงุก ูููุฐุฌ ONNX ุงูุฅุฎุฑุงุฌ ูููุฐุฌ ุงููุฑุฌุน (start_logitsุ end_logits)
- ุงูุชุญูู ูู ุตุญุฉ ุฅุฎุฑุงุฌ ูููุฐุฌ ONNX "start_logits":
-[โ] (2ุ 16) ุชุชุทุงุจู ูุน (2ุ 16)
-[โ] ุฌููุน ุงูููู ูุฑูุจุฉ (atol: 0.0001)
- ุงูุชุญูู ูู ุตุญุฉ ุฅุฎุฑุงุฌ ูููุฐุฌ ONNX "end_logits":
-[โ] (2ุ 16) ุชุชุทุงุจู ูุน (2ุ 16)
-[โ] ุฌููุน ุงูููู ูุฑูุจุฉ (atol: 0.0001)
ูู ุดูุก ุนูู ูุง ูุฑุงูุ ุชู ุญูุธ ุงููููุฐุฌ ูู: distilbert_base_uncased_squad_onnx/model.onnx
```

```bash
Automatic task detection to question-answering.
Framework not specified. Using pt to export the model.
Using framework PyTorch: 1.12.1

Validating ONNX model...
        -[โ] ONNX model output names match reference model (start_logits, end_logits)
        - Validating ONNX Model output "start_logits":
                -[โ] (2, 16) matches (2, 16)
                -[โ] all values close (atol: 0.0001)
        - Validating ONNX Model output "end_logits":
                -[โ] (2, 16) matches (2, 16)
                -[โ] all values close (atol: 0.0001)
All good, model saved at: distilbert_base_uncased_squad_onnx/model.onnx
```

ูุฐุง ูุตุฏุฑ ุฑุณู ุจูุงูู ONNX ูููุทุฉ ุงูุชูุชูุด ุงูุชู ุญุฏุฏูุง ุงูุญุฌุฉ `--model`.

ููุง ุชุฑููุ ุชู ุงูุชุดุงู ุงููููุฉ ุชููุงุฆููุง. ูุงู ูุฐุง ูููููุง ูุฃู ุงููููุฐุฌ ูุงู ุนูู Hub.

ุจุงููุณุจุฉ ููููุงุฐุฌ ุงููุญููุฉุ ูุฅู ุชูููุฑ ุญุฌุฉ `--task` ุฃูุฑ ุถุฑูุฑู ุฃู ุณูุชู ุชุนูููู ุงูุชุฑุงุถููุง ุฅูู ููุฏุณุฉ ุงููููุฐุฌ ุฏูู ุฃู ุฑุฃุณ ูููุฉ ูุญุฏุฏุฉ:

```bash
optimum-cli export onnx --model local_path --task question-answering distilbert_base_uncased_squad_onnx/
```

ูุงุญุธ ุฃู ุชูููุฑ ุญุฌุฉ `--task` ููููุฐุฌ ุนูู Hub ุณูุคุฏู ุฅูู ุชุนุทูู ุงูุชุดุงู ุงููููุฉ ุงูุชููุงุฆู.

ูููู ุจุนุฏ ุฐูู ุชุดุบูู ููู `model.onnx` ุงููุงุชุฌ ุนูู ุฃุญุฏ [ุงููุณุฑุนุงุช](https://onnx.ai/supported-tools.html#deployModel) ุงูุนุฏูุฏุฉ ุงูุชู ุชุฏุนู ูุนูุงุฑ ONNX. ุนูู ุณุจูู ุงููุซุงูุ ูููููุง ุชุญููู ุงููููุฐุฌ ูุชุดุบููู ุจุงุณุชุฎุฏุงู [ONNX Runtime](https://onnxruntime.ai/) ุจุงุณุชุฎุฏุงู ุญุฒูุฉ `optimum.onnxruntime` ููุง ููู:

```python
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import ORTModelForQuestionAnswering

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert_base_uncased_squad_onnx")  # doctest: +SKIP
>>> model = ORTModelForQuestionAnswering.from_pretrained("distilbert_base_uncased_squad_onnx")  # doctest: +SKIP

>>> inputs = tokenizer("What am I using?", "Using DistilBERT with ONNX Runtime!", return_tensors="pt")  # doctest: +SKIP
>>> outputs = model(**inputs)  # doctest: +SKIP
```

ุณูุนุทู ุทุจุงุนุฉ ุงูุฅุฎุฑุงุฌุงุช ูุง ููู:

```bash
QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-4.7652, -1.0452, -7.0409, -4.6864, -4.0277, -6.2021, -4.9473,  2.6287,
          7.6111, -1.2488, -2.0551, -0.9350,  4.9758, -0.7707,  2.1493, -2.0703,
         -4.3232, -4.9472]]), end_logits=tensor([[ 0.4382, -1.6502, -6.3654, -6.0661, -4.1482, -3.5779, -0.0774, -3.6168,
         -1.8750, -2.8910,  6.2582,  0.5425, -3.7699,  3.8232, -1.5073,  6.2311,
          3.3604, -0.0772]]), hidden_states=None, attentions=None)
```

ููุง ุชุฑููุ ูุฅู ุชุญููู ูููุฐุฌ ุฅูู ONNX ูุง ูุนูู ูุบุงุฏุฑุฉ ูุธุงู Hugging Face ุงูุจูุฆู. ุชูุชูู ุจู ุงูุฃูุฑ ูุน ูุงุฌูุฉ ุจุฑูุฌุฉ ุชุทุจููุงุช ููุงุซูุฉ ูููุงุฐุฌ ๐ค Transformers ุงูุนุงุฏูุฉ!

<Tip>
ูู ุงููููู ุฃูุถูุง ุชุตุฏูุฑ ุงููููุฐุฌ ุฅูู ONNX ูุจุงุดุฑุฉ ูู ูุฆุฉ `ORTModelForQuestionAnswering` ุนู ุทุฑูู ุงูููุงู ุจูุง ููู:

```python
>>> model = ORTModelForQuestionAnswering.from_pretrained("distilbert-base-uncased-distilled-squad", export=True)
```

ููุญุตูู ุนูู ูุฒูุฏ ูู ุงููุนูููุงุชุ ุฑุงุฌุน ุตูุญุฉ ูุซุงุฆู `optimum.onnxruntime` [ุญูู ูุฐุง ุงูููุถูุน](/onnxruntime/overview).
</Tip>

ุงูุนูููุฉ ูุชุทุงุจูุฉ ูููุงุท ุชูุชูุด TensorFlow ุงููููุฉ ุนูู Hub. ุนูู ุณุจูู ุงููุซุงูุ ูููููุง ุชุตุฏูุฑ ููุทุฉ ุชูุชูุด TensorFlow ูููุฉ ูู [ููุธูุฉ Keras](https://huggingface.co/keras-io) ููุง ููู:

```bash
optimum-cli export onnx --model keras-io/transformers-qa distilbert_base_cased_squad_onnx/
```

### ุชุตุฏูุฑ ูููุฐุฌ ูุงุณุชุฎุฏุงูู ูุน ORTModel ูู Optimum

ูููู ุงุณุชุฎุฏุงู ุงูููุงุฐุฌ ุงููุตุฏุฑุฉ ุนุจุฑ `optimum-cli export onnx` ูุจุงุดุฑุฉ ูู [`~onnxruntime.ORTModel`]. ูุฐุง ูููุฏ ุจุดูู ุฎุงุต ูููุงุฐุฌ ุงูุชุฑููุฒ ููู ุงูุชุดููุฑุ ุญูุซ ูู ูุฐู ุงูุญุงูุฉ ุณููุณู ุงูุชุตุฏูุฑ ุงูุชุฑููุฒ ููู ุงูุชุดููุฑ ุฅูู ููููู `.onnx`ุ ูุธุฑูุง ูุฃูู ูุชู ุชุดุบูู ุงูุชุฑููุฒ ุนุงุฏุฉู ูุฑุฉ ูุงุญุฏุฉ ููุท ุจูููุง ูุฏ ูุชู ุชุดุบูู ูู ุงูุชุดููุฑ ุนุฏุฉ ูุฑุงุช ูู ููุงู ุงูุชูููุฏ ุงูุชููุงุฆู.

### ุชุตุฏูุฑ ูููุฐุฌ ุจุงุณุชุฎุฏุงู ููุงุชูุญ/ููู ุงููุงุถู ูู ูู ุงูุชุดููุฑ

ุนูุฏ ุชุตุฏูุฑ ูููุฐุฌ ูู ุชุดููุฑ ูุณุชุฎุฏู ููุชูููุฏุ ูุฏ ูููู ูู ุงููููุฏ ุชุถููู [ุฅุนุงุฏุฉ ุงุณุชุฎุฏุงู ููุงุชูุญ ูููู ุงููุงุถู](https://discuss.huggingface.co/t/what-is-the-purpose-of-use-cache-in-decoder/958/2) ูู ONNX ุงููุตุฏุฑุฉ. ูุชูุญ ุฐูู ุชุฌูุจ ุฅุนุงุฏุฉ ุญุณุงุจ ููุณ ุงูุชูุดูุทุงุช ุงููุณูุทุฉ ุฃุซูุงุก ุงูุชูููุฏ.

ูู ุชุตุฏูุฑ ONNXุ ูุชู ุฅุนุงุฏุฉ ุงุณุชุฎุฏุงู ููุงุชูุญ/ููู ุงููุงุถู ุจุดูู ุงูุชุฑุงุถู. ูุชูุงูู ูุฐุง ุงูุณููู ูุน `--task text2text-generation-with-past`ุ ุฃู `--task text-generation-with-past`ุ ุฃู `--task automatic-speech-recognition-with-past`. ุฅุฐุง ููุช ุชุฑุบุจ ูู ุชุนุทูู ุงูุชุตุฏูุฑ ุจุงุณุชุฎุฏุงู ููุงุชูุญ/ููู ุงููุงุถูุ ููุฌุจ ุชูุฑูุฑ ูููุฉ `text2text-generation` ุฃู `text-generation` ุฃู `automatic-speech-recognition` ุจุดูู ุตุฑูุญ ุฅูู `optimum-cli export onnx`.

ูููู ุฅุนุงุฏุฉ ุงุณุชุฎุฏุงู ูููุฐุฌ ูุตุฏุฑ ุจุงุณุชุฎุฏุงู ููุงุชูุญ/ููู ุงููุงุถู ูุจุงุดุฑุฉ ูู [`~onnxruntime.ORTModel`] ูู Optimum:

```bash
optimum-cli export onnx --model gpt2 gpt2_onnx/
```

ู

```python
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import ORTModelForCausalLM

>>> tokenizer = AutoTokenizer.from_pretrained("./gpt2_onnx/")  # doctest: +SKIP
>>> model = ORTModelForCausalLM.from_pretrained("./gpt2_onnx/")  # doctest: +SKIP

>>> inputs = tokenizer("My name is Arthur and I live in", return_tensors="pt")  # doctest: +SKIP

>>> gen_tokens = model.generate(**inputs)  # doctest: +SKIP
>>> print(tokenizer.batch_decode(gen_tokens))  # doctest: +SKIP
# print ['My name is Arthur and I live in the United States of America. I am a member of the']
```

## ุชุญุฏูุฏ ูููุฉ

ุชุญุฏูุฏ `--task` ูุง ููุจุบู ุฃู ูููู ุถุฑูุฑููุง ูู ูุนุธู ุงูุญุงูุงุช ุนูุฏ ุงูุชุตุฏูุฑ ูู ูููุฐุฌ ุนูู Hugging Face Hub.

ููุน ุฐููุ ูู ุญุงูุฉ ุงูุญุงุฌุฉ ุฅูู ุงูุชุญูู ูู ููุฏุณุฉ ูููุฐุฌ ูุนููุฉุ ูุฅู ุงูููุงู ุงูุชู ูุฏุนููุง ุชุตุฏูุฑ ONNX ูุบุทุงุฉ. ุฃููุงูุ ููููู ุงูุชุญูู ูู ูุงุฆูุฉ ุงูููุงู ุงููุฏุนููุฉ ููู ูู PyTorch ูTensorFlow [ููุง](/exporters/task_manager).

ุจุงููุณุจุฉ ููู ููุฏุณุฉ ูููุฐุฌุ ููููู ุงูุนุซูุฑ ุนูู ูุงุฆูุฉ ุงูููุงู ุงููุฏุนููุฉ ุนุจุฑ [`~exporters.tasks.TasksManager`]. ุนูู ุณุจูู ุงููุซุงูุ ุจุงููุณุจุฉ ูู DistilBERTุ ุจุงููุณุจุฉ ูุชุตุฏูุฑ ONNXุ ูุฏููุง:

```python
>>> from optimum.exporters.tasks import TasksManager

>>> distilbert_tasks = list(TasksManager.get_supported_tasks_for_model_type("distilbert", "onnx").keys())
>>> print(distilbert_tasks)
['default', 'fill-mask', 'text-classification', 'multiple-choice', 'token-classification', 'question-answering']
```
ููููู ุจุนุฏ ุฐูู ุชูุฑูุฑ ุฅุญุฏู ูุฐู ุงูููุงู ุฅูู ูุณูุทุฉ `--task` ูู ุงูุฃูุฑ ุงูุฃูุซู`-cli Export onnx`ุ ููุง ูู ูุฐููุฑ ุฃุนูุงู.

## ุงูุชุตุฏูุฑ ุงููุฎุตุต ูููุงุฐุฌ ุงููุญููุงุช 

### ุชุฎุตูุต ุชุตุฏูุฑ ููุงุฐุฌ ุงููุญููุงุช ุงูุฑุณููุฉ

ูููุฑ Optimum ูููุณุชุฎุฏููู ุงููุชูุฏููู ุชุญูููุง ุฃูุซุฑ ุฏูุฉ ูู ุชูููู ุชุตุฏูุฑ ONNX. ููุฐุง ูููุฏ ุจุดูู ุฎุงุต ุฅุฐุง ููุช ุชุฑูุฏ ุชุตุฏูุฑ ุงูููุงุฐุฌ ุจุงุณุชุฎุฏุงู ูุณูุทุงุช ูููุงุช ุฑุฆูุณูุฉ ูุฎุชููุฉุ ุนูู ุณุจูู ุงููุซุงูุ ุจุงุณุชุฎุฏุงู `output_attentions=True` ุฃู `output_hidden_states=True`.

ูุฏุนู ูุฐู ุงูุญุงูุงุช ุงูุงุณุชุฎุฏุงููุฉุ ูุฏุนู [`~exporters.main_export`] ูุณูุทูู ููุง: `model_kwargs` ู`custom_onnx_configs`ุ ูุงููุฐุงู ูุชู ุงุณุชุฎุฏุงูููุง ุจุงูุทุฑููุฉ ุงูุชุงููุฉ:

- `model_kwargs`: ูุชูุญ ุชุฌุงูุฒ ุจุนุถ ูุณูุทุงุช ุงูุชููุฆุฉ ุงูุงูุชุฑุงุถูุฉ ูุทุฑููุฉ `forward` ูู ุงููููุฐุฌุ ููุชู ุงุณุชุฎุฏุงููุง ุนููููุง ููุง ููู: `model(**reference_model_inputs, **model_kwargs)`.
- `custom_onnx_configs`: ูุฌุจ ุฃู ูููู ุนุจุงุฑุฉ ุนู `Dict[str, OnnxConfig]`ุ ูููู ุจุงูุฑุจุท ูู ุงุณู ุงููููุฐุฌ ุงููุฑุนู (ุนุงุฏุฉู `model` ุฃู `encoder_model` ุฃู `decoder_model` ุฃู `decoder_model_with_past` - [ุงููุฑุฌุน](https://github.com/huggingface/optimum/blob/main/optimum/exporters/onnx/constants.py)) ุฅูู ุชูููู ONNX ูุฎุตุต ูููููุฐุฌ ุงููุฑุนู ุงููุนุทู.

ูููุง ููู ูุซุงู ูุงูู ูุณูุญ ุจุชุตุฏูุฑ ุงูููุงุฐุฌ ูุน `output_attentions=True`.

```python
from optimum.exporters.onnx import main_export
from optimum.exporters.onnx.model_configs import WhisperOnnxConfig
from transformers import AutoConfig

from optimum.exporters.onnx.base import ConfigBehavior
from typing import Dict

class CustomWhisperOnnxConfig(WhisperOnnxConfig):
    @property
    def outputs(self) -> Dict[str, Dict[int, str]]:
        common_outputs = super().outputs

        if self._behavior is ConfigBehavior.ENCODER:
            for i in range(self._config.encoder_layers):
                common_outputs[f"encoder_attentions.{i}"] = {0: "batch_size"}
        elif self._behavior is ConfigBehavior.DECODER:
            for i in range(self._config.decoder_layers):
                common_outputs[f"decoder_attentions.{i}"] = {
                    0: "batch_size",
                    2: "decoder_sequence_length",
                    3: "past_decoder_sequence_length + 1"
                }
            for i in range(self._config.decoder_layers):
                common_outputs[f"cross_attentions.{i}"] = {
                    0: "batch_size",
                    2: "decoder_sequence_length",
                    3: "encoder_sequence_length_out"
                }

        return common_outputs

    @property
    def torch_to_onnx_output_map(self):
        if self._behavior is ConfigBehavior.ENCODER:
            # The encoder export uses WhisperEncoder that returns the key "attentions"
            return {"attentions": "encoder_attentions"}
        else:
            return {}

model_id = "openai/whisper-tiny.en"
config = AutoConfig.from_pretrained(model_id)

custom_whisper_onnx_config = CustomWhisperOnnxConfig(
        config=config,
        task="automatic-speech-recognition",
)

encoder_config = custom_whisper_onnx_config.with_behavior("encoder")
decoder_config = custom_whisper_onnx_config.with_behavior("decoder", use_past=False)
decoder_with_past_config = custom_whisper_onnx_config.with_behavior("decoder", use_past=True)

custom_onnx_configs={
    "encoder_model": encoder_config,
    "decoder_model": decoder_config,
    "decoder_with_past_model": decoder_with_past_config,
}

main_export(
    model_id,
    output="custom_whisper_onnx",
    no_post_process=True,
    model_kwargs={"output_attentions": True},
    custom_onnx_configs=custom_onnx_configs
)
```

ุจุงููุณุจุฉ ููููุงู ุงูุชู ุชุชุทูุจ ููู ONNX ูุงุญุฏ ููุท (ุนูู ุณุจูู ุงููุซุงูุ ุงูุชุฑููุฒ ููุท)ุ ูููู ุจุนุฏ ุฐูู ุงุณุชุฎุฏุงู ุงููููุฐุฌ ุงููุตุฏุฑ ูุน ุชุฎุตูุต ุงููุฏุฎูุงุช/ุงููุฎุฑุฌุงุช ูุน ุงููุฆุฉ [`optimum.onnxruntime.ORTModelForCustomTasks`] ููุงุณุชูุชุงุฌ ุจุงุณุชุฎุฏุงู ONNX Runtime ุนูู ูุญุฏุฉ ุงููุนุงูุฌุฉ ุงููุฑูุฒูุฉ ุฃู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณููุงุช.

### ุชุฎุตูุต ุชุตุฏูุฑ ููุงุฐุฌ ุงููุญููุงุช ูุน ุงูููุฐุฌุฉ ุงููุฎุตุตุฉ

ูุฏุนู Optimum ุชุตุฏูุฑ ููุงุฐุฌ ุงููุญููุงุช ูุน ุงูููุฐุฌุฉ ุงููุฎุตุตุฉ ุงูุชู ุชุณุชุฎุฏู [`trust_remote_code=True`](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModel.from_pretrained.trust_remote_code)ุ ูุงูุชู ูุง ูุชู ุฏุนููุง ุฑุณูููุง ูู ููุชุจุฉ ุงููุญููุงุช ูููู ูููู ุงุณุชุฎุฏุงููุง ูุน ูุธุงุฆููุง ูุซู [pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) ู[generation](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationMixin.generate).

ููู ุฃูุซูุฉ ูุฐู ุงูููุงุฐุฌ [THUDM/chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) ู[mosaicml/mpt-30b](https://huggingface.co/mosaicml/mpt-30b).

ูุชุตุฏูุฑ ุงูููุงุฐุฌ ุงููุฎุตุตุฉุ ูุฌุจ ุชูุฑูุฑ ูุงููุณ `custom_onnx_configs` ุฅูู [`~optimum.exporters.onnx.main_export`]ุ ูุน ุชุนุฑูู ุชูููู ONNX ูุฌููุน ุงูุฃุฌุฒุงุก ุงููุฑุนูุฉ ูููููุฐุฌ ุงูุฐู ุณูุชู ุชุตุฏูุฑู (ุนูู ุณุจูู ุงููุซุงูุ ุงูุฃุฌุฒุงุก ุงููุฑุนูุฉ ููุชุฑููุฒ ููู ุงูุชุฑููุฒ). ูุณูุญ ุงููุซุงู ุงูุชุงูู ุจุชุตุฏูุฑ ูููุฐุฌ `mosaicml/mpt-7b`:

```python
from optimum.exporters.onnx import main_export

from transformers import AutoConfig

from optimum.exporters.onnx.config import TextDecoderOnnxConfig
from optimum.utils import NormalizedTextConfig, DummyPastKeyValuesGenerator
from typing import Dict


class MPTDummyPastKeyValuesGenerator(DummyPastKeyValuesGenerator):
    """
    MPT swaps the two last dimensions for the key cache compared to usual transformers
    decoder models, thus the redefinition here.
    """
    def generate(self, input_name: str, framework: str = "pt"):
        past_key_shape = (
            self.batch_size,
            self.num_attention_heads,
            self.hidden_size // self.num_attention_heads,
            self.sequence_length,
        )
        past_value_shape = (
            self.batch_size,
            self.num_attention_heads,
            self.sequence_length,
            self.hidden_size // self.num_attention_heads,
        )
        return [
            (
                self.random_float_tensor(past_key_shape, framework=framework),
                self.random_float_tensor(past_value_shape, framework=framework),
            )
            for _ in range(self.num_layers)
        ]

class CustomMPTOnnxConfig(TextDecoderOnnxConfig):
    DUMMY_INPUT_GENERATOR_CLASSES = (MPTDummyPastKeyValuesGenerator,) + TextDecoderOnnxConfig.DUMMY_INPUT_GENERATOR_CLASSES
    DUMMY_PKV_GENERATOR_CLASS = MPTDummyPastKeyValuesGenerator

    DEFAULT_ONNX_OPSET = 14  # aten::tril operator requires opset>=14
    NORMALIZED_CONFIG_CLASS = NormalizedTextConfig.with_args(
        hidden_size="d_model",
        num_layers="n_layers",
        num_attention_heads="n_heads"
    )

    def add_past_key_values(self, inputs_or_outputs: Dict[str, Dict[int, str]], direction: str):
        """
        Adapted from https://github.com/huggingface/optimum/blob/v1.9.0/optimum/exporters/onnx/base.py#L625
        """
        if direction not in ["inputs", "outputs"]:
            raise ValueError(f'direction must either be "inputs" or "outputs", but {direction} was given')

        if direction == "inputs":
            decoder_sequence_name = "past_sequence_length"
            name = "past_key_values"
        else:
            decoder_sequence_name = "past_sequence_length + 1"
            name = "present"

        for i in range(self._normalized_config.num_layers):
            inputs_or_outputs[f"{name}.{i}.key"] = {0: "batch_size", 3: decoder_sequence_name}
            inputs_or_outputs[f"{name}.{i}.value"] = {0: "batch_size", 2: decoder_sequence_name}


model_id = "/home/fxmarty/hf_internship/optimum/tiny-mpt-random-remote-code"
config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)

onnx_config = CustomMPTOnnxConfig(
    config=config,
    task="text-generation",
    use_past_in_inputs=False,
    use_present_in_outputs=True,
)
onnx_config_with_past = CustomMPTOnnxConfig(config, task="text-generation", use_past=True)

custom_onnx_configs = {
    "decoder_model": onnx_config,
    "decoder_with_past_model": onnx_config_with_past,
}

main_export(
    model_id,
    output="mpt_onnx",
    task="text-generation-with-past",
    trust_remote_code=True,
    custom_onnx_configs=custom_onnx_configs,
    no_post_process=True,
)
```

ุนูุงูุฉ ุนูู ุฐููุ ุชุณูุญ ูุณูุทุฉ `fn_get_submodels` ุงููุชูุฏูุฉ ูู `main_export` ุจุชุฎุตูุต ููููุฉ ุงุณุชุฎุฑุงุฌ ุงูููุงุฐุฌ ุงููุฑุนูุฉ ูู ุญุงูุฉ ุงูุญุงุฌุฉ ุฅูู ุชุตุฏูุฑ ุงููููุฐุฌ ูู ุนุฏุฉ ููุงุฐุฌ ูุฑุนูุฉ. ูููู ุงูุงุทูุงุน ุนูู ุฃูุซูุฉ ููุธุงุฆู ููุฐู [ููุง](link to utils.py relevant code once merged).