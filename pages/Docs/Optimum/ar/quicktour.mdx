# ุฌููุฉ ุณุฑูุนุฉ

ุชูุฏู ูุฐู ุงูุฌููุฉ ุงูุณุฑูุนุฉ ุฅูู ุงููุทูุฑูู ุงูุฐูู ูุฑุบุจูู ูู ุงูุบูุต ูู ุงูุดูุฑุฉ ุงูุจุฑูุฌูุฉ ูุฑุคูุฉ ุฃูุซูุฉ ุนูู ููููุฉ ุฏูุฌ ๐ค Optimum ูู ุณูุฑ ุนูู ุชุฏุฑูุจ ุงูููุงุฐุฌ ูุงูุงุณุชุฏูุงู.

## ุชุณุฑูุน ุงูุงุณุชุฏูุงู

#### OpenVINO

ูุชุญููู ูููุฐุฌ ูุชุดุบูู ุงูุงุณุชุฏูุงู ุจุงุณุชุฎุฏุงู OpenVINO Runtimeุ ููููู ุจุจุณุงุทุฉ ุงุณุชุจุฏุงู ูุฆุฉ `AutoModelForXxx` ุจุงููุฆุฉ ุงูููุงุณุจุฉ `OVModelForXxx`. ุฅุฐุง ููุช ุชุฑูุฏ ุชุญููู ููุทุฉ ุชูุชูุด PyTorchุ ูู ุจุชุนููู `export=True` ูุชุญููู ูููุฐุฌู ุฅูู OpenVINO IR (ุชูุซูู ูุณูุท).

```diff
- from transformers import AutoModelForSequenceClassification
+ from optimum.intel.openvino import OVModelForSequenceClassification
from transformers import AutoTokenizer, pipeline

# ูู ุจุชุญููู ุจุฑูุงูุฌ tokenizer ููููุฐุฌ ูู Hub ูุชุญูููู ุฅูู ุชูุณูู OpenVINO
tokenizer = AutoTokenizer.from_pretrained(model_id)
model_id = "distilbert-base-uncased-finetuned-sst-2-english"
- model = AutoModelForSequenceClassification.from_pretrained(model_id)
+ model = OVModelForSequenceClassification.from_pretrained(model_id, export=True)

# ูู ุจุชุดุบูู ุงูุงุณุชุฏูุงู!
classifier = pipeline("text-classification", model=model, tokenizer=tokenizer)
results = classifier("He's a dreadful magician.")
```

ููููู ุงูุนุซูุฑ ุนูู ุงููุฒูุฏ ูู ุงูุฃูุซูุฉ ูู [ุงููุซุงุฆู](https://huggingface.co/docs/optimum/intel/inference) ููู [ุงูุฃูุซูุฉ](https://github.com/huggingface/optimum-intel/tree/main/examples/openvino).

#### ONNX Runtime

ูุชุณุฑูุน ุงูุงุณุชุฏูุงู ุจุงุณุชุฎุฏุงู ONNX Runtimeุ ูุณุชุฎุฏู ๐ค Optimum _ุฃุดูุงุก ุงูุชูููู_ ูุชุญุฏูุฏ ุงููุนููุงุช ูุชุญุณูู ุงูุฑุณู ุงูุจูุงูู ูุงููููุฉ. ุซู ูุชู ุงุณุชุฎุฏุงู ูุฐู ุงูุฃุดูุงุก ูุชููุฆุฉ _ุงููุญุณูุงุช_ ู_ุงููููุงุช_ ุงููุฎุตุตุฉ.

ูุจู ุชุทุจูู ุงููููุฉ ุฃู ุงูุชุญุณููุ ูุญุชุงุฌ ุฃููุงู ุฅูู ุชุญููู ูููุฐุฌูุง. ูุชุญููู ูููุฐุฌ ูุชุดุบูู ุงูุงุณุชุฏูุงู ุจุงุณุชุฎุฏุงู ONNX Runtimeุ ููููู ุจุจุณุงุทุฉ ุงุณุชุจุฏุงู ูุฆุฉ [`AutoModelForXxx`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModel) ุงูููุงุณูุฉ ูู Transformers ุจุงููุฆุฉ ุงูููุงุณุจุฉ [`ORTModelForXxx`](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTModel). ุฅุฐุง ููุช ุชุฑูุฏ ุงูุชุญููู ูู ููุทุฉ ุชูุชูุด PyTorchุ ููู ุจุชุนููู `export=True` ูุชุตุฏูุฑ ูููุฐุฌู ุฅูู ุชูุณูู ONNX.

```python
>>> from optimum.onnxruntime import ORTModelForSequenceClassification
>>> from transformers import AutoTokenizer

>>> model_checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
>>> save_directory = "tmp/onnx/"

>>> # ูู ุจุชุญููู ูููุฐุฌ ูู Transformers ูุชุตุฏูุฑู ุฅูู ONNX
>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
>>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)

>>> # ุงุญูุธ ูููุฐุฌ ONNX ูุจุฑูุงูุฌ tokenizer
>>> ort_model.save_pretrained(save_directory)
>>> tokenizer.save_pretrained(save_directory)  # doctest: +IGNORE_RESULT
```

ุฏุนููุง ูุฑู ุงูุขู ููู ูููููุง ุชุทุจูู ุงููููุฉ ุงูุฏููุงููููุฉ ูุน ONNX Runtime:

```python
>>> from optimum.onnxruntime.configuration import AutoQuantizationConfig
>>> from optimum.onnxruntime import ORTQuantizer

>>> # ุญุฏุฏ ูููุฌูุฉ ุงููููุฉ
>>> qconfig = AutoQuantizationConfig.arm64(is_static=False, per_channel=False)
>>> quantizer = ORTQuantizer.from_pretrained(ort_model)

>>> # ุชุทุจูู ุงููููุฉ ุงูุฏููุงููููุฉ ุนูู ุงููููุฐุฌ
>>> quantizer.quantize(save_dir=save_directory, quantization_config=qconfig)  # doctest: +IGNORE_RESULT
```

ูู ูุฐุง ุงููุซุงูุ ูููุง ุจูููุฉ ูููุฐุฌ ูู Hugging Face Hubุ ูุจููุณ ุงูุทุฑููุฉ ูููููุง ูููุฉ ูููุฐุฌ ูุณุชุถุงู ูุญูููุง ุนู ุทุฑูู ุชูููุฑ ุงููุณุงุฑ ุฅูู ุงูุฏููู ุงูุฐู ูุญุชูู ุนูู ุฃูุฒุงู ุงููููุฐุฌ. ูุงููุชูุฌุฉ ูู ุชุทุจูู ุทุฑููุฉ `quantize()` ูู ููู `model_quantized.onnx` ุงูุฐู ูููู ุงุณุชุฎุฏุงูู ูุชุดุบูู ุงูุงุณุชุฏูุงู. ูููุง ููู ูุซุงู ุนูู ููููุฉ ุชุญููู ูููุฐุฌ ONNX Runtime ูุชูููุฏ ุงูุชูุจุคุงุช ุจุงุณุชุฎุฏุงูู:

```python
>>> from optimum.onnxruntime import ORTModelForSequenceClassification
>>> from transformers import pipeline, AutoTokenizer

>>> model = ORTModelForSequenceClassification.from_pretrained(save_directory, file_name="model_quantized.onnx")
>>> tokenizer = AutoTokenizer.from_pretrained(save_directory)
>>> classifier = pipeline("text-classification", model=model, tokenizer=tokenizer)
>>> results = classifier("I love burritos!")
```

ููููู ุงูุนุซูุฑ ุนูู ุงููุฒูุฏ ูู ุงูุฃูุซูุฉ ูู [ุงููุซุงุฆู](https://huggingface.co/docs/optimum/onnxruntime/quickstart) ููู [ุงูุฃูุซูุฉ](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime).

## ุชุณุฑูุน ุงูุชุฏุฑูุจ

#### Habana

ูุชุฏุฑูุจ ุงููุญููุงุช ุนูู ูุนุงูุฌุงุช Gaudi ูู Habanaุ ูููุฑ ๐ค Optimum `GaudiTrainer` ูุดุงุจู ุฌุฏูุง ูู ๐ค Transformers [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer). ูููุง ููู ูุซุงู ุจุณูุท:

```diff
- from transformers import Trainer, TrainingArguments
+ from optimum.habana import GaudiTrainer, GaudiTrainingArguments

# ูู ุจุชูุฒูู ูููุฐุฌ ููุฏุฑุจ ูุณุจููุง ูู Hub
model = AutoModelForXxx.from_pretrained("bert-base-uncased")

# ุญุฏุฏ ูุณุงุฆุท ุงูุชุฏุฑูุจ
- training_args = TrainingArguments(
+ training_args = GaudiTrainingArguments(
output_dir="path/to/save/folder/",
+     use_habana=True,
+     use_lazy_mode=True,
+     gaudi_config_name="Habana/bert-base-uncased",
...
)

# ูู ุจุชููุฆุฉ ุงููุฏุฑุจ
- trainer = Trainer(
+ trainer = GaudiTrainer(
model=model,
args=training_args,
train_dataset=train_dataset,
...
)

# ุงุณุชุฎุฏู ูุนุงูุฌ Habana Gaudi ููุชุฏุฑูุจ!
trainer.train()
```

ููููู ุงูุนุซูุฑ ุนูู ุงููุฒูุฏ ูู ุงูุฃูุซูุฉ ูู [ุงููุซุงุฆู](https://huggingface.co/docs/optimum/habana/quickstart) ููู [ุงูุฃูุซูุฉ](https://github.com/huggingface/optimum-habana/tree/main/examples).

#### ONNX Runtime

ูุชุฏุฑูุจ ุงููุญููุงุช ุจุงุณุชุฎุฏุงู ููุฒุงุช ุชุณุฑูุน ONNX Runtimeุ ูููุฑ ๐ค Optimum `ORTTrainer` ูุดุงุจู ุฌุฏูุง ูู ๐ค Transformers [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer). ูููุง ููู ูุซุงู ุจุณูุท:

```diff
- from transformers import Trainer, TrainingArguments
+ from optimum.onnxruntime import ORTTrainer, ORTTrainingArguments

# ูู ุจุชูุฒูู ูููุฐุฌ ููุฏุฑุจ ูุณุจููุง ูู Hub
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")

# ุญุฏุฏ ูุณุงุฆุท ุงูุชุฏุฑูุจ
- training_args = TrainingArguments(
+ training_args = ORTTrainingArguments(
output_dir="path/to/save/folder/",
optim="adamw_ort_fused",
...
)

# ูู ุจุฅูุดุงุก ูุฏุฑุจ ONNX Runtime
- trainer = Trainer(
+ trainer = ORTTrainer(
model=model,
args=training_args,
train_dataset=train_dataset,
+     feature="text-classification"ุ # ููุน ุงููููุฐุฌ ูุชุตุฏูุฑู ุฅูู ONNX
...
)

# ุงุณุชุฎุฏู ONNX Runtime ููุชุฏุฑูุจ!
trainer.train()
```

ููููู ุงูุนุซูุฑ ุนูู ุงููุฒูุฏ ูู ุงูุฃูุซูุฉ ูู [ุงููุซุงุฆู](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/trainer) ููู [ุงูุฃูุซูุฉ](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training).

## ุชุตุฏูุฑ ONNX ุฎุงุฑุฌ ุงูุตูุฏูู

ุชุชุนุงูู ููุชุจุฉ Optimum ุฎุงุฑุฌ ุงูุตูุฏูู ูุน ุชุตุฏูุฑ ONNX ูููุงุฐุฌ Transformers ูDiffusers!

ุชุตุฏูุฑ ูููุฐุฌ ุฅูู ONNX ุจุณูุท ูุซู

```bash
optimum-cli export onnx --model gpt2 gpt2_onnx/
```

ุชุญูู ูู ุงููุณุงุนุฏุฉ ููุญุตูู ุนูู ุฎูุงุฑุงุช ุฃูุซุฑ:

```bash
optimum-cli export onnx --help
```

ุฑุงุฌุน [ุงููุซุงุฆู](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model) ููุญุตูู ุนูู ูุฒูุฏ ูู ุงููุนูููุงุช.

## ุฏุนู BetterTransformer ูู PyTorch

[BetterTransformer](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) ูู ุชุญุณูู ูุฌุงูู ูู PyTorch ุงูุฃุตูู ูุชุญููู ุฒูุงุฏุฉ ูู ุงูุณุฑุนุฉ ุชุชุฑุงูุญ ูู 1.25 ุฅูู 4 ูุฑุงุช ูู ุงูุงุณุชุฏูุงู ูููุงุฐุฌ ุงููุณุชูุฏุฉ ุฅูู ุงููุญูู. ุชู ุชูููุฒู ุนูู ุฃูู ูุณุชูุฑ ูู [PyTorch 1.13](https://pytorch.org/blog/PyTorch-1.13-release/). ููุฏ ูููุง ุจุชูุงูู BetterTransformer ูุน ุฃูุซุฑ ุงูููุงุฐุฌ ุงุณุชุฎุฏุงููุง ูู ููุชุจุฉ ๐ค Transformersุ ูุงุณุชุฎุฏุงู ุงูุชูุงูู ุจุณูุท ูุซู:

```python
>>> from optimum.bettertransformer import BetterTransformer
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
>>> model = BetterTransformer.transform(model)
```

ุฑุงุฌุน [ุงููุซุงุฆู](https://huggingface.co/docs/optimum/bettertransformer/overview) ููุฒูุฏ ูู ุงูุชูุงุตููุ ู[ููุดูุฑ ุงููุฏููุฉ ุนูู Medium ูู PyTorch](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2) ููุนุฑูุฉ ุงููุฒูุฏ ุญูู ุงูุชูุงูู!

## ุชูุงูู `torch.fx`

ูุชูุงูู Optimum ูุน `torch.fx`ุ ููุง ูููุฑ ูุฎุท ูุงุญุฏ ุงูุนุฏูุฏ ูู ุชุญูููุงุช ุงูุฑุณูู ุงูุจูุงููุฉ. ููุฏู ุฅูู ุฏุนู ุฅุฏุงุฑุฉ ุฃูุถู ูู [ุงููููุฉ](https://huggingface.co/docs/optimum/concept_guides/quantization) ูู ุฎูุงู `torch.fx`ุ ููู ูู ุงูุชุฏุฑูุจ ุนูู ุงููุนู ุจุงููููุฉ (QAT) ูุงููููุฉ ุจุนุฏ ุงูุชุฏุฑูุจ (PTQ).

ุฑุงุฌุน [ุงููุซุงุฆู](https://huggingface.co/docs/optimum/torch_fx/usage_guides/optimization) ู[ุงููุฑุฌุน](https://huggingface.co/docs/optimum/torch_fx/package_reference/optimization) ููุญุตูู ุนูู ูุฒูุฏ ูู ุงููุนูููุงุช!