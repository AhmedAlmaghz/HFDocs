# ูุธุฑุฉ ุนุงูุฉ
๐ค Optimum ูููุฑ ูุงุฌูุฉ ุจุฑูุฌุฉ ุชุทุจููุงุช API ุชุณูู BetterTransformerุ ููู ูุณุงุฑ ุณุฑูุน ููุงุฌูุงุช ุจุฑูุฌุฉ ุงูุชุทุจููุงุช APIs ุงูุฎุงุตุฉ ุจู PyTorch Transformer ููุงุณุชูุงุฏุฉ ูู ุชุณุฑูุน ุงูุฃุฏุงุก ุนูู ูุญุฏุฉ ุงููุนุงูุฌุฉ ุงููุฑูุฒูุฉ CPU ููุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุช GPU ูู ุฎูุงู ุชูููุงุช ูุซู ุงูุชูุฑูู sparsity ูุงูููุงุฉ ุงูููุฏูุฌุฉ fused kernels ูุซู Flash Attention. ูู ุงูููุช ุงูุญุงููุ ูุฏุนู BetterTransformer ุงููุณุงุฑ ุงูุณุฑูุน ูู [`nn.TransformerEncoderLayer`](https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/) ุงูุฃุตููุ ุจุงูุฅุถุงูุฉ ุฅูู Flash Attention ูMemory-Efficient Attention ูู [`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html).

## ุงูุจุฏุงูุฉ ุงูุณุฑูุนุฉ
ููุฐ ุงูุฅุตุฏุงุฑ 1.13ุ ุฃุตุฏุฑุช [PyTorch](https://pytorch.org/blog/PyTorch-1.13-release/) ุงูุฅุตุฏุงุฑ ุงููุณุชูุฑ ูู ุงููุณุงุฑ ุงูุณุฑูุน ููุงุฌูุงุช ุจุฑูุฌุฉ ุงูุชุทุจููุงุช APIs ุงูุฎุงุตุฉ ุจุงูู Transformer ูุงูุชู ุชููุฑ ุชุญุณููุงุช ูู ุงูุฃุฏุงุก ุจุดูู ูุจุงุดุฑ ููููุงุฐุฌ ุงููุจููุฉ ุนูู ุงูู Transformer. ููููู ุงูุงุณุชูุงุฏุฉ ูู ุชุณุฑูุน ุงูุฃุฏุงุก ุนูู ูุนุธู ุงูุฃุฌูุฒุฉ ุงูุงุณุชููุงููุฉุ ุจูุง ูู ุฐูู ูุญุฏุงุช ุงููุนุงูุฌุฉ ุงููุฑูุฒูุฉ CPUs ูุฅุตุฏุงุฑุงุช NVIDIA GPUs ุงููุฏููุฉ ูุงูุญุฏูุซุฉ.

ููููู ุงูุขู ุงุณุชุฎุฏุงู ูุฐู ุงูููุฒุฉ ูู ๐ค Optimum ูุน Transformers ูุงุณุชุฎุฏุงููุง ููููุงุฐุฌ ุงูุฑุฆูุณูุฉ ูู ูุธุงู Hugging Face.

ูู ุงูุฅุตุฏุงุฑ 2.0ุ ุชุชุถูู PyTorch ูุธููุฉ ุงูุชูุงู ุงูููุชุฌ ุงูููุทู ุงููููููููุณ SDPA ุงูุฃุตููุฉ ูุฌุฒุก ูู `torch.nn.functional`. ุชุดูู ูุฐู ุงููุธููุฉ ุนุฏุฉ ุชุทุจููุงุช ูููู ุงุณุชุฎุฏุงููุง ุญุณุจ ุงููุฏุฎูุงุช ูุงูุนุชุงุฏ ุงูุตูุจ ุงููุณุชุฎุฏู. ุฑุงุฌุน [ุงููุซุงุฆู ุงูุฑุณููุฉ](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention) ููุฒูุฏ ูู ุงููุนูููุงุชุ ู[ููุดูุฑ ุงููุฏููุฉ](https://pytorch.org/blog/out-of-the-box-acceleration/) ููุญุตูู ุนูู ูุนุงููุฑ ุงูุฃุฏุงุก.

ูููุฑ ุชูุงูููุง ูุน ูุฐู ุงูุชุญุณููุงุช ุจุดูู ูุจุงุดุฑ ูู ๐ค Optimumุ ุจุญูุซ ููููู ุชุญููู ุฃู ูููุฐุฌ ูุฏุนูู ูู ๐ค Transformers ูุงุณุชุฎุฏุงู ุงููุณุงุฑุงุช ุงููุญุณูุฉ ููุธููุฉ `scaled_dot_product_attention` ุนูุฏ ุงูุญุงุฌุฉ.

<Tip warning={true}>
ูุชู ุงุนุชูุงุฏ ูุธููุฉ `scaled_dot_product_attention` ุงูุฃุตููุฉ ูู PyTorch ุจุดูู ุชุฏุฑูุฌู ูู ๐ค Transformers. ุจุงููุณุจุฉ ููููุงุฐุฌ ุงูุชู ุชุฏุนู SDPA ูู Transformersุ ูููู ุงุณุชุฎุฏุงู BetterTransformer ูููุตุญ ุจุงุณุชุฎุฏุงู Transformers ูุขุฎุฑ ุฅุตุฏุงุฑ ูู PyTorch ููุชุญุณููุงุช ุงูุฎุงุตุฉ ุจุงูุงูุชูุงู (Flash Attention ูmemory-efficient attention) ูู ุฎูุงู SDPA.
</Tip>

<Tip warning={true}>
ูููู ููุธููุฉ `scaled_dot_product_attention` ุงูุฃุตููุฉ ูู PyTorch ุฃู ุชุฑุณู ููุท ุฅูู Flash Attention ุฅุฐุง ูู ูุชู ุชูููุฑ `attention_mask`.

ูุฐููุ ุจุดูู ุงูุชุฑุงุถู ูู ูุถุน ุงูุชุฏุฑูุจุ ูุชููู ุชูุงูู BetterTransformer ุนู ุฏุนู ุงูููุงุน ููููู ุงุณุชุฎุฏุงูู ููุท ููุชุฏุฑูุจ ุงูุฐู ูุง ูุชุทูุจ ููุงุนูุง ููุชุฑููุฒ padding mask ููุชุฏุฑูุจ ุงููุฌูุน batched training. ูุฐุง ูู ุงูุญุงูุ ุนูู ุณุจูู ุงููุซุงูุ ูู ููุฐุฌุฉ ุงููุบุฉ ุงููููุนุฉ masked language modeling ุฃู ููุฐุฌุฉ ุงููุบุฉ ุงูุณุจุจูุฉ causal language modeling. ูุง ููุงุณุจ BetterTransformer ุงูุถุจุท ุงูุฏููู ููููุงุฐุฌ ุนูู ุงูููุงู ุงูุชู ุชุชุทูุจ ููุงุนูุง ููุชุฑููุฒ.

ูู ูุถุน ุงูุงุณุชุฏูุงูุ ูุชู ุงูุงุญุชูุงุธ ุจููุงุน ุงูุชุฑููุฒ ูู ุฃุฌู ุงูุฏูุฉุ ูุจุงูุชุงูู ูุง ููุจุบู ุชููุน ุชุณุฑูุน ุงูุฃุฏุงุก ุฅูุง ูู ุญุงูุฉ ุญุฌู ุงูุฏูุนุฉ batch size = 1.
</Tip>

### ุงูููุงุฐุฌ ุงููุฏุนููุฉ
ูููุง ููู ูุงุฆูุฉ ุจุงูููุงุฐุฌ ุงููุฏุนููุฉ:

- [AlBERT](https://arxiv.org/abs/1909.11942)
- [Bark](https://github.com/suno-ai/bark)
- [BART](https://arxiv.org/abs/1910.13461)
- [BERT](https://arxiv.org/abs/1810.04805)
- [BERT-generation](https://arxiv.org/abs/1907.12461)
- [BLIP-2](https://arxiv.org/abs/2301.12597)
- [BLOOM](https://arxiv.org/abs/2211.05100)
- [CamemBERT](https://arxiv.org/abs/1911.03894)
- [CLIP](https://arxiv.org/abs/2103.00020)
- [CodeGen](https://arxiv.org/abs/2203.13474)
- [Data2VecText](https://arxiv.org/abs/2202.03555)
- [DistilBert](https://arxiv.org/abs/1910.01108)
- [DeiT](https://arxiv.org/abs/2012.12877)
- [Electra](https://arxiv.org/abs/2003.10555)
- [Ernie](https://arxiv.org/abs/1904.09223)
- [Falcon](https://arxiv.org/abs/2306.01116) (ูุง ุญุงุฌุฉ ูุงุณุชุฎุฏุงู BetterTransformerุ ููู [ูุฏุนูู ูุจุงุดุฑุฉ ูู ูุจู Transformers](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-and-memory-efficient-attention-through-pytorchs-scaleddotproductattention))
- [FSMT](https://arxiv.org/abs/1907.06616)
- [GPT2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [GPT-j](https://huggingface.co/EleutherAI/gpt-j-6B)
- [GPT-neo](https://github.com/EleutherAI/gpt-neo)
- [GPT-neo-x](https://arxiv.org/abs/2204.06745)
- [GPT BigCode](https://arxiv.org/abs/2301.03988) (SantaCoder, StarCoder - ูุง ุญุงุฌุฉ ูุงุณุชุฎุฏุงู BetterTransformerุ ููู [ูุฏุนูู ูุจุงุดุฑุฉ ูู ูุจู Transformers](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-and-memory-efficient-attention-through-pytorchs-scaleddotproductattention))
- [HuBERT](https://arxiv.org/pdf/2106.07447.pdf)
- [LayoutLM](https://arxiv.org/abs/1912.13318)
- [Llama & Llama2](https://arxiv.org/abs/2302.13971) (ูุง ุญุงุฌุฉ ูุงุณุชุฎุฏุงู BetterTransformerุ ููู [ูุฏุนูู ูุจุงุดุฑุฉ ูู ูุจู Transformers](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-and-memory-efficient-attention-through-pytorchs-scaleddotproductattention))
- [MarkupLM](https://arxiv.org/abs/2110.08518)
- [Marian](https://arxiv.org/abs/1804.00344)
- [MBart](https://arxiv.org/abs/2001.08210)
- [M2M100](https://arxiv.org/abs/2010.11125)
- [OPT](https://arxiv.org/abs/2205.01068)
- [ProphetNet](https://arxiv.org/abs/2001.04063)
- [RemBERT](https://arxiv.org/abs/2010.12821)
- [RoBERTa](https://arxiv.org/abs/1907.11692)
- [RoCBert](https://aclanthology.org/2022.acl-long.65.pdf)
- [RoFormer](https://arxiv.org/abs/2104.09864)
- [Splinter](https://arxiv.org/abs/2101.00438)
- [Tapas](https://arxiv.org/abs/2211.06550)
- [ViLT](https://arxiv.org/abs/2102.03334)
- [ViT](https://arxiv.org/abs/2010.11929)
- [ViT-MAE](https://arxiv.org/abs/2111.06377)
- [ViT-MSN](https://arxiv.org/abs/2204.07141)
- [Wav2Vec2](https://arxiv.org/abs/2006.11477)
- [Whisper](https://cdn.openai.com/papers/whisper.pdf) (ูุง ุญุงุฌุฉ ูุงุณุชุฎุฏุงู BetterTransformerุ ููู [ูุฏุนูู ูุจุงุดุฑุฉ ูู ูุจู Transformers](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-and-memory-efficient-attention-through-pytorchs-scaleddotproductattention))
- [XLMRoberta](https://arxiv.org/abs/1911.02116)
- [YOLOS](https://arxiv.org/abs/2106.00666)

ุฃุฎุจุฑูุง ุจูุชุญ ูุดููุฉ issue ูู ๐ค Optimum ุฅุฐุง ููุช ุชุฑูุฏ ุงููุฒูุฏ ูู ุงูููุงุฐุฌ ุงููุฏุนููุฉุ ุฃู ุงุทูุน ุนูู [ุฏููู ุงููุณุงููุฉ](https://huggingface.co/docs/optimum/bettertransformer/tutorials/contribute) ุฅุฐุง ููุช ุชุฑูุฏ ุฅุถุงูุชูุง ุจููุณู!

### ุงูุงุณุชุฎุฏุงู ุงูุณุฑูุน
ูุงุณุชุฎุฏุงู ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช `BetterTransformer` APIุ ูู ุจุชุดุบูู ุงูุฃูุงูุฑ ุงูุชุงููุฉ:

```python
>>> from transformers import AutoModelForSequenceClassification
>>> from optimum.bettertransformer import BetterTransformer
>>> model_hf = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")
>>> model = BetterTransformer.transform(model_hf, keep_original_model=True)
```

ููููู ุชุฑู `keep_original_model=False` ูู ุญุงูุฉ ุงูุฑุบุจุฉ ูู ุงููุชุงุจุฉ ููู ุงููููุฐุฌ ุงูุญุงูู ุจูุณุฎุฉ `BetterTransformer` ุงูุฎุงุตุฉ ุจู.

ููุฒูุฏ ูู ุงูุชูุงุตููุ ุฑุงุฌุน ูุณู "ุงูุฏุฑูุณ ุงูุชุนููููุฉ" tutorials ููุชุนุฑู ุจุดูู ุฃุนูู ุนูู ููููุฉ ุงุณุชุฎุฏุงููุ ุฃู ุงุทูุน ุนูู [ูููุฑุฉ ุฌูุฌู ูููุงุจ Google Colab](https://colab.research.google.com/drive/1Lv2RCG_AT6bZNdlL1oDDNNiwBBuirwI-?usp=sharing) ุงูุชูุถูุญูุฉ!

<div class="mt-10">
<div class="w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2 md:gap-y-4 md:gap-x-5">
<a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./tutorials/convert"
><div class="w-full text-center bg-gradient-to-br from-blue-400 to-blue-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">ุงูุฏุฑูุณ ุงูุชุนููููุฉ</div>
<p class="text-gray-700">ุชุนูู ุงูุฃุณุงุณูุงุช ูุชุนุฑูู ุนูู ุชูุงูู ๐ค ู`BetterTransformer`. ุงุจุฏุฃ ูู ููุง ุฅุฐุง ููุช ุชุณุชุฎุฏู ๐ค Optimum ูููุฑุฉ ุงูุฃููู!</p>
</a>
<a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./tutorials/contribute"
><div class="w-full text-center bg-gradient-to-br from-indigo-400 to-indigo-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">ุฃุฏูุฉ ููููุฉ ุงูุงุณุชุฎุฏุงู</div>
<p class="text-gray-700">ูู ุชุฑูุฏ ุฅุถุงูุฉ ูููุฐุฌู ุงูุฎุงุต ูุฏุนู `BetterTransformer`ุ ุงุจุฏุฃ ูู ููุง ููุงุทูุงุน ุนูู ุฏููู ุงููุณุงููุฉ!</p>
</a>
</div>
</div>