# ุงูุงุณุชุฏูุงู ุงููุนุฌู ุนูู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุช NVIDIA

ุจุดูู ุงูุชุฑุงุถูุ ูููู ONNX Runtime ุจุชุดุบูู ุงูุงุณุชุฏูุงู ุนูู ุฃุฌูุฒุฉ CPU. ููุน ุฐููุ ูู ุงููููู ูุถุน ุงูุนูููุงุช ุงููุฏุนููุฉ ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณูููุงุช (GPU) ูู NVIDIAุ ูุน ุชุฑู ุงูุนูููุงุช ุบูุฑ ุงููุฏุนููุฉ ุนูู ูุญุฏุฉ ุงููุนุงูุฌุฉ ุงููุฑูุฒูุฉ (CPU). ูู ูุนุธู ุงูุญุงูุงุชุ ูุณูุญ ุฐูู ุจูุถุน ุงูุนูููุงุช ุงูููููุฉ ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณูููุงุช (GPU) ูุชุณุฑูุน ุงูุงุณุชุฏูุงู ุจุดูู ูุจูุฑ.

ุณููุถุญ ูุฐุง ุงูุฏููู ููููุฉ ุชุดุบูู ุงูุงุณุชุฏูุงู ุนูู ูููุฑูู ููุชูููุฐ ูุฏุนูููุง ONNX Runtime ููุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุช NVIDIA:

- `CUDAExecutionProvider`: ุชุณุฑูุน ุนุงู ุนูู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุช NVIDIA ุงููููููุฉ ูู CUDA.
- `TensorrtExecutionProvider`: ูุณุชุฎุฏู ูุญุฑู ุงูุงุณุชุฏูุงู TensorRT ูู NVIDIA ููููุฑ ุจุดูู ุนุงู ุฃูุถู ุฃุฏุงุก ูู ููุช ุงูุชุดุบูู.

<Tip warning={true}>
ุจุณุจุจ ูููุฏ ONNX Runtimeุ ูุง ูููู ุชุดุบูู ุงูููุงุฐุฌ ุงููููุฉ ุนูู `CUDAExecutionProvider`ุ ููุง ูููู ุชุดุบูู ุณูู ุงูููุงุฐุฌ ุฐุงุช ุงููููุงุช ุงูุซุงุจุชุฉ ุนูู `TensorrtExecutionProvider`.
</Tip>

## CUDAExecutionProvider

### ุชุซุจูุช CUDA

ูู ุจุชุซุจูุช ุงูุชุจุนูุงุช ุงูุฅุถุงููุฉ ุนู ุทุฑูู ุชุดุบูู ูุง ูููุ ุจุดุฑุท ุฃู ุชููู ูุชุทูุจุงุช CUDA ูcuDNN [ูุชููุฑุฉ](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements):

```bash
pip install optimum[onnxruntime-gpu]
```

ูุชุฌูุจ ุญุฏูุซ ุชุนุงุฑุถุงุช ุจูู `onnxruntime` ู`onnxruntime-gpu`ุ ุชุฃูุฏ ูู ุนุฏู ุชุซุจูุช ุญุฒูุฉ `onnxruntime` ุนู ุทุฑูู ุชุดุบูู `pip uninstall onnxruntime` ูุจู ุชุซุจูุช Optimum.

### ุงูุชุญูู ูู ูุฌุงุญ ุชุซุจูุช CUDA

ูุจู ุงููุชุงุจุนุฉุ ูู ุจุชุดุบูู ููุฏ ุงูุนููุฉ ุงูุชุงูู ููุชุญูู ููุง ุฅุฐุง ูุงู ุงูุชุซุจูุช ูุงุฌุญูุง:

```python
>>> from optimum.onnxruntime import ORTModelForSequenceClassification
>>> from transformers import AutoTokenizer

>>> ort_model = ORTModelForSequenceClassification.from_pretrained(
...   "philschmid/tiny-bert-sst2-distilled",
...   export=True,
...   provider="CUDAExecutionProvider",
... )

>>> tokenizer = AutoTokenizer.from_pretrained("philschmid/tiny-bert-sst2-distilled")
>>> inputs = tokenizer("expectations were low, actual enjoyment was high", return_tensors="pt", padding=True)

>>> outputs = ort_model(**inputs)
>>> assert ort_model.providers == ["CUDAExecutionProvider", "CPUExecutionProvider"]
```

ุฅุฐุง ุชู ุชุดุบูู ูุฐุง ุงูููุฏ ุจุณูุงุณุฉุ ุชูุงูููุง! ููุฏ ูุงู ุงูุชุซุจูุช ูุงุฌุญูุง. ุฅุฐุง ูุงุฌูุช ุงูุฎุทุฃ ุงูุชุงูู ุฃู ูุง ุดุงุจู ุฐููุ ูููุงู ุฎุทุฃ ูุง ูู ุชุซุจูุช CUDA ุฃู ONNX Runtime:

```
ValueError: Asked to use CUDAExecutionProvider as an ONNX Runtime execution provider, but the available execution providers are ['CPUExecutionProvider'].
```

### ุงุณุชุฎุฏุงู ูููุฑ ุงูุชูููุฐ CUDA ูุน ุงูููุงุฐุฌ ุฐุงุช ุงููุงุตูุฉ ุงูุนุงุฆูุฉ

ุจุงููุณุจุฉ ููููุงุฐุฌ ุบูุฑ ุงููููุฉุ ูููู ุงูุงุณุชุฎุฏุงู ูุจุงุดุฑูุง. ูุง ุนููู ุณูู ุชุญุฏูุฏ ูุณูุท `provider` ูู ุทุฑููุฉ `ORTModel.from_pretrained()`. ุฅููู ูุซุงู:

```python
>>> from optimum.onnxruntime import ORTModelForSequenceClassification

>>> ort_model = ORTModelForSequenceClassification.from_pretrained(
...   "distilbert-base-uncased-finetuned-sst-2-english",
...   export=True,
...   provider="CUDAExecutionProvider",
... )
```

ุจุนุฏ ุฐููุ ูููู ุงุณุชุฎุฏุงู ุงููููุฐุฌ ูุน ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช (API) ุงูุดุงุฆุนุฉ ูู ๐ค Transformers ููุงุณุชุฏูุงู ูุงูุชููููุ ูุซู [pipelines](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/pipelines).

ุนูุฏ ุงุณุชุฎุฏุงู ุฎุท ุฃูุงุจูุจ Transformersุ ูุงุญุธ ุฃูู ูุฌุจ ุชุนููู ูุณูุท `device` ููููุงู ุจุงููุนุงูุฌุฉ ุงูุฃูููุฉ ูุงูููุงุฆูุฉ ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณูููุงุช (GPU)ุ ููุง ูู ููุถุญ ูู ุงููุซุงู ุงูุชุงูู:

```python
>>> from optimum.pipelines import pipeline
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

>>> pipe = pipeline(task="text-classification", model=ort_model, tokenizer=tokenizer, device="cuda:0")
>>> result = pipe("Both the music and visual were astounding, not to mention the actors performance.")
>>> print(result)  # doctest: +IGNORE_RESULT
# printing: [{'label': 'POSITIVE', 'score': 0.9997727274894714}]
```

ุจุงูุฅุถุงูุฉ ุฅูู ุฐููุ ููููู ุชูุฑูุฑ ุฎูุงุฑ ุงูุฌูุณุฉ `log_severity_level = 0` (verbose) ููุชุญูู ููุง ุฅุฐุง ูุงูุช ุฌููุน ุงูุนูุฏ ููุฌูุฏุฉ ุจุงููุนู ุนูู ูููุฑ ุงูุชูููุฐ CUDA ุฃู ูุง:

```python
>>> import onnxruntime

>>> session_options = onnxruntime.SessionOptions()
>>> session_options.log_severity_level = 0

>>> ort_model = ORTModelForSequenceClassification.from_pretrained(
...     "distilbert-base-uncased-finetuned-sst-2-english",
...     export=True,
...     provider="CUDAExecutionProvider",
...     session_options=session_options
... )
```

ูุฌุจ ุฃู ุชุดุงูุฏ ุงูุณุฌูุงุช ุงูุชุงููุฉ:

```
2022-10-18 14:59:13.728886041 [V:onnxruntime:, session_state.cc:1193 VerifyEachN
odeIsAssignedToAnEp] Provider: [CPUExecutionProvider]: [Gather (Gather_76), Uns
queeze (Unsqueeze_78), Gather (Gather_97), Gather (Gather_100), Concat (Concat_1
10), Unsqueeze (Unsqueeze_125), ...]
2022-10-18 14:59:13.728906431 [V:onnxruntime:, session_state.cc:1193 VerifyEachN
odeIsAssignedToAnEp] Provider: [CUDAExecutionProvider]: [Shape (Shape_74), Slic
e (Slice_80), Gather (Gather_81), Gather (Gather_82), Add (Add_83), Shape (Shape
_95), MatMul (MatMul_101), ...]
```

ูู ูุฐุง ุงููุซุงูุ ูููููุง ุฃู ูุฑู ุฃู ุฌููุน ุนูููุงุช ุงูุถุฑุจ ุงูููููุฉ ูู ุงููุตูููุฉ ููุฌูุฏุฉ ุนูู ูููุฑ ุงูุชูููุฐ CUDA.

### ุงุณุชุฎุฏุงู ูููุฑ ุงูุชูููุฐ CUDA ูุน ุงูููุงุฐุฌ ุงููููุฉ

ุจุณุจุจ ุงููููุฏ ุงูุญุงููุฉ ูู ONNX Runtimeุ ูุง ูููู ุงุณุชุฎุฏุงู ุงูููุงุฐุฌ ุงููููุฉ ูุน `CUDAExecutionProvider`. ูุงูุฃุณุจุงุจ ูู ููุง ููู:

- ุนูุฏ ุงุณุชุฎุฏุงู ุงููู ุงูุฏููุงูููู ูู ๐ค Optimumุ ูุฏ ูุชู ุฅุฏุฑุงุฌ ุนูุฏ ูุซู [`MatMulInteger`](https://github.com/onnx/onnx/blob/v1.12.0/docs/Operators.md#MatMulInteger) ู [`DynamicQuantizeLinear`](https://github.com/onnx/onnx/blob/v1.12.0/docs/Operators.md#DynamicQuantizeLinear) ูู ุฑุณู ONNX ุงูุจูุงููุ ูุงูุชู ูุง ูููู ููููุฑ ุงูุชูููุฐ CUDA ุงุณุชููุงููุง.
- ุนูุฏ ุงุณุชุฎุฏุงู ุงููู ุงูุซุงุจุชุ ุณูุชุถูู ุฑุณู ุงูุญุณุงุจ ONNX ุนูููุงุช ุงูุถุฑุจ ูู ุงููุตูููุฉ ูุงูุถุฑุจุงุช ุงูุงูุฒูุงููุฉ ูู ุงูุญุณุงุจ ุงูุนุงุฆูุ ุฅูู ุฌุงูุจ ุนูููุงุช ุงูุชูููู ูุฅูุบุงุก ุงูุชูููู ููุญุงูุงุฉ ุงูุชูููู. ูู ูุฐู ุงูุญุงูุฉุ ุนูู ุงูุฑุบู ูู ุฃู ุนูููุงุช ุงูุถุฑุจ ูู ุงููุตูููุฉ ูุงูุถุฑุจุงุช ุงูุงูุฒูุงููุฉ ุงูููููุฉ ุณุชุนูู ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณูููุงุช (GPU)ุ ุฅูุง ุฃููุง ุณุชุณุชุฎุฏู ุงูุญุณุงุจ ุงูุนุงุฆู ุญูุซ ูุง ูููู ููููุฑ ุงูุชูููุฐ CUDA ุงุณุชููุงู ุนูุฏ ุงูุชูููู ูุฅูุบุงุก ุงูุชูููู ูุงุณุชุจุฏุงููุง ุจุงูุนูููุงุช ุงูุชู ุชุณุชุฎุฏู ุงูุญุณุงุจ ุงูุตุญูุญ.

### ุชูููู ุงูุจุตูุฉ ุงูุฐุงูุฑูุฉ ุจุงุณุชุฎุฏุงู IOBinding

[IOBinding](https://onnxruntime.ai/docs/api/python/api_summary.html#iobinding) ูู ุทุฑููุฉ ูุนุงูุฉ ูุชุฌูุจ ูุณุฎ ุงูุจูุงูุงุช ุงููููู ุนูุฏ ุงุณุชุฎุฏุงู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุช (GPU). ุจุดูู ุงูุชุฑุงุถูุ ุณููุณุฎ ONNX Runtime ุงูุฅุฏุฎุงู ูู ูุญุฏุฉ ุงููุนุงูุฌุฉ ุงููุฑูุฒูุฉ (CPU) (ุญุชู ุฅุฐุง ุชู ูุณุฎ ุงูููุณูุฌุงุช ุจุงููุนู ุฅูู ุงูุฌูุงุฒ ุงููุณุชูุฏู)ุ ูููุชุฑุถ ุฃู ุงูุฅุฎุฑุงุฌ ูุญุชุงุฌ ุฃูุถูุง ุฅูู ูุณุฎู ูุฑุฉ ุฃุฎุฑู ุฅูู ูุญุฏุฉ ุงููุนุงูุฌุฉ ุงููุฑูุฒูุฉ (CPU) ูู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุช (GPU) ุจุนุฏ ุงูุชุดุบูู. ุชุนุชุจุฑ ุนูููุงุช ูุณุฎ ุงูุจูุงูุงุช ูุฐู ุจูู ุงููุถูู ูุงูุฃุฌูุฒุฉ ููููุฉุ ููููู ุฃู ุชุคุฏู ุฅูู ุฒูุงุฏุฉ ููุช ุงูุงุณุชุฏูุงู ุนู PyTorch ุงูุนุงุฏูุ ุฎุงุตุฉ ูุนูููุฉ ูู ุงูุชุดููุฑ.

ูุชุฌูุจ ุงูุชุจุงุทุคุ ูุนุชูุฏ ๐ค Optimum IOBinding ููุณุฎ ุงูุฅุฏุฎุงูุงุช ุฅูู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณูููุงุช (GPU) ูุชุญุฏูุฏ ูุณุงุญุฉ ุฐุงูุฑุฉ ุงูุฅุฎุฑุงุฌ ูุณุจููุง ูุจู ุงูุงุณุชุฏูุงู. ุนูุฏ ุฅูุดุงุก ูุซูู ูู `ORTModel`ุ ูู ุจุชุนููู ูููุฉ ูุณูุท `use_io_binding` ูุงุฎุชูุงุฑ ูุง ุฅุฐุง ูุงู ุณูุชู ุชุดุบูู IOBinding ุฃุซูุงุก ุงูุงุณุชุฏูุงู ุฃู ูุง. ูุชู ุชุนููู `use_io_binding` ุฅูู `True` ุจุดูู ุงูุชุฑุงุถูุ ุฅุฐุง ุงุฎุชุฑุช CUDA ููููุฑ ููุชูููุฐ.

ูุฅุฐุง ููุช ุชุฑูุฏ ุฅููุงู ุชุดุบูู IOBinding:

```python
>>> from transformers import AutoTokenizer, pipeline
>>> from optimum.onnxruntime import ORTModelForSeq2SeqLM

# ุชุญููู ุงููููุฐุฌ ูู ุงููุฑูุฒ ูุชุตุฏูุฑู ุฅูู ุชูุณูู ONNX
>>> model = ORTModelForSeq2SeqLM.from_pretrained("t5-small", export=True, use_io_binding=False)
>>> tokenizer = AutoTokenizer.from_pretrained("t5-small")

# ุฅูุดุงุก ุฎุท ุฃูุงุจูุจ
>>> onnx_translation = pipeline("translation_en_to_fr", model=model, tokenizer=tokenizer, device="cuda:0")
```

ูู ุงูููุช ุงูุญุงููุ ูุชู ุฏุนู IOBinding ููููุงุฐุฌ ุงููุญุฏุฏุฉ ูููููุฉุ ูุฅุฐุง ููุช ุชุฑูุฏ ููุง ุฅุถุงูุฉ ุฏุนู ููููุงุฐุฌ ุงููุฎุตุตุฉุ ููู ุจุฅูุดุงุก ูุดููุฉ ูู ูุณุชูุฏุน Optimum.

### ุงูููุงุณุจ ุงูุฒูููุฉ ุงูููุญูุธุฉ

ููุฏ ุงุฎุชุจุฑูุง ุซูุงุซุฉ ููุงุฐุฌ ุดุงุฆุนุฉ ูุน ุนูููุฉ ูู ุชุดููุฑ: `GPT2` / `T5-small` / `M2M100-418M`ุ ูุชู ุชุดุบูู ุงููุนูุงุฑ ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณูููุงุช Tesla T4 ูุชุนุฏุฏุฉ ุงูุงุณุชุฎุฏุงูุงุช (ุชูุฌุฏ ุชูุงุตูู ุจูุฆูุฉ ุฃูุซุฑ ูู ููุงูุฉ ูุฐุง ุงููุณู).

ูููุง ููู ุจุนุถ ูุชุงุฆุฌ ุงูุฃุฏุงุก ุนูุฏ ุงูุชุดุบูู ูุน `CUDAExecutionProvider` ุนูุฏูุง ุชู ุชุดุบูู IOBinding. ููุฏ ุงุฎุชุจุฑูุง ุฃุทูุงู ุชุณูุณู ุงูุฅุฏุฎุงู ูู 8 ุฅูู 512ุ ููููุง ุจุชูููุฏ ุงูุฅุฎุฑุงุฌ ุจุงุณุชุฎุฏุงู ูู ูู ุงูุจุญุซ ุงูุดุฑู ูุงูุจุญุซ ุงูุดุนุงุนู (`num_beam=5`):

<table><tr>
<td>
<p align="center">
<img alt="GPT2" src="https://huggingface.co/datasets/optimum/documentation-images/resolve/main/onnxruntime/t4_res_ort_gpt2.png" width="450">
<br>
<em style="color: grey">GPT2</em>
</p>
</td>
<td>
<p align="center">
<img alt="T5-small" src="https://huggingface.co/datasets/optimum/documentation-images/resolve/main/onnxruntime/t4_res_ort_t5_s.png" width="450">
<br>
<em style="color: grey">T5-small</em>
</p>
</td></tr>
<tr><td>
<p align="center">
<img alt="M2M100-418M" src="https://huggingface.co/datasets/optimum/documentation-images/resolveMzMz/onnxruntime/t4_res_ort_m2m100_418m.png" width="450">
<br>
<em style="color: grey">M2M100-418M</em>
</p>
</td>
</tr></table>

ูููุง ููุฎุต ูููุช ุงูุชูููุฑ ูุน ุฃุทูุงู ุชุณูุณู ูุฎุชููุฉ (32 / 128) ูุฃูุถุงุน ุงูุชูููุฏ (ุงูุจุญุซ ุงูุดุฑู / ุงูุจุญุซ ุงูุดุนุงุนู) ุฃุซูุงุก ุงุณุชุฎุฏุงู ONNX Runtime ููุงุฑูุฉ ุจู PyTorch:

<table><tr>
<td>
<p align="center">
<img alt="seq32" src="https://huggingface.co/datasets/optimum/documentation-images/resolve/main/onnxruntime/inference_models_32.png" width="800">
<br>
<em style="color: grey">ุทูู ุงูุชุณูุณู: 32</em>
</p>
</td></tr>
<tr><td>
<p align="center">
<img alt="seq128" src="https://huggingface.co/datasets/optimum/documentation-images/resolve/main/onnxruntime/inference_models_128.png" width="800">
<br>
<em style="color: grey">ุทูู ุงูุชุณูุณู: 128</em>
</p>
</td>
</tr></table>

ุงูุจูุฆุฉ:

```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 11.3     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |
| N/A   28C    P8     8W /  70W |      0MiB / 15109MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

- ุงูููุตุฉ: Linux-5.4.0-1089-aws-x86_64-with-glibc2.29
- ุฅุตุฏุงุฑ Python: 3.8.10
- ุฅุตุฏุงุฑ `transformers`: 4.24.0
- ุฅุตุฏุงุฑ `optimum`: 1.5.0
- ุฅุตุฏุงุฑ PyTorch: 1.12.0+cu113
```

ูุงุญุธ ุฃู ุงูุชุฌุงุฑุจ ุงูุณุงุจูุฉ ุชุนูู ูุน ููุงุฐุฌ ONNX ุงูุนุงุฏูุฉ ุงููุตุฏุฑุฉ ูุจุงุดุฑุฉ ูู ุงููุตุฏุฑ. ุฅุฐุง ููุช ููุชููุง ุจุงูุชุณุฑูุน ุงูุฅุถุงููุ ููููููุ ุจุงุณุชุฎุฏุงู `ORTOptimizer`ุ ุชุญุณูู ุงูุฑุณู ุงูุจูุงูู ูุชุญููู ูููุฐุฌู ุฅูู FP16 ุฅุฐุง ูุงู ูุฏูู ูุญุฏุฉ ูุนุงูุฌุฉ ุฑุณูููุงุช (GPU) ุจูุฏุฑุงุช ุงูุฏูุฉ ุงููุฎุชูุทุฉ.

## TensorrtExecutionProvider

ูุณุชุฎุฏู TensorRT ูุฌููุนุฉ ุงูุชุนุฏููุงุช ุงูุฎุงุตุฉ ุจูุ ู**ูุง ูุฏุนู ุจุดูู ุนุงู ุงูุชุนุฏููุงุช ูู [`~onnxruntime.ORTOptimizer`]**. ูุฐููุ ููุตู ุจุงุณุชุฎุฏุงู ุงูููุงุฐุฌ ุงูุฃุตููุฉ ูู ONNX ุนูุฏ ุงุณุชุฎุฏุงู TensorrtExecutionProvider ([reference](https://github.com/microsoft/onnxruntime/issues/10905#issuecomment-1072649358)).

### ุชุซุจูุช TensorRT

ุฃุณูู ุทุฑููุฉ ูุงุณุชุฎุฏุงู TensorRT ููููุฑ ููุชูููุฐ ููููุงุฐุฌ ุงููุญุณูุฉ ูู ุฎูุงู ๐ค Optimum ูู ุงุณุชุฎุฏุงู ูููุฑ ุงูุชูููุฐ `TensorrtExecutionProvider` ุงููุชุงุญ ูู ONNX Runtime.

ูุงุณุชุฎุฏุงู ๐ค Optimum ูุน TensorRT ูู ุจูุฆุฉ ูุญููุฉุ ููุตู ุจุงุชุจุงุน ุฃุฏูุฉ ุงูุชุซุจูุช ุงูุฎุงุตุฉ ุจู NVIDIA:

- CUDA toolkit: https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html
- cuDNN: https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html
- TensorRT: https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html

ุจุงููุณุจุฉ ุฅูู TensorRTุ ููุตู ุจุทุฑููุฉ ุชุซุจูุช ููู TAR. ุฃู ูููู ุชุซุจูุช TensorRT ุจุงุณุชุฎุฏุงู `pip` ุจุงุชุจุงุน [ูุฐู ุงูุชุนูููุงุช](https://github.com/microsoft/onnxruntime/issues/9986).

ุจูุฌุฑุฏ ุชุซุจูุช ุงูุญุฒู ุงููุทููุจุฉุ ูุฌุจ ุชุนููู ูุชุบูุฑุงุช ุงูุจูุฆุฉ ุงูุชุงููุฉ ุจุงุณุชุฎุฏุงู ุงููุณุงุฑุงุช ุงูููุงุณุจุฉ ุญุชู ูุชููู ONNX Runtime ูู ุงูุชุดุงู ุชุซุจูุช TensorRT:

```bash
export CUDA_PATH=/usr/local/cuda
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-x.x/lib64:/path/to/TensorRT-8.x.x/lib
```
### ุงูุชุญูู ูู ูุฌุงุญ ุชุซุจูุช TensorRT

ูุจู ุงููุชุงุจุนุฉุ ูู ุจุชุดุบูู ููุฏ ุงููุซุงู ุงูุชุงูู ููุชุญูู ููุง ุฅุฐุง ูุงู ุงูุชุซุจูุช ูุงุฌุญูุง:

```python
>>> from optimum.onnxruntime import ORTModelForSequenceClassification
>>> from transformers import AutoTokenizer

>>> ort_model = ORTModelForSequenceClassification.from_pretrained(
...     "philschmid/tiny-bert-sst2-distilled",
...     export=True,
...     provider="TensorrtExecutionProvider",
... )

>>> tokenizer = AutoTokenizer.from_pretrained("philschmid/tiny-bert-sst2-distilled")
>>> inp = tokenizer("expectations were low, actual enjoyment was high", return_tensors="pt", padding=True)

>>> result = ort_model(**inp)
>>> assert ort_model.providers == ["TensorrtExecutionProvider", "CUDAExecutionProvider", "CPUExecutionProvider"]
```

ุฅุฐุง ุชู ุชุดุบูู ูุฐุง ุงูููุฏ ุจุณูุงุณุฉุ ุชูุงูููุง! ููุฏ ูุงู ุงูุชุซุจูุช ูุงุฌุญูุง.

ุฅุฐุง ูุดู ุงูุชุฃููุฏ ุฃุนูุงูุ ุฃู ุฅุฐุง ูุงุฌูุช ุงูุชุญุฐูุฑ ุงูุชุงูู:

```
Failed to create TensorrtExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html#requirements to ensure all dependencies are met.
```

ููุฏ ุญุฏุซ ุฎุทุฃ ูุง ูู ุชุซุจูุช TensorRT ุฃู ONNX Runtime.

### ุจูุงุก ูุญุฑู TensorRT ูุงูุฅุญูุงุก

ูุชุทูุจ TensorRT ุจูุงุก [ูุญุฑู ุงูุงุณุชุฏูุงู](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#build-phase) ูุณุจููุง ูุจู ุงูุงุณุชุฏูุงูุ ุงูุฃูุฑ ุงูุฐู ูุณุชุบุฑู ุจุนุถ ุงูููุช ุจุณุจุจ ุชุญุณูู ุงููููุฐุฌ ูุฏูุฌ ุงูุนูุฏ. ูุชุฌูุจ ุฅุนุงุฏุฉ ุจูุงุก ุงููุญุฑู ูู ูู ูุฑุฉ ูุชู ูููุง ุชุญููู ุงููููุฐุฌุ ูููุฑ ONNX Runtime ุฒูุฌูุง ูู ุงูุฎูุงุฑุงุช ูุญูุธ ุงููุญุฑู: `trt_engine_cache_enable` ู`trt_engine_cache_path`.

ููุตู ุจุชุนููู ูุฐูู ุงูุฎูุงุฑูู ุนูุฏ ุงุณุชุฎุฏุงู ูููุฑ ุงูุชูููุฐ TensorRT. ูููุง ููู ูุซุงู ุนูู ุงูุงุณุชุฎุฏุงูุ ุญูุซ [`optimum/gpt2`](https://huggingface.co/optimum/gpt2) ูู ูููุฐุฌ ONNX ูุญูู ูู PyTorch ุจุงุณุชุฎุฏุงู [ูุตุฏุฑ ONNX Optimum](https://huggingface.co/docs/optimum/main/en/exporters/onnx/usage_guides/export_a_model):

```python
>>> from optimum.onnxruntime import ORTModelForCausalLM

>>> provider_options = {
...     "trt_engine_cache_enable": True,
...     "trt_engine_cache_path": "tmp/trt_cache_gpt2_example"
... }

# ูุง ูุชู ุจูุงุก ูุญุฑู TensorRT ููุงุ ุจู ูุชู ุจูุงุคู ุฃุซูุงุก ุงูุงุณุชุฏูุงู
>>> ort_model = ORTModelForCausalLM.from_pretrained(
...     "optimum/gpt2",
...     use_cache=False,
...     provider="TensorrtExecutionProvider",
...     provider_options=provider_options
... )
```

ูุจูู TensorRT ูุญุฑูู ุจูุงุกู ุนูู ุฃุดูุงู ุงูุฅุฏุฎุงู ุงููุญุฏุฏุฉ. ููุฃุณูุ ูู [ุงูุชูููุฐ ุงูุญุงูู ูู ONNX Runtime](https://github.com/microsoft/onnxruntime/blob/613920d6c5f53a8e5e647c5f1dcdecb0a8beef31/onnxruntime/core/providers/tensorrt/tensorrt_execution_provider.cc#L1677-L1688) (ุงููุฑุงุฌุน: [1](https://github.com/microsoft/onnxruntime/issues/13559)ุ [2](https://github.com/microsoft/onnxruntime/issues/13851))ุ ูุชู ุฅุนุงุฏุฉ ุจูุงุก ุงููุญุฑู ูู ูู ูุฑุฉ ูููู ูููุง ุงูุฅุฏุฎุงู ุฃุตุบุฑ ูู ุฃุตุบุฑ ุดูู ุชู ููุงุฌูุชู ุณุงุจููุงุ ูุงูุนูุณ ุตุญูุญ ุฅุฐุง ูุงู ุงูุฅุฏุฎุงู ุฃูุจุฑ ูู ุฃูุจุฑ ุดูู ุชู ููุงุฌูุชู ุณุงุจููุง. ุนูู ุณุจูู ุงููุซุงูุ ุฅุฐุง ูุงู ุงููููุฐุฌ ูุฃุฎุฐ `(batch_sizeุ input_ids)` ูุฅุฏุฎุงูุงุชุ ููุฃุฎุฐ ุงููููุฐุฌ ุนูู ุงูุชูุงูู ุงูุฅุฏุฎุงูุงุช ุงูุชุงููุฉ:

1. `input.shape: (4ุ 5) --> ูุชู ุจูุงุก ุงููุญุฑู (ุฃูู ุฅุฏุฎุงู)`
2. `input.shape: (4ุ 10) --> ุฅุนุงุฏุฉ ุจูุงุก ุงููุญุฑู (10 ุฃูุจุฑ ูู 5)`
3. `input.shape: (4ุ 7) --> ูุง ุฅุนุงุฏุฉ ุจูุงุก (5 <= 7 <= 10)`
4. `input.shape: (4ุ 12) --> ุฅุนุงุฏุฉ ุจูุงุก ุงููุญุฑู (10 <= 12)`
5. `input.shape: (4ุ 3) --> ุฅุนุงุฏุฉ ุจูุงุก ุงููุญุฑู (3 <= 5)`

ุชุชูุซู ุฅุญุฏู ุงููุดููุงุช ุงููุจูุฑุฉ ูู ุฃู ุจูุงุก ุงููุญุฑู ูุฏ ูุณุชุบุฑู ููุชูุง ุทูููุงูุ ุฎุงุตุฉ ุจุงููุณุจุฉ ููููุงุฐุฌ ุงููุจูุฑุฉ. ูุฐููุ ูุญู ุจุฏููุ ุชุชูุซู ุฅุญุฏู ุงูุชูุตูุงุช ูู **ุจูุงุก ูุญุฑู TensorRT ุฃููุงู ุจุฅุฏุฎุงู ุดูู ุตุบูุฑุ ุซู ุจุฅุฏุฎุงู ุดูู ูุจูุฑ ููููู ูุฏูู ูุญุฑู ุตุงูุญ ูุฌููุน ุงูุฃุดูุงู ุงููุชูุณุทุฉ**. ูุณูุญ ุฐูู ุจุชุฌูุจ ุฅุนุงุฏุฉ ุจูุงุก ุงููุญุฑู ูุฃุดูุงู ุฌุฏูุฏุฉ ุตุบูุฑุฉ ููุจูุฑุฉุ ููู ุฃูุฑ ุบูุฑ ูุฑุบูุจ ููู ุจูุฌุฑุฏ ูุดุฑ ุงููููุฐุฌ ููุงุณุชุฏูุงู.

ูู ุฎูุงู ุชูุฑูุฑ ูุณุงุฑ ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ูููุญุฑู ูู ุฎูุงุฑุงุช ุงููููุฑุ ูููู ุจูุงุก ุงููุญุฑู ูุฑุฉ ูุงุญุฏุฉ ูุฌููุนูุง ูุงุณุชุฎุฏุงูู ุจุงููุงูู ููุงุณุชุฏูุงู ุจุนุฏ ุฐูู.

ุนูู ุณุจูู ุงููุซุงูุ ุจุงููุณุจุฉ ูุชูููุฏ ุงููุตุ ูููู ุจูุงุก ุงููุญุฑู ุจุงุณุชุฎุฏุงู:

```python
>>> import os
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import ORTModelForCausalLM

>>> os.makedirs("tmp/trt_cache_gpt2_example", exist_ok=True)
>>> provider_options = {
...     "trt_engine_cache_enable": True,
...     "trt_engine_cache_path": "tmp/trt_cache_gpt2_example"
... }

>>> ort_model = ORTModelForCausalLM.from_pretrained(
...     "optimum/gpt2",
...     use_cache=False,
...     provider="TensorrtExecutionProvider",
...     provider_options=provider_options,
... )
>>> tokenizer = AutoTokenizer.from_pretrained("optimum/gpt2")

>>> print("Building engine for a short sequence...")  # doctest: +IGNORE_RESULT
>>> text = ["short"]
>>> encoded_input = tokenizer(text, return_tensors="pt").to("cuda")
>>> output = ort_model(**encoded_input)

>>> print("Building engine for a long sequence...")  # doctest: +IGNORE_RESULT
>>> text = ["a very long input just for demo purpose, this is very long" * 10]
>>> encoded_input = tokenizer(text, return_tensors="pt").to("cuda")
>>> output = ort_model(**encoded_input)
```

ูุชู ุชุฎุฒูู ุงููุญุฑู ุนูู ุงููุญู ุงูุชุงูู:

![ูุฌูุฏ ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ููุญุฑู TensorRT](https://huggingface.co/datasets/optimum/documentation-images/resolve/main/onnxruntime/tensorrt_cache.png)

ุจูุฌุฑุฏ ุจูุงุก ุงููุญุฑูุ ูููู ุฅุนุงุฏุฉ ุชุญููู ุงูุฐุงูุฑุฉ ุงููุคูุชุฉ ููุง ุชุญุชุงุฌ ุงูุชูููุฏ ุฅูู ุฅุนุงุฏุฉ ุจูุงุก ุงููุญุฑู:

```python
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import ORTModelForCausalLM

>>> provider_options = {
...     "trt_engine_cache_enable": True,
...     "trt_engine_cache_path": "tmp/trt_cache_gpt2_example"
... }

>>> ort_model = ORTModelForCausalLM.from_pretrained(
...     "optimum/gpt2",
...     use_cache=False,
...     provider="TensorrtExecutionProvider",
...     provider_options=provider_options,
... )
>>> tokenizer = AutoTokenizer.from_pretrained("optimum/gpt2")

>>> text = ["ุงุณุชุจุฏููู ุจุฃู ูุต ุชุฑูุฏ."]
>>> encoded_input = tokenizer(textุ return_tensors="pt").to("cuda")

>>> for i in range(3):
...     output = ort_model.generate(**encoded_input)
...     print(tokenizer.decode(output[0]))  # doctest: +IGNORE_RESULT
```

#### ุงูุฅุญูุงุก

ุจูุฌุฑุฏ ุจูุงุก ุงููุญุฑูุ ููุตู ุจุงูููุงู ุจุฎุทูุฉ ุฃู ุฎุทูุงุช ุฅุญูุงุก ูุจู ุงูุงุณุชุฏูุงูุ ุญูุซ ุฃู ุชุดุบููุงุช ุงูุงุณุชุฏูุงู ุงูุฃููู ุจูุง [ุจุนุถ ุงููููุงุช ุงูุนุงูุฉ](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#trtexec-flags).

### ุงุณุชุฎุฏุงู ูููุฑ ุงูุชูููุฐ TensorRT ูุน ุงูููุงุฐุฌ ุฐุงุช ุงูููุทุฉ ุงูุนุงุฆูุฉ

ุจุงููุณุจุฉ ููููุงุฐุฌ ุบูุฑ ุงููููุฉุ ูููู ุงูุงุณุชุฎุฏุงู ูุจุงุดุฑูุงุ ูุฐูู ุจุจุณุงุทุฉ ุจุงุณุชุฎุฏุงู ูุณูุท `provider` ูู `ORTModel.from_pretrained()`. ุนูู ุณุจูู ุงููุซุงู:

```python
>>> from optimum.onnxruntime import ORTModelForSequenceClassification

>>> ort_model = ORTModelForSequenceClassification.from_pretrained(
...     "distilbert-base-uncased-finetuned-sst-2-english"ุ
...     export=True,
...     provider="TensorrtExecutionProvider"ุ
... )
```

[ููุง ูู ููุถุญ ุณุงุจููุง ูู `CUDAExecutionProvider`](#use-cuda-execution-provider-with-floatingpoint-models)ุ ูู ุฎูุงู ุชูุฑูุฑ ุฎูุงุฑ ุงูุฌูุณุฉ `log_severity_level = 0` (verbose)ุ ูููููุง ุงูุชุญูู ูู ุงูุณุฌูุงุช ููุง ุฅุฐุง ูุงูุช ุฌููุน ุงูุนูุฏ ููุถูุนุฉ ุนูู ูููุฑ ุงูุชูููุฐ TensorRT ุฃู ูุง:

```
2022-09-22 14:12:48.371513741 [V:onnxruntime:, session_state.cc:1188 VerifyEachNodeIsAssignedToAnEp] All nodes have been placed on [TensorrtExecutionProvider]
```

ุจุนุฏ ุฐููุ ูููู ุงุณุชุฎุฏุงู ุงููููุฐุฌ ูุน ูุงุฌูุฉ ุจุฑูุฌุฉ ุชุทุจููุงุช Transformers ุงูุดุงุฆุนุฉ ููุงุณุชุฏูุงู ูุงูุชููููุ ูุซู [pipelines](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/pipelines).

### ุงุณุชุฎุฏุงู ูููุฑ ุงูุชูููุฐ TensorRT ูุน ุงูููุงุฐุฌ ุงููููุฉ

ุนูุฏูุง ูุชุนูู ุงูุฃูุฑ ุจุงูููุงุฐุฌ ุงููููุฉุ ูุฏุนู TensorRT ููุท ุงูููุงุฐุฌ ุงูุชู ุชุณุชุฎุฏู [**ุงููููุฉ ุงูุซุงุจุชุฉ**](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#enable_int8_c) ูุน [**ุงููููุฉ ุงููุชูุงุซูุฉ** ูููุฒู ูุงูุชูุดูุท](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#intro-quantization).

ูููุฑ Optimum ุชูููู ูููุฉ ุฌุงูุฒูุง ููุงุณุชุฎุฏุงู ูุน [`~onnxruntime.ORTQuantizer`] ูุน ูููุฏ ุงููููุฉ TensorRT:

```python
>>> from optimum.onnxruntime import AutoQuantizationConfig

>>> qconfig = AutoQuantizationConfig.tensorrt(per_channel=False)
```

ุจุงุณุชุฎุฏุงู ูุฐุง `qconfig`ุ ูููู ุฅุฌุฑุงุก ุงููููุฉ ุงูุซุงุจุชุฉ ููุง ูู ููุถุญ ูู [ุฏููู ุงููููุฉ ุงูุซุงุจุชุฉ](quantization#static-quantization-example).

ูู ุนููุฉ ุงูููุฏ ุฃุฏูุงูุ ุจุนุฏ ุฅุฌุฑุงุก ุงููููุฉ ุงูุซุงุจุชุฉุ ูุชู ุชุญููู ุงููููุฐุฌ ุงููุงุชุฌ ูู ูุฆุฉ [`~onnxruntime.ORTModel`] ุจุงุณุชุฎุฏุงู TensorRT ููููุฑ ุงูุชูููุฐ. ูุฌุจ ุชุนุทูู ุชุญุณูู ุงูุฑุณู ุงูุจูุงูู ูู ONNX Runtime ูููููุฐุฌ ููุชู ุงุณุชููุงูู ูุชุญุณููู ุจูุงุณุทุฉ TensorRTุ ููุฌุจ ุชุญุฏูุฏ ุญูููุฉ ุฃู ุนูููุงุช INT8 ุชุณุชุฎุฏู ู TensorRT.

```python
>>> import onnxruntime
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import ORTModelForSequenceClassification

>>> session_options = onnxruntime.SessionOptions()
>>> session_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_DISABLE_ALL

>>> tokenizer = AutoTokenizer.from_pretrained("fxmarty/distilbert-base-uncased-sst2-onnx-int8-for-tensorrt")
>>> ort_model = ORTModelForSequenceClassification.from_pretrained(
...     "fxmarty/distilbert-base-uncased-sst2-onnx-int8-for-tensorrt"ุ
...     provider="TensorrtExecutionProvider"ุ
...     session_options=session_options,
...     provider_options={"trt_int8_enable": True}ุ
... )

>>> inp = tokenizer("TensorRT ูู ุฃูุฑ ูุคูู ุจุนุถ ุงูุดูุก ููุงุณุชุฎุฏุงูุ ูููู ูู ููุงูุฉ ุงููููุ ูุนูู ุจุณูุงุณุฉ ูุจุณุฑุนุฉ ูุงุฆูุฉ!"ุ return_tensors="np")

>>> res = ort_model(**inp)

>>> print(res)
>>> print(ort_model.config.id2label[res.logits[0].argmax()])
>>> # SequenceClassifierOutput(loss=None, logits=array([[-0.545066, 0.5609764]], dtype=float32), hidden_states=None, attentions=None)
>>> # ุฅูุฌุงุจู
```

ุจุนุฏ ุฐููุ ูููู ุงุณุชุฎุฏุงู ุงููููุฐุฌ ูุน ูุงุฌูุฉ ุจุฑูุฌุฉ ุชุทุจููุงุช Transformers ุงูุดุงุฆุนุฉ ููุงุณุชุฏูุงู ูุงูุชููููุ ูุซู [pipelines](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/pipelines).

### ูููุฏ TensorRT ููููุงุฐุฌ ุงููููุฉ

ููุง ุชู ุชุณููุท ุงูุถูุก ุนููู ูู ุงููุณู ุงูุณุงุจูุ ูุฏุนู TensorRT ููุท ูุฌููุนุฉ ูุญุฏูุฏุฉ ูู ุงูููุงุฐุฌ ุงููููุฉ:

- ุงููููุฉ ุงูุซุงุจุชุฉ ููุท
- ูุทุงูุงุช ุงููููุฉ ูููุฒู ูุงูุชูุดูุท ูุชูุงุซูุฉ
- ูุฌุจ ุชุฎุฒูู ุงูุฃูุฒุงู ูู float32 ูู ูููุฐุฌ ONNXุ ูุจุงูุชุงูู ูุง ููุฌุฏ ุชูููุฑ ูุณุงุญุฉ ุงูุชุฎุฒูู ูู ุงููููุฉ. ูู ุงููุงูุนุ ูุชุทูุจ TensorRT ุฅุฏุฑุงุฌ ุฃุฒูุงุฌ ุงููููุฉ ุงููุงููุฉ ูุฅูุบุงุก ุงููููุฉ. ุนุงุฏุฉุ ุณูุชู ุชุฎุฒูู ุงูุฃูุฒุงู ุจุชูุณูู ุงูููุทุฉ ุงูุซุงุจุชุฉ 8 ุจุช ููู ูุชู ุชุทุจูู ุณูู `DequantizeLinear` ุนูู ุงูุฃูุฒุงู.

ูู ุญุงูุฉ ุชูุฑูุฑ `provider="TensorrtExecutionProvider"` ููู ูุชู ุชูููุฉ ุงููููุฐุฌ ุจุฏูุฉ ููููุง ููุฐู ุงููููุฏุ ููุฏ ูุชู ุฑูุน ุงุณุชุซูุงุกุงุช ูุฎุชููุฉุ ููุฏ ุชููู ุฑุณุงุฆู ุงูุฎุทุฃ ุบูุฑ ูุงุถุญุฉ.

### ุงูููุงุณุจ ุงูุฒูููุฉ ุงูููุญูุธุฉ

ูููู ุงุณุชุฎุฏุงู ุฃุฏุงุฉ Nvidia Nsight Systems ูุชุตููู ููุช ุงูุชูููุฐ ุนูู GPU. ูุจู ุงูุชูุตูู ุฃู ููุงุณ ุงููููู/ุงูุณุฑุนุฉุ ูู ุงูุฌูุฏ ุฅุฌุฑุงุก ุจุถุน **ุฎุทูุงุช ุฅุญูุงุก**.

ูุฑูุจุง!