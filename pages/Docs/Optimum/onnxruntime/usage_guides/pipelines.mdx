# أنابيب الاستدلال مع مسرع ONNX Runtime

تسهل وظيفة [`~pipelines.pipeline`] استخدام النماذج من [Model Hub](https://huggingface.co/models) للاستدلال المعجل على مجموعة متنوعة من المهام مثل تصنيف النصوص، والأسئلة والإجابة، وتصنيف الصور.

<Tip>

يمكنك أيضًا استخدام وظيفة
[pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#pipelines) من
Transformers وتوفير فئة النموذج الأمثل.

</Tip>

في الوقت الحالي، المهام المدعومة هي:

- `feature-extraction`
- `text-classification`
- `token-classification`
- `question-answering`
- `zero-shot-classification`
- `text-generation`
- `text2text-generation`
- `summarization`
- `translation`
- `image-classification`
- `automatic-speech-recognition`
- `image-to-text`

## استخدام أنابيب Optimum

في حين أن لكل مهمة فئة خط أنابيب مرتبطة بها، فمن الأبسط استخدام وظيفة [`~pipelines.pipeline`] العامة التي تقوم بتغليف جميع خطوط الأنابيب الخاصة بالمهمة في كائن واحد.

تقوم وظيفة [`~pipelines.pipeline`] تلقائيًا بتحميل نموذج افتراضي ومصنف/مستخرج ميزات قادر على أداء الاستدلال لمهمتك.

1. ابدأ بإنشاء خط أنابيب عن طريق تحديد مهمة الاستدلال:

```python
>>> from optimum.pipelines import pipeline

>>> classifier = pipeline(task="text-classification", accelerator="ort")
```

2. مرر نص/صورة الإدخال إلى وظيفة [`~pipelines.pipeline`]:

```python
>>> classifier("I like you. I love you.")  # doctest: +IGNORE_RESULT
[{'label': 'POSITIVE', 'score': 0.9998838901519775}]
```

_ملاحظة: النماذج الافتراضية المستخدمة في وظيفة [`~pipelines.pipeline`] غير مُحسَّنة للاستدلال أو التكميم، لذا لن يكون هناك تحسن في الأداء مقارنة بنظيراتها في PyTorch._

### استخدام نموذج Transformers عادي وتحويله إلى ONNX

تقبل وظيفة [`~pipelines.pipeline`] أي نموذج مدعوم من [Hugging Face Hub](https://huggingface.co/models).

هناك علامات على Model Hub تسمح لك بتصفية النموذج الذي تريد استخدامه لمهمتك.

<Tip>

لتحميل النموذج باستخدام backend ONNX Runtime، يجب دعم التصدير إلى ONNX للهندسة المعمارية قيد النظر.

يمكنك التحقق من قائمة الهندسات المعمارية المدعومة [هنا](https://huggingface.co/docs/optimum/exporters/onnx/overview#overview).

</Tip>

بمجرد اختيار نموذج مناسب، يمكنك إنشاء [`~pipelines.pipeline`] عن طريق تحديد مستودع النماذج:

```python
>>> from optimum.pipelines import pipeline

# سيتم تحميل النموذج إلى ORTModelForQuestionAnswering.
>>> onnx_qa = pipeline("question-answering", model="deepset/roberta-base-squad2", accelerator="ort")
>>> question = "What's my name?"
>>> context = "My name is Philipp and I live in Nuremberg."

>>> pred = onnx_qa(question=question, context=context)
```

من الممكن أيضًا تحميله باستخدام طريقة `from_pretrained(model_name_or_path، export=True)` المرتبطة بفئة `ORTModelForXXX`.

على سبيل المثال، إليك كيفية تحميل فئة [`~onnxruntime.ORTModelForQuestionAnswering`] للاستدلال على الأسئلة:

```python
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import ORTModelForQuestionAnswering
>>> from optimum.pipelines import pipeline

>>> tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2")

>>> # تحميل نقطة تفتيش PyTorch وتحويلها إلى تنسيق ONNX عن طريق توفير
>>> # export=True
>>> model = ORTModelForQuestionAnswering.from_pretrained(
...     "deepset/roberta-base-squad2",
...     export=True
... )

>>> onnx_qa = pipeline("question-answering", model=model, tokenizer=tokenizer, accelerator="ort")
>>> question = "What's my name?"
>>> context = "My name is Philipp and I live in Nuremberg."

>>> pred = onnx_qa(question=question, context=context)
```

### استخدام نماذج Optimum

تتكامل وظيفة [`~pipelines.pipeline`] بشكل وثيق مع [Hugging Face Hub](https://huggingface.co/models) ويمكنها تحميل نماذج ONNX مباشرة.

```python
>>> from optimum.pipelines import pipeline

>>> onnx_qa = pipeline("question-answering", model="optimum/roberta-base-squad2", accelerator="ort")
>>> question = "What's my name?"
>>> context = "My name is Philipp and I live in Nuremberg."

>>> pred = onnx_qa(question=question, context=context)
```

من الممكن أيضًا تحميله باستخدام طريقة `from_pretrained(model_name_or_path)` المرتبطة بفئة `ORTModelForXXX`.

على سبيل المثال، إليك كيفية تحميل فئة [`~onnxruntime.ORTModelForQuestionAnswering`] للاستدلال على الأسئلة:

```python
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import ORTModelForQuestionAnswering
>>> from optimum.pipelines import pipeline

>>> tokenizer = AutoTokenizer.from_pretrained("optimum/roberta-base-squad2")

>>> # تحميل نموذج ONNX مباشرة من مستودع النماذج.
>>> model = ORTModelForQuestionAnswering.from_pretrained("optimum/roberta-base-squad2")

>>> onnx_qa = pipeline("question-answering", model=model, tokenizer=tokenizer, accelerator="ort")
>>> question = "What's my name?"
>>> context = "My name is Philipp and I live in Nuremberg."

>>> pred = onnx_qa(question=question, context=context)
```

## التحسين والتحديد الكمي في خطوط الأنابيب

لا يمكن لوظيفة [`~pipelines.pipeline`] تشغيل الاستدلال على نقاط تفتيش ONNX Runtime العادية فحسب - بل يمكنك أيضًا استخدام نقاط تفتيش محسنة باستخدام [`~optimum.onnxruntime.ORTQuantizer`] و [`~optimum.onnxruntime.ORTOptimizer`].

فيما يلي مثالان على كيفية استخدام [`~optimum.onnxruntime.ORTOptimizer`] و [`~optimum.onnxruntime.ORTQuantizer`] لتحسين/تحديد كمية نموذجك واستخدامه للاستدلال بعد ذلك.

### التحديد الكمي باستخدام `ORTQuantizer`

```python
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import (
...     AutoQuantizationConfig,
...     ORTModelForSequenceClassification,
...     ORTQuantizer
... )
>>> from optimum.pipelines import pipeline

>>> # تحميل المصنف وتصدير النموذج إلى تنسيق ONNX
>>> model_id = "distilbert-base-uncased-finetuned-sst-2-english"
>>> save_dir = "distilbert_quantized"

>>> model = ORTModelForSequenceClassification.from_pretrained(model_id, export=True)

>>> # تحميل تكوين التحديد الكمي الذي يوضح التحديد الكمي الذي نرغب في تطبيقه
>>> qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=True)
>>> quantizer = ORTQuantizer.from_pretrained(model)

>>> # تطبيق التحديد الكمي الديناميكي وحفظ النموذج الناتج
>>> quantizer.quantize(save_dir=save_dir, quantization_config=qconfig)  # doctest: +IGNORE_RESULT

>>> # تحميل النموذج المحدد كميًا من مستودع محلي
>>> model = ORTModelForSequenceClassification.from_pretrained(save_dir)

>>> # إنشاء خط أنابيب المحولات
>>> onnx_clx = pipeline("text-classification", model=model, accelerator="ort")
>>> text = "I like the new ORT pipeline"
>>> pred = onnx_clx(text)
>>> print(pred)  # doctest: +IGNORE_RESULT
>>> # [{'label': 'POSITIVE', 'score': 0.9974810481071472}]

>>> # حفظ ودفع النموذج إلى المركز (في الممارسة العملية، يمكن استخدام save_dir هنا بدلاً من ذلك)
>>> model.save_pretrained("new_path_for_directory")
>>> model.push_to_hub("new_path_for_directory", repository_id="my-onnx-repo", use_auth_token=True)  # doctest: +SKIP
```

### التحسين باستخدام `ORTOptimizer`

```python
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import (
...     AutoOptimizationConfig,
...     ORTModelForSequenceClassification,
...     ORTOptimizer
... )
>>> from optimum.onnxruntime.configuration import OptimizationConfig
>>> from optimum.pipelines import pipeline

>>> # تحميل المصنف وتصدير النموذج إلى تنسيق ONNX
>>> model_id = "distilbert-base-uncased-finetuned-sst-2-english"
>>> save_dir = "distilbert_optimized"

>>> tokenizer = AutoTokenizer.from_pretrained(model_id)
>>> model = ORTModelForSequenceClassification.from_pretrained(model_id, export=True)

>>> # تحميل تكوين التحسين الذي يوضح التحسين الذي نرغب في تطبيقه
>>> optimization_config = AutoOptimizationConfig.O3()
>>> optimizer = ORTOptimizer.from_pretrained(model)

>>> optimizer.optimize(save_dir=save_dir, optimization_config=optimization_config)  # doctest: +IGNORE_RESULT

# تحميل النموذج المحسن من مستودع محلي
>>> model = ORTModelForSequenceClassification.from_pretrained(save_dir)

# إنشاء خط أنابيب المحولات
>>> onnx_clx = pipeline("text-classification", model=model, accelerator="ort")
>>> text = "I like the new ORT pipeline"
>>> pred = onnx_clx(text)
>>> print(pred)  # doctest: +IGNORE_RESULT
>>> # [{'label': 'POSITIVE', 'score': 0.9973127245903015}]

# حفظ ودفع النموذج إلى المركز
>>> tokenizer.save_pretrained("new_path_for_directory")  # doctest: +IGNORE_RESULT
>>> model.save_pretrained("new_path_for_directory")
>>> model.push_to_hub("new_path_for_directory", repository_id="my-onnx-repo", use_auth_token=True)  # doctest: +SKIP
```