# ุงูุถุจุท ุงูููู 

## ุชูุงูู AutoGPTQ 

ุชุนุงูู ูุดุฑูุน Optimum ูุน [ููุชุจุฉ AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) ูุชูููุฑ ูุงุฌูุฉ ุจุฑูุฌุฉ ุชุทุจููุงุช ุจุณูุทุฉ ูุชุทุจูู ุงูุถุจุท ุงูููู GPTQ ุนูู ููุงุฐุฌ ุงููุบุฉ. ูุน ุงูุถุจุท ุงูููู GPTQุ ููููู ุถุจุท ูููุฐุฌ ุงููุบุฉ ุงูููุถู ูุฏูู ุฅูู 8 ุฃู 4 ุฃู 3 ุฃู ุญุชู 2 ุจุช. ูุฃุชู ูุฐุง ุจุฏูู ุงูุฎูุงุถ ูุจูุฑ ูู ุงูุฃุฏุงุก ูุจุณุฑุนุฉ ุงุณุชุฏูุงู ุฃุณุฑุน. ูุฏุนู ูุฐุง ูุนุธู ุฃุฌูุฒุฉ GPU.

ุฅุฐุง ููุช ุชุฑุบุจ ูู ุถุจุท ููุงุฐุฌ ๐ค Transformers ุจุงุณุชุฎุฏุงู GPTQุ ุงุชุจุน ูุฐู [ุงููุซุงุฆู](https://huggingface.co/docs/transformers/main_classes/quantization).

ููุนุฑูุฉ ุงููุฒูุฏ ุนู ุชูููุฉ ุงูุถุจุท ุงููุณุชุฎุฏูุฉ ูู GPTQุ ูุฑุฌู ุงูุฑุฌูุน ุฅูู:

- ูุฑูุฉ [GPTQ](https://arxiv.org/pdf/2210.17323.pdf) 
- ููุชุจุฉ [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) ุงููุณุชุฎุฏูุฉ ูุฎูููุฉ 

ูุงุญุธ ุฃู ููุชุจุฉ AutoGPTQ ุชููุฑ ุงุณุชุฎุฏุงููุง ูุชูุฏููุง ุฃูุซุฑ (ุฎูููุฉ Tritonุ ูุงูุงูุชูุงู ุงูููุฏูุฌุ ูMLP ุงูููุฏูุฌ) ูุงูุชู ูุง ูุชู ุฏูุฌูุง ูุน Optimum. ูู ุงูููุช ุงูุญุงููุ ูุณุชููุฏ ููุท ูู ููุงุฉ CUDA ูู GPTQ.

### ุงููุชุทูุจุงุช

ูุฌุจ ุฃู ูููู ูุฏูู ุงููุชุทูุจุงุช ุงูุชุงููุฉ ุงููุซุจุชุฉ ูุชุดุบูู ุงูููุฏ ุฃุฏูุงู:

- ููุชุจุฉ AutoGPTQ:

```
pip install auto-gptq
```

- ููุชุจุฉ Optimum:

```
pip install --upgrade optimum
```

- ูู ุจุชุซุจูุช ุฃุญุฏุซ ููุชุจุฉ `transformers` ูู ุงููุตุฏุฑ:

```
pip install --upgrade git+https://github.com/huggingface/transformers.git
```

- ูู ุจุชุซุจูุช ุฃุญุฏุซ ููุชุจุฉ `accelerate`:

```
pip install --upgrade accelerate
```

### ุชุญููู ูุถุจุท ูููุฐุฌ

ุชูุณุชุฎุฏู ูุฆุฉ [`~optimum.gptq.GPTQQuantizer`] ูุถุจุท ูููุฐุฌู. ูุถุจุท ูููุฐุฌูุ ุชุญุชุงุฌ ุฅูู ุชูููุฑ ุจุนุถ ุงูุญุฌุฌ:

- ุนุฏุฏ ุงูุจุชุงุช: `bits`
- ูุฌููุนุฉ ุงูุจูุงูุงุช ุงููุณุชุฎุฏูุฉ ููุนุงูุฑุฉ ุงูุถุจุท: `dataset`
- ุทูู ุชุณูุณู ุงููููุฐุฌ ุงููุณุชุฎุฏู ููุนุงูุฌุฉ ูุฌููุนุฉ ุงูุจูุงูุงุช: `model_seqlen`
- ุงุณู ุงููุชูุฉ ุงูุชู ุณูุชู ุถุจุทูุง: `block_name_to_quantize` 

ูุน ุชูุงูู ๐ค Transformersุ ูุง ุชุญุชุงุฌ ุฅูู ุชูุฑูุฑ `block_name_to_quantize` ู`model_seqlen` ุญูุซ ูููููุง ุงุณุชุฑุฏุงุฏูุง. ููุน ุฐููุ ุจุงููุณุจุฉ ูููููุฐุฌ ุงููุฎุตุตุ ุชุญุชุงุฌ ุฅูู ุชุญุฏูุฏูุง. ุฃูุถูุงุ ุชุฃูุฏ ูู ุชุญููู ูููุฐุฌู ุฅูู `torch.float16` ูุจู ุงูุถุจุท.

```py
from transformers import AutoModelForCausalLM, AutoTokenizer
from optimum.gptq import GPTQQuantizer, load_quantized_model
import torch

model_name = "facebook/opt-125m"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)

quantizer = GPTQQuantizer(bits=4, dataset="c4", block_name_to_quantize = "model.decoder.layers", model_seqlen = 2048)
quantized_model = quantizer.quantize_model(model, tokenizer)
```

<Tip warning={true}>

ูุนูู ุงูุถุจุท ุงูููู GPTQ ููุท ุนูู ููุงุฐุฌ ุงููุต ูู ุงูููุช ุงูุญุงูู. ุนูุงูุฉ ุนูู ุฐููุ ูููู ุฃู ุชุณุชุบุฑู ุนูููุฉ ุงูุถุจุท ุงููุซูุฑ ูู ุงูููุช ุงุนุชูุงุฏูุง ุนูู ุนุชุงุฏ ุงููุณุชุฎุฏู (ุงููููุฐุฌ 175B = 4 ุณุงุนุงุช ูุญุฏุฉ ูุนุงูุฌุฉ ุฑุณูููุฉ ุจุงุณุชุฎุฏุงู NVIDIA A100). ูุฑุฌู ุงูุชุญูู ุนูู Hugging Face Hub ุฅุฐุง ูู ููู ููุงู ุจุงููุนู ุฅุตุฏุงุฑ ูุถุจูุท ูู GPTQ ูููููุฐุฌ ุงูุฐู ุชุฑูุฏ ุถุจุทู.

</Tip>

### ุญูุธ ุงููููุฐุฌ

ูุญูุธ ูููุฐุฌูุ ุงุณุชุฎุฏู ุทุฑููุฉ ุงูุญูุธ ูู ูุฆุฉ [`~optimum.gptq.GPTQQuantizer`]. ุณูููู ุจุฅูุดุงุก ูุฌูุฏ ูุน ูุงููุณ ุญุงูุฉ ูููุฐุฌู ุฌูุจูุง ุฅูู ุฌูุจ ูุน ุชูููู ุงูุถุจุท.

```python
save_folder = "/path/to/save_folder/"
quantizer.save(model,save_folder)
```

### ุชุญููู ุงูุฃูุฒุงู ุงููุถุจูุทุฉ

ููููู ุชุญููู ุงูุฃูุฒุงู ุงููุถุจูุทุฉ ุจุงุณุชุฎุฏุงู ุฏุงูุฉ [`~optimum.gptq.load_quantized_model`].

ูู ุฎูุงู ููุชุจุฉ Accelerateุ ูููู ุชุญููู ูููุฐุฌ ุฃุณุฑุน ูุน ุงุณุชุฎุฏุงู ุฐุงูุฑุฉ ุฃูู. ูุฌุจ ุชููุฆุฉ ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู ุฃูุฒุงู ูุงุฑุบุฉุ ูุน ุชุญููู ุงูุฃูุฒุงู ูุฎุทูุฉ ุชุงููุฉ.

```python
from accelerate import init_empty_weights
with init_empty_weights():
empty_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)
empty_model.tie_weights()
quantized_model = load_quantized_model(empty_model, save_folder=save_folder, device_map="auto")
```

### ููู Exllama ููุงุณุชุฏูุงู ุงูุฃุณุฑุน

ูุน ุฅุตุฏุงุฑ ููู exllamav2ุ ููููู ุงูุญุตูู ุนูู ุณุฑุนุฉ ุงุณุชุฏูุงู ุฃุณุฑุน ููุงุฑูุฉ ุจููู exllama ููููุฐุฌ 4-ุจุช. ูุชู ุชูุดูุทู ุจุดูู ุงูุชุฑุงุถู: `disable_exllamav2=False` ูู [`~optimum.gptq.load_quantized_model`]. ูุงุณุชุฎุฏุงู ูุฐู ุงููููุ ุชุญุชุงุฌ ุฅูู ูุฌูุฏ ุงููููุฐุฌ ุจุงููุงูู ุนูู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณููุงุช.

```py
from optimum.gptq import GPTQQuantizer, load_quantized_model
import torch

from accelerate import init_empty_weights
with init_empty_weights():
empty_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)
empty_model.tie_weights()
quantized_model = load_quantized_model(empty_model, save_folder=save_folder, device_map="auto")
```

ุฅุฐุง ููุช ุชุฑุบุจ ูู ุงุณุชุฎุฏุงู ููู exllamaุ ูุณูุชุนูู ุนููู ุชุบููุฑ ุงูุฅุตุฏุงุฑ ุนู ุทุฑูู ุชุนููู `exllama_config`:

```py
from optimum.gptq import GPTQQuantizer, load_quantized_model
import torch

from accelerate import init_empty_weights
with init_empty_weights():
empty_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)
empty_model.tie_weights()
quantized_model = load_quantized_model(empty_model, save_folder=save_folder, device_map="auto", exllama_config = {"version":1})
```

ูุงุญุธ ุฃู ุงูููู exllama/exllamav2 ุชุฏุนู ููุท ููุงุฐุฌ 4-ุจุช ูู ุงูููุช ุงูุญุงูู. ุนูุงูุฉ ุนูู ุฐููุ ููุตู ุจุชุนุทูู ููู exllama/exllamav2 ุนูุฏูุง ุชููู ุจุถุจุท ูููุฐุฌู ุจุงุณุชุฎุฏุงู peft.

ููููู ุงูุนุซูุฑ ุนูู ูุนูุงุฑ ูุฐู ุงูููู [ููุง](https://github.com/huggingface/optimum/tree/main/tests/benchmark#gptq-benchmark) 

#### ุถุจุท ูููุฐุฌ ูุถุจูุท ุจุดูู ุฏููู

ูุน ุงูุฏุนู ุงูุฑุณูู ูููุญุฏุงุช ุงูููุทูุฉ ูู ูุธุงู Hugging Face ุงูุจูุฆูุ ููููู ุถุจุท ุงูููุงุฐุฌ ุงูุชู ุชู ุถุจุทูุง ุจุงุณุชุฎุฏุงู GPTQ.

ูุฑุฌู ุงูุงุทูุงุน ุนูู ููุชุจุฉ [`peft`](https://github.com/huggingface/peft) ููุฒูุฏ ูู ุงูุชูุงุตูู.